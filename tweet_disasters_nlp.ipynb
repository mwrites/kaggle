{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Kaggle](https://www.kaggle.com/competitions/nlp-getting-started/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from fastai.imports import *\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp-getting-started.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                                                                         Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                        Forest fire near La Ronge Sask. Canada   \n",
       "2         All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                             13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                                      Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "...                                                                                                                                         ...   \n",
       "7608                                                        Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5   \n",
       "7609              @aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.   \n",
       "7610                                                                          M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ   \n",
       "7611  Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.   \n",
       "7612                                             The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d   \n",
       "\n",
       "      target  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "7608       1  \n",
       "7609       1  \n",
       "7610       1  \n",
       "7611       1  \n",
       "7612       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kagtool.datasets.kaggle_downloader import KaggleDownloader\n",
    "\n",
    "dataset_name = 'nlp-getting-started'\n",
    "creds = ''\n",
    "\n",
    "path = KaggleDownloader(dataset_name, creds).load_or_fetch_kaggle_dataset()\n",
    "df = pd.read_csv(path/'train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj top link : xxmaj reddit 's new content policy goes into effect many horrible subreddits banned or quarantined http : / / t.co / xxunk xxbos xxmaj truly a scene of chaos xxunk in xxunk . xxmaj pandemonium even . xxmaj utter xxunk . xxmaj that anyone survived such xxunk is xxunk . xxunk xxbos xxmaj xxunk xxmaj xxunk 's knee injury could xxunk xxmaj jazz 's xxunk - for</td>\n",
       "      <td>xxmaj top link : xxmaj reddit 's new content policy goes into effect many horrible subreddits banned or quarantined http : / / t.co / xxunk xxbos xxmaj truly a scene of chaos xxunk in xxunk . xxmaj pandemonium even . xxmaj utter xxunk . xxmaj that anyone survived such xxunk is xxunk . xxunk xxbos xxmaj xxunk xxmaj xxunk 's knee injury could xxunk xxmaj jazz 's xxunk - for xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a xxmaj dream of xxmaj humanity ' xxunk by xxunk û _ û _ http : / / t.co / xxunk xxbos ' planted xxunk years ago ' it was moved to xxup u - s. xxmaj this xxmaj xxunk xxmaj survived xxmaj hiroshima xxmaj but xxmaj its xxmaj story xxmaj was xxmaj nearly xxmaj lost http : / / t.co / xxunk xxunk via xxunk xxbos # xxunk xxup u.s .</td>\n",
       "      <td>xxmaj dream of xxmaj humanity ' xxunk by xxunk û _ û _ http : / / t.co / xxunk xxbos ' planted xxunk years ago ' it was moved to xxup u - s. xxmaj this xxmaj xxunk xxmaj survived xxmaj hiroshima xxmaj but xxmaj its xxmaj story xxmaj was xxmaj nearly xxmaj lost http : / / t.co / xxunk xxunk via xxunk xxbos # xxunk xxup u.s . xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this . xxbos xxmaj all injuries xxmaj pre xxmaj foster / xxmaj floyd . xxmaj those will be covered next week . https : / / t.co / xxunk xxbos xxunk xxmaj hey xxmaj stephen xxmaj remember that time you drowned all the xxunk \\n\\n xxmaj read : http : / / t.co / xxunk xxbos xxunk for the record i m xxunk out the window early … i got xxunk body</td>\n",
       "      <td>. xxbos xxmaj all injuries xxmaj pre xxmaj foster / xxmaj floyd . xxmaj those will be covered next week . https : / / t.co / xxunk xxbos xxunk xxmaj hey xxmaj stephen xxmaj remember that time you drowned all the xxunk \\n\\n xxmaj read : http : / / t.co / xxunk xxbos xxunk for the record i m xxunk out the window early … i got xxunk body bagging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3952, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls_lm = DataBlock(\n",
    "    blocks=TextBlock.from_df('text', is_lm=True),\n",
    "    get_items=ColReader('text'),  # Assuming each item in your DataFrame is a text entry\n",
    "    splitter=RandomSplitter(0.2)  # Randomly split the data into training and validation sets\n",
    ").dataloaders(df, bs=bs, drop_last=True, shuffle=False)\n",
    "\n",
    "if 'darwin' in platform.system().lower(): dls_lm.device = None\n",
    "\n",
    "dls_lm.show_batch(max_n=3)\n",
    "len(dls_lm.vocab), dls_lm.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(3952, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(3952, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0-2): 3 x RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=3952, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()\n",
    "\n",
    "# learn.lr_find()\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.045155</td>\n",
       "      <td>3.287371</td>\n",
       "      <td>0.426085</td>\n",
       "      <td>26.772390</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.071996</td>\n",
       "      <td>3.084633</td>\n",
       "      <td>0.448763</td>\n",
       "      <td>21.859446</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.936670</td>\n",
       "      <td>2.987650</td>\n",
       "      <td>0.468099</td>\n",
       "      <td>19.838999</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.737832</td>\n",
       "      <td>3.063449</td>\n",
       "      <td>0.472396</td>\n",
       "      <td>21.401251</td>\n",
       "      <td>08:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.472211</td>\n",
       "      <td>3.120795</td>\n",
       "      <td>0.476063</td>\n",
       "      <td>22.664402</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.178889</td>\n",
       "      <td>3.169253</td>\n",
       "      <td>0.477322</td>\n",
       "      <td>23.789703</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.901013</td>\n",
       "      <td>3.305815</td>\n",
       "      <td>0.476628</td>\n",
       "      <td>27.270771</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.648637</td>\n",
       "      <td>3.379928</td>\n",
       "      <td>0.473372</td>\n",
       "      <td>29.368652</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.414544</td>\n",
       "      <td>3.462519</td>\n",
       "      <td>0.476280</td>\n",
       "      <td>31.897238</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.195374</td>\n",
       "      <td>3.596308</td>\n",
       "      <td>0.474935</td>\n",
       "      <td>36.463352</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.033792</td>\n",
       "      <td>3.593688</td>\n",
       "      <td>0.477040</td>\n",
       "      <td>36.367935</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 5e-2)\n",
    "learn.save('1epoch')\n",
    "learn = learn.load('1epoch')\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 5e-3)\n",
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire Burns On California U.s . China Economic Capital û _ : http :\n",
      "Fire crews evacuate passengers from a Gold Coast tram trapped when others collided : http : / / t.co\n",
      "Fire in Buffalo SW is 8 m Old Lady Lower Dies of Cramer\n",
      "Fire burns two buildings on 2nd Street in # Manchester http : / / t.co / Fire\n",
      "Fire truck and ambulance in NYC All Girls All Girls Just Girls Having\n"
     ]
    }
   ],
   "source": [
    "def predict_next(txt, learn):\n",
    "    N_WORDS = 20\n",
    "    N_SENTENCES = 5\n",
    "    preds = [learn.predict(txt, N_WORDS, temperature=0.75) \n",
    "             for _ in range(N_SENTENCES)]\n",
    "    print(\"\\n\".join(preds))\n",
    "\n",
    "predict_next(\"Fire\", learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos _ xxunk xxrep 5 ? xxup xxunk xxunk xxrep 7 ? xxunk xxrep 5 ? xxup follow xxup all xxup who xxup rt xxunk xxrep 7 ? xxunk xxrep 5 ? xxup xxunk xxunk xxrep 7 ? xxunk xxrep 5 ? xxup xxunk xxup with xxunk xxrep 7 ? xxunk xxrep 5 ? xxup follow ? xxunk # xxup xxunk xxunk # xxup ty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj truth … \\n https : / / t.co / xxunk xxunk # xxmaj news xxunk # xxup bbc xxunk # xxup cnn xxunk # xxmaj islam xxunk # xxmaj truth xxunk # god xxunk # xxup isis xxunk # terrorism xxunk # xxmaj quran xxunk # xxmaj lies http : / / t.co / xxunk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj truth … \\n https : / / t.co / xxunk xxunk # xxmaj news xxunk # xxup bbc xxunk # xxup cnn xxunk # xxmaj islam xxunk # xxmaj truth xxunk # god xxunk # xxup isis xxunk # terrorism xxunk # xxmaj quran xxunk # xxmaj lies http : / / t.co / xxunk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = DataBlock(\n",
    "    blocks=(TextBlock.from_df('text', vocab=dls_lm.vocab), CategoryBlock),\n",
    "    get_x=ColReader('text'),\n",
    "    get_y=ColReader('target'),\n",
    "    splitter=RandomSplitter(0.2)\n",
    ").dataloaders(df, bs=32, num_workers=8)\n",
    "\n",
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialRNN(\n",
      "  (0): SentenceEncoder(\n",
      "    (module): AWD_LSTM(\n",
      "      (encoder): Embedding(3952, 400, padding_idx=1)\n",
      "      (encoder_dp): EmbeddingDropout(\n",
      "        (emb): Embedding(3952, 400, padding_idx=1)\n",
      "      )\n",
      "      (rnns): ModuleList(\n",
      "        (0): WeightDropout(\n",
      "          (module): LSTM(400, 1152, batch_first=True)\n",
      "        )\n",
      "        (1): WeightDropout(\n",
      "          (module): LSTM(1152, 1152, batch_first=True)\n",
      "        )\n",
      "        (2): WeightDropout(\n",
      "          (module): LSTM(1152, 400, batch_first=True)\n",
      "        )\n",
      "      )\n",
      "      (input_dp): RNNDropout()\n",
      "      (hidden_dps): ModuleList(\n",
      "        (0-2): 3 x RNNDropout()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): PoolingLinearClassifier(\n",
      "    (layers): Sequential(\n",
      "      (0): LinBnDrop(\n",
      "        (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Dropout(p=0.16000000000000003, inplace=False)\n",
      "        (2): Linear(in_features=1200, out_features=50, bias=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): LinBnDrop(\n",
      "        (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Dropout(p=0.1, inplace=False)\n",
      "        (2): Linear(in_features=50, out_features=2, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.0014454397605732083)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    gc.collect()\n",
    "    del learn\n",
    "except: pass\n",
    "\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.4, \n",
    "                                metrics=accuracy).to_fp16()\n",
    "\n",
    "learn = learn.load_encoder('finetuned')\n",
    "print(learn.model)\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.536797</td>\n",
       "      <td>0.481152</td>\n",
       "      <td>0.765440</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial training with all layers frozen except the final classifier layers\n",
    "learn.fit_one_cycle(1, 1e-3, wd=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.506742</td>\n",
       "      <td>0.452837</td>\n",
       "      <td>0.798292</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.483780</td>\n",
       "      <td>0.445159</td>\n",
       "      <td>0.803548</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [5/10 01:24&lt;01:24]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.410593</td>\n",
       "      <td>0.444920</td>\n",
       "      <td>0.804862</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.379414</td>\n",
       "      <td>0.473237</td>\n",
       "      <td>0.812746</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.353666</td>\n",
       "      <td>0.485051</td>\n",
       "      <td>0.805519</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.285198</td>\n",
       "      <td>0.528499</td>\n",
       "      <td>0.809461</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.247618</td>\n",
       "      <td>0.618593</td>\n",
       "      <td>0.795007</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='13' class='' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      6.84% [13/190 00:01&lt;00:15 0.2389]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Unfreeze all layers and fine-tune the entire model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m learn\u001b[38;5;241m.\u001b[39munfreeze()\n\u001b[0;32m---> 11\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_one_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.6\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/callback/schedule.py:119\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    116\u001b[0m lr_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    117\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[38;5;241m/\u001b[39mdiv, lr_max, lr_max\u001b[38;5;241m/\u001b[39mdiv_final),\n\u001b[1;32m    118\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmom\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, \u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoms \u001b[38;5;28;01mif\u001b[39;00m moms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m moms))}\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/data/load.py:127\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_iter()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idxs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_idxs() \u001b[38;5;66;03m# called in context of main process (not workers/subprocesses)\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfake_l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfake_l\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pin_memory causes tuples to be converted to lists, so convert them back to tuples\u001b[39;49;00m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:41\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/data/load.py:138\u001b[0m, in \u001b[0;36mDataLoader.create_batches\u001b[0;34m(self, samps)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m    137\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m o:o \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_item, samps))\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunkify(res))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/data/load.py:183\u001b[0m, in \u001b[0;36mDataLoader.do_batch\u001b[0;34m(self, b)\u001b[0m\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, b): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_batch(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbefore_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m), b)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastcore/transform.py:208\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, o)\u001b[0m\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, o): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompose_tfms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastcore/transform.py:158\u001b[0m, in \u001b[0;36mcompose_tfms\u001b[0;34m(x, tfms, is_enc, reverse, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m tfms:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_enc: f \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mdecode\n\u001b[0;32m--> 158\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/text/data.py:175\u001b[0m, in \u001b[0;36mPad_Chunk.__call__\u001b[0;34m(self, b, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, b, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_call(b)\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastcore/transform.py:81\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastcore/transform.py:91\u001b[0m, in \u001b[0;36mTransform._call\u001b[0;34m(self, fn, x, split_idx, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, x, split_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m split_idx\u001b[38;5;241m!=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_idx \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastcore/transform.py:98\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[0;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     ret \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreturns(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(f(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), x, ret)\n\u001b[0;32m---> 98\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastcore/transform.py:98\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m     ret \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreturns(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(f(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), x, ret)\n\u001b[0;32m---> 98\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastcore/transform.py:98\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[0;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     ret \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreturns(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(f(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), x, ret)\n\u001b[0;32m---> 98\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastcore/transform.py:98\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m     ret \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreturns(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(f(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), x, ret)\n\u001b[0;32m---> 98\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastcore/transform.py:97\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[0;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     96\u001b[0m     ret \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreturns(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, x, ret)\n\u001b[1;32m     98\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(f, x_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastcore/dispatch.py:120\u001b[0m, in \u001b[0;36mTypeDispatch.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m MethodType(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mowner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m MethodType(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mowner)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/text/data.py:177\u001b[0m, in \u001b[0;36mPad_Chunk.encodes\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencodes\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:TensorText):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpad_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/text/data.py:153\u001b[0m, in \u001b[0;36mpad_chunk\u001b[0;34m(x, pad_idx, pad_first, seq_len, pad_len)\u001b[0m\n\u001b[1;32m    151\u001b[0m pad_chunk \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnew_zeros((l\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mseq_len) \u001b[38;5;241m*\u001b[39m seq_len) \u001b[38;5;241m+\u001b[39m pad_idx\n\u001b[1;32m    152\u001b[0m pad_res   \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnew_zeros(l \u001b[38;5;241m%\u001b[39m seq_len) \u001b[38;5;241m+\u001b[39m pad_idx\n\u001b[0;32m--> 153\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpad_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_res\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m pad_first \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([x, pad_chunk, pad_res])\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(x1, x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/torch_core.py:382\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__str__\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _torch_handled(args, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_opt, func): types \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mTensor,)\n\u001b[0;32m--> 382\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mifnone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m dict_objs \u001b[38;5;241m=\u001b[39m _find_args(args) \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m _find_args(\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(res),TensorBase) \u001b[38;5;129;01mand\u001b[39;00m dict_objs: res\u001b[38;5;241m.\u001b[39mset_meta(dict_objs[\u001b[38;5;241m0\u001b[39m],as_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:1447\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:1537\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(ret, cls)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Tensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, \u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m-> 1537\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mret\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_subclass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# Also handles things like namedtuples\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(ret)(_convert(r, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ret)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fastai/torch_core.py:331\u001b[0m, in \u001b[0;36mas_subclass\u001b[0;34m(self, typ)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;129m@patch\u001b[39m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_subclass\u001b[39m(\u001b[38;5;28mself\u001b[39m:Tensor, typ):\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCast to `typ` and include `__dict__` and meta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retain_meta(\u001b[38;5;28mself\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_subclass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Freeze all layers except the last two\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2 / (2.6 ** 4), 1e-2), wd=1e-2)\n",
    "\n",
    "# Freeze all layers except the last three\n",
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3 / (2.6 ** 4), 5e-3), wd=1e-2)\n",
    "\n",
    "# Unfreeze all layers and fine-tune the entire model\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(1, slice(1e-3 / (2.6 ** 4), 1e-3), wd=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst_df = pd.read_csv(path/'test.csv')\n",
    "tst_dl = learn.dls.test_dl(tst_df)\n",
    "preds = learn.get_preds(dl=tst_dl)[0]\n",
    "\n",
    "def subm(preds, suff):\n",
    "    tst_df['target'] = preds.argmax(dim=1)\n",
    "    sub_df = tst_df[['id','target']]\n",
    "    sub_df.to_csv(f'sub-{suff}.csv', index=False)\n",
    "\n",
    "subm(preds, 'lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest With Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#6091) [(TensorText([   2,    8,  919,  564,   20, 3542,   14,  435,   18,   33, 2900,\n",
       "               0,  783,    5,  110,   21]), TensorCategory(0)),(TensorText([   2,    8, 1148,    9,    8,   69,    8,    0,   28,    0,    8,\n",
       "             587,    8, 2023,   28,    8,  290,    8, 1634,    8,    0,   19,\n",
       "               8,    0,    8, 2287,   26,   12,   10,    9,    9,   11,    9,\n",
       "               0,   12,   10,    9,    9,   11,    9,    0]), TensorCategory(1)),(TensorText([   2,   24,  164, 1157, 2360,   17,    8,    0,    8, 2788,   24,\n",
       "               7,    0,    7, 2788,  475,   16,  164,   23, 2360,  325,  460,\n",
       "               0, 3451,    8,   14,  117,    0,   46,   12,   10,    9,    9,\n",
       "              11,    9,    0]), TensorCategory(1)),(TensorText([   2,    8, 1590,    8,  520,    8,   75,   74,    8,  740,    8,\n",
       "             339,    8,  941,    8,   91,    8, 3436,   31,    8,    0,   19,\n",
       "               8,  121,   17,    8,   76,    7,    0,    8,  464,    8,    0,\n",
       "               8,    0,   12,   10,    9,    9,   11,    9,    0,   83,    0]), TensorCategory(0)),(TensorText([   2,    8,   89,  163,  770,   50,  197, 3802,  264,  147,  492,\n",
       "               0,   36,    0,   15,    8,   80,  202,   47,  951,   13,    0]), TensorCategory(0)),(TensorText([  2,   0, 164, 264,  21,  21]), TensorCategory(1)),(TensorText([   2,    8,    0,   19,    7, 1114,   40,    8,  985,  602,   40,\n",
       "               8,  775,   40,    8,  192,    0,   19,    0,   40,    8, 3026,\n",
       "              36,  466,  156,   40,    8, 2334,    0,  102,    8, 2306,   61,\n",
       "             227,  114,   32, 1567]), TensorCategory(0)),(TensorText([   2,    8,  240,    8, 1335, 2594,    8,  341,   56,  108, 1246,\n",
       "              47,    0,   62,   12,   10,    9,    9,   11,    9,    0,   13,\n",
       "            2915]), TensorCategory(0)),(TensorText([   2,    8,  161,   90,  136,  553,   34,   34,   16,  554,   68,\n",
       "             278,    0, 1129, 1306,  115, 3778, 1442,  462,  138,   42,  205,\n",
       "              79,  904,   79,   17,   34,   34, 1568,  171,  218]), TensorCategory(0)),(TensorText([   2,    8,  225,    8,  675,    8,   97,   28,  252,   22,  367,\n",
       "               8,  732,   10,    8,  225,  733,   97,  866,   28,    8,  236,\n",
       "               8, 1075,   16,   25,   12,   10,    9,    9,   11,    9,    0]), TensorCategory(1))...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.dls.train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): SentenceEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(3952, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(3952, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0-2): 3 x RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): LinBnDrop(\n",
       "        (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Dropout(p=0.16000000000000003, inplace=False)\n",
       "        (2): Linear(in_features=1200, out_features=50, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): LinBnDrop(\n",
       "        (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=50, out_features=2, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AWD_LSTM(\n",
       "  (encoder): Embedding(3952, 400, padding_idx=1)\n",
       "  (encoder_dp): EmbeddingDropout(\n",
       "    (emb): Embedding(3952, 400, padding_idx=1)\n",
       "  )\n",
       "  (rnns): ModuleList(\n",
       "    (0): WeightDropout(\n",
       "      (module): LSTM(400, 1152, batch_first=True)\n",
       "    )\n",
       "    (1): WeightDropout(\n",
       "      (module): LSTM(1152, 1152, batch_first=True)\n",
       "    )\n",
       "    (2): WeightDropout(\n",
       "      (module): LSTM(1152, 400, batch_first=True)\n",
       "    )\n",
       "  )\n",
       "  (input_dp): RNNDropout()\n",
       "  (hidden_dps): ModuleList(\n",
       "    (0-2): 3 x RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model[0].module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer:\n",
       "encodes: (Path,object) -> encodes\n",
       "(str,object) -> encodes\n",
       "decodes: (object,object) -> decodes"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.dataset.tfms[0].tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mAWD_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvocab_sz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0memb_sz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_hid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_layers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpad_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhidden_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0membed_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweight_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbidir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mAWD_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minitrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mvocab_sz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Size of the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0memb_sz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Size of embedding vector\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mn_hid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Number of features in hidden state\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Number of LSTM layers\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpad_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Padding token id\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mhidden_p\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Dropout probability for hidden state between layers\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minput_p\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Dropout probability for LSTM stack input\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0membed_p\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Embedding layer dropout probabillity\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mweight_p\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Hidden-to-hidden wight dropout probability for LSTM layers\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbidir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;31m# If set to `True` uses bidirectional LSTM layers\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstore_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'emb_sz,n_hid,n_layers,pad_token'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbidir\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_one_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_sz\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mn_hid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_hid\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0memb_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                                 \u001b[0mbidir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRNNDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_p\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_embeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfrom_embeds\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_change_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfrom_embeds\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnew_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhid_dp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnew_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhid_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_change_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_change_one_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_one_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"Return one of the inner rnn\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbidir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mWeightDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_one_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"Return one hidden state\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hid\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_sz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dir\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mone_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_change_one_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hid\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_sz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dir\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"Reset the hidden states\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_one_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/lib/python3.12/site-packages/fastai/text/models/awdlstm.py\n",
      "\u001b[0;31mType:\u001b[0m           PrePostInitMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "AWD_LSTM??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6091, 400) (1522, 400)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>emb_9</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_390</th>\n",
       "      <th>emb_391</th>\n",
       "      <th>emb_392</th>\n",
       "      <th>emb_393</th>\n",
       "      <th>emb_394</th>\n",
       "      <th>emb_395</th>\n",
       "      <th>emb_396</th>\n",
       "      <th>emb_397</th>\n",
       "      <th>emb_398</th>\n",
       "      <th>emb_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.034183</td>\n",
       "      <td>-0.028445</td>\n",
       "      <td>0.026426</td>\n",
       "      <td>0.070351</td>\n",
       "      <td>-0.063002</td>\n",
       "      <td>0.078609</td>\n",
       "      <td>-0.019157</td>\n",
       "      <td>-0.263661</td>\n",
       "      <td>-0.056502</td>\n",
       "      <td>0.060781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029279</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>-0.022329</td>\n",
       "      <td>-0.015441</td>\n",
       "      <td>0.146932</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>0.012273</td>\n",
       "      <td>-0.039030</td>\n",
       "      <td>-0.056550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.007141</td>\n",
       "      <td>0.102240</td>\n",
       "      <td>0.077724</td>\n",
       "      <td>0.108877</td>\n",
       "      <td>-0.137661</td>\n",
       "      <td>0.252389</td>\n",
       "      <td>0.178327</td>\n",
       "      <td>-0.303554</td>\n",
       "      <td>-0.056530</td>\n",
       "      <td>0.218585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027753</td>\n",
       "      <td>0.096219</td>\n",
       "      <td>0.143482</td>\n",
       "      <td>0.030739</td>\n",
       "      <td>0.027956</td>\n",
       "      <td>0.027284</td>\n",
       "      <td>-0.045246</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.277017</td>\n",
       "      <td>-0.061442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.085716</td>\n",
       "      <td>0.040413</td>\n",
       "      <td>-0.011065</td>\n",
       "      <td>0.076396</td>\n",
       "      <td>-0.062790</td>\n",
       "      <td>0.072354</td>\n",
       "      <td>0.130806</td>\n",
       "      <td>-0.230223</td>\n",
       "      <td>-0.025018</td>\n",
       "      <td>0.145530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069291</td>\n",
       "      <td>0.092060</td>\n",
       "      <td>0.037058</td>\n",
       "      <td>-0.018767</td>\n",
       "      <td>0.018342</td>\n",
       "      <td>0.047074</td>\n",
       "      <td>-0.030460</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.130267</td>\n",
       "      <td>-0.053380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.086707</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>-0.027758</td>\n",
       "      <td>0.063932</td>\n",
       "      <td>-0.095097</td>\n",
       "      <td>0.111422</td>\n",
       "      <td>0.115529</td>\n",
       "      <td>-0.280657</td>\n",
       "      <td>-0.011152</td>\n",
       "      <td>0.288354</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016040</td>\n",
       "      <td>0.120837</td>\n",
       "      <td>0.021732</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>0.106014</td>\n",
       "      <td>-0.014085</td>\n",
       "      <td>0.052972</td>\n",
       "      <td>0.203015</td>\n",
       "      <td>0.036189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022853</td>\n",
       "      <td>0.020545</td>\n",
       "      <td>0.033012</td>\n",
       "      <td>0.096078</td>\n",
       "      <td>-0.058596</td>\n",
       "      <td>0.077099</td>\n",
       "      <td>0.073410</td>\n",
       "      <td>-0.141959</td>\n",
       "      <td>-0.099903</td>\n",
       "      <td>0.259655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130409</td>\n",
       "      <td>0.157641</td>\n",
       "      <td>-0.089053</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>-0.076420</td>\n",
       "      <td>0.065658</td>\n",
       "      <td>0.050418</td>\n",
       "      <td>-0.017438</td>\n",
       "      <td>0.026353</td>\n",
       "      <td>0.070412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>0.065367</td>\n",
       "      <td>-0.020309</td>\n",
       "      <td>-0.017432</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.021332</td>\n",
       "      <td>0.068432</td>\n",
       "      <td>0.045116</td>\n",
       "      <td>-0.244753</td>\n",
       "      <td>0.029292</td>\n",
       "      <td>0.188979</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002863</td>\n",
       "      <td>0.135913</td>\n",
       "      <td>-0.003287</td>\n",
       "      <td>-0.042199</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.009207</td>\n",
       "      <td>0.026914</td>\n",
       "      <td>0.036417</td>\n",
       "      <td>0.080097</td>\n",
       "      <td>0.097813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>-0.013671</td>\n",
       "      <td>0.027028</td>\n",
       "      <td>-0.024378</td>\n",
       "      <td>0.129615</td>\n",
       "      <td>-0.017604</td>\n",
       "      <td>0.013615</td>\n",
       "      <td>0.121575</td>\n",
       "      <td>-0.403670</td>\n",
       "      <td>0.133510</td>\n",
       "      <td>0.270534</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015863</td>\n",
       "      <td>0.092178</td>\n",
       "      <td>-0.014462</td>\n",
       "      <td>-0.070922</td>\n",
       "      <td>-0.032325</td>\n",
       "      <td>-0.044633</td>\n",
       "      <td>-0.063741</td>\n",
       "      <td>0.016771</td>\n",
       "      <td>-0.102552</td>\n",
       "      <td>0.101940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>0.060602</td>\n",
       "      <td>0.015483</td>\n",
       "      <td>-0.026816</td>\n",
       "      <td>0.061389</td>\n",
       "      <td>-0.101071</td>\n",
       "      <td>0.195014</td>\n",
       "      <td>0.084519</td>\n",
       "      <td>-0.220690</td>\n",
       "      <td>-0.120597</td>\n",
       "      <td>0.128546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052578</td>\n",
       "      <td>-0.008985</td>\n",
       "      <td>0.101729</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>0.008364</td>\n",
       "      <td>-0.036131</td>\n",
       "      <td>-0.053305</td>\n",
       "      <td>0.049233</td>\n",
       "      <td>0.275304</td>\n",
       "      <td>-0.082848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>0.029968</td>\n",
       "      <td>0.008605</td>\n",
       "      <td>-0.005549</td>\n",
       "      <td>0.028587</td>\n",
       "      <td>0.006650</td>\n",
       "      <td>0.078267</td>\n",
       "      <td>0.103960</td>\n",
       "      <td>-0.199022</td>\n",
       "      <td>0.028620</td>\n",
       "      <td>0.208258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017471</td>\n",
       "      <td>0.124545</td>\n",
       "      <td>0.067195</td>\n",
       "      <td>-0.006884</td>\n",
       "      <td>-0.018684</td>\n",
       "      <td>-0.161837</td>\n",
       "      <td>-0.021244</td>\n",
       "      <td>0.028774</td>\n",
       "      <td>0.032378</td>\n",
       "      <td>0.068750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>0.053008</td>\n",
       "      <td>0.038360</td>\n",
       "      <td>0.023448</td>\n",
       "      <td>0.052072</td>\n",
       "      <td>-0.097234</td>\n",
       "      <td>0.313675</td>\n",
       "      <td>0.161035</td>\n",
       "      <td>-0.263834</td>\n",
       "      <td>-0.167844</td>\n",
       "      <td>0.146753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043684</td>\n",
       "      <td>0.019424</td>\n",
       "      <td>0.200846</td>\n",
       "      <td>0.035497</td>\n",
       "      <td>-0.044247</td>\n",
       "      <td>-0.038127</td>\n",
       "      <td>-0.035668</td>\n",
       "      <td>0.107089</td>\n",
       "      <td>0.342910</td>\n",
       "      <td>-0.255305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6091 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         emb_0     emb_1     emb_2     emb_3     emb_4     emb_5     emb_6  \\\n",
       "0    -0.034183 -0.028445  0.026426  0.070351 -0.063002  0.078609 -0.019157   \n",
       "1    -0.007141  0.102240  0.077724  0.108877 -0.137661  0.252389  0.178327   \n",
       "2    -0.085716  0.040413 -0.011065  0.076396 -0.062790  0.072354  0.130806   \n",
       "3    -0.086707  0.019600 -0.027758  0.063932 -0.095097  0.111422  0.115529   \n",
       "4     0.022853  0.020545  0.033012  0.096078 -0.058596  0.077099  0.073410   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6086  0.065367 -0.020309 -0.017432  0.002117  0.021332  0.068432  0.045116   \n",
       "6087 -0.013671  0.027028 -0.024378  0.129615 -0.017604  0.013615  0.121575   \n",
       "6088  0.060602  0.015483 -0.026816  0.061389 -0.101071  0.195014  0.084519   \n",
       "6089  0.029968  0.008605 -0.005549  0.028587  0.006650  0.078267  0.103960   \n",
       "6090  0.053008  0.038360  0.023448  0.052072 -0.097234  0.313675  0.161035   \n",
       "\n",
       "         emb_7     emb_8     emb_9  ...   emb_390   emb_391   emb_392  \\\n",
       "0    -0.263661 -0.056502  0.060781  ... -0.029279  0.030145  0.000772   \n",
       "1    -0.303554 -0.056530  0.218585  ... -0.027753  0.096219  0.143482   \n",
       "2    -0.230223 -0.025018  0.145530  ... -0.069291  0.092060  0.037058   \n",
       "3    -0.280657 -0.011152  0.288354  ... -0.016040  0.120837  0.021732   \n",
       "4    -0.141959 -0.099903  0.259655  ... -0.130409  0.157641 -0.089053   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "6086 -0.244753  0.029292  0.188979  ... -0.002863  0.135913 -0.003287   \n",
       "6087 -0.403670  0.133510  0.270534  ... -0.015863  0.092178 -0.014462   \n",
       "6088 -0.220690 -0.120597  0.128546  ... -0.052578 -0.008985  0.101729   \n",
       "6089 -0.199022  0.028620  0.208258  ... -0.017471  0.124545  0.067195   \n",
       "6090 -0.263834 -0.167844  0.146753  ... -0.043684  0.019424  0.200846   \n",
       "\n",
       "       emb_393   emb_394   emb_395   emb_396   emb_397   emb_398   emb_399  \n",
       "0    -0.022329 -0.015441  0.146932 -0.002725  0.012273 -0.039030 -0.056550  \n",
       "1     0.030739  0.027956  0.027284 -0.045246  0.081974  0.277017 -0.061442  \n",
       "2    -0.018767  0.018342  0.047074 -0.030460  0.030900  0.130267 -0.053380  \n",
       "3    -0.021828 -0.026521  0.106014 -0.014085  0.052972  0.203015  0.036189  \n",
       "4     0.005738 -0.076420  0.065658  0.050418 -0.017438  0.026353  0.070412  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "6086 -0.042199 -0.000557  0.009207  0.026914  0.036417  0.080097  0.097813  \n",
       "6087 -0.070922 -0.032325 -0.044633 -0.063741  0.016771 -0.102552  0.101940  \n",
       "6088  0.041420  0.008364 -0.036131 -0.053305  0.049233  0.275304 -0.082848  \n",
       "6089 -0.006884 -0.018684 -0.161837 -0.021244  0.028774  0.032378  0.068750  \n",
       "6090  0.035497 -0.044247 -0.038127 -0.035668  0.107089  0.342910 -0.255305  \n",
       "\n",
       "[6091 rows x 400 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.text.all import *\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text_embeddings(learner, dl):\n",
    "    texts = dl.dataset.items['text'].tolist()  # Extract text column from DataFrame\n",
    "    embeddings = []\n",
    "\n",
    "    # Get the tokenizer and numericalizer from the DataLoaders\n",
    "    tokenizer = dl.dataset.tfms[0].tokenizer\n",
    "    numericalizer = dl.numericalize\n",
    "\n",
    "    # Access the embedding layer from the AWD_LSTM model\n",
    "    embedding_layer = learner.model[0].module.encoder\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize the text\n",
    "        tokens = tokenizer([text])[0]\n",
    "        # Numericalize the tokens\n",
    "        numericalized = tensor(numericalizer(tokens)).unsqueeze(0).to(learner.dls.device)\n",
    "\n",
    "        # Get embeddings using the embedding layer\n",
    "        with torch.no_grad():\n",
    "            embeddings_tensor = embedding_layer(numericalized)\n",
    "            embedding = embeddings_tensor.mean(dim=1).cpu().numpy()  # Mean pooling over the sequence length\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    # Convert embeddings to DataFrame\n",
    "    embedding_df = pd.DataFrame(np.vstack(embeddings), columns=[f'emb_{i}' for i in range(embeddings[0].shape[1])])\n",
    "    return embedding_df\n",
    "\n",
    "# Example usage\n",
    "# Assuming learn is your learner object and dls is your DataLoaders\n",
    "embedded_text_train_df = extract_text_embeddings(learn, dls.train)\n",
    "embedded_text_valid_df = extract_text_embeddings(learn, dls.valid)\n",
    "\n",
    "# Verify the shape of the resulting DataFrames\n",
    "print(embedded_text_train_df.shape, embedded_text_valid_df.shape)\n",
    "embedded_text_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6091, 1522)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(learn.dls.train.dataset.items), len(learn.dls.valid.dataset.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>emb_9</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_390</th>\n",
       "      <th>emb_391</th>\n",
       "      <th>emb_392</th>\n",
       "      <th>emb_393</th>\n",
       "      <th>emb_394</th>\n",
       "      <th>emb_395</th>\n",
       "      <th>emb_396</th>\n",
       "      <th>emb_397</th>\n",
       "      <th>emb_398</th>\n",
       "      <th>emb_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.034183</td>\n",
       "      <td>-0.028445</td>\n",
       "      <td>0.026426</td>\n",
       "      <td>0.070351</td>\n",
       "      <td>-0.063002</td>\n",
       "      <td>0.078609</td>\n",
       "      <td>-0.019157</td>\n",
       "      <td>-0.263661</td>\n",
       "      <td>-0.056502</td>\n",
       "      <td>0.060781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029279</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>-0.022329</td>\n",
       "      <td>-0.015441</td>\n",
       "      <td>0.146932</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>0.012273</td>\n",
       "      <td>-0.039030</td>\n",
       "      <td>-0.056550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.007141</td>\n",
       "      <td>0.102240</td>\n",
       "      <td>0.077724</td>\n",
       "      <td>0.108877</td>\n",
       "      <td>-0.137661</td>\n",
       "      <td>0.252389</td>\n",
       "      <td>0.178327</td>\n",
       "      <td>-0.303554</td>\n",
       "      <td>-0.056530</td>\n",
       "      <td>0.218585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027753</td>\n",
       "      <td>0.096219</td>\n",
       "      <td>0.143482</td>\n",
       "      <td>0.030739</td>\n",
       "      <td>0.027956</td>\n",
       "      <td>0.027284</td>\n",
       "      <td>-0.045246</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.277017</td>\n",
       "      <td>-0.061442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.085716</td>\n",
       "      <td>0.040413</td>\n",
       "      <td>-0.011065</td>\n",
       "      <td>0.076396</td>\n",
       "      <td>-0.062790</td>\n",
       "      <td>0.072354</td>\n",
       "      <td>0.130806</td>\n",
       "      <td>-0.230223</td>\n",
       "      <td>-0.025018</td>\n",
       "      <td>0.145530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069291</td>\n",
       "      <td>0.092060</td>\n",
       "      <td>0.037058</td>\n",
       "      <td>-0.018767</td>\n",
       "      <td>0.018342</td>\n",
       "      <td>0.047074</td>\n",
       "      <td>-0.030460</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.130267</td>\n",
       "      <td>-0.053380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.086707</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>-0.027758</td>\n",
       "      <td>0.063932</td>\n",
       "      <td>-0.095097</td>\n",
       "      <td>0.111422</td>\n",
       "      <td>0.115529</td>\n",
       "      <td>-0.280657</td>\n",
       "      <td>-0.011152</td>\n",
       "      <td>0.288354</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016040</td>\n",
       "      <td>0.120837</td>\n",
       "      <td>0.021732</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>0.106014</td>\n",
       "      <td>-0.014085</td>\n",
       "      <td>0.052972</td>\n",
       "      <td>0.203015</td>\n",
       "      <td>0.036189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022853</td>\n",
       "      <td>0.020545</td>\n",
       "      <td>0.033012</td>\n",
       "      <td>0.096078</td>\n",
       "      <td>-0.058596</td>\n",
       "      <td>0.077099</td>\n",
       "      <td>0.073410</td>\n",
       "      <td>-0.141959</td>\n",
       "      <td>-0.099903</td>\n",
       "      <td>0.259655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130409</td>\n",
       "      <td>0.157641</td>\n",
       "      <td>-0.089053</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>-0.076420</td>\n",
       "      <td>0.065658</td>\n",
       "      <td>0.050418</td>\n",
       "      <td>-0.017438</td>\n",
       "      <td>0.026353</td>\n",
       "      <td>0.070412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emb_0     emb_1     emb_2     emb_3     emb_4     emb_5     emb_6  \\\n",
       "0 -0.034183 -0.028445  0.026426  0.070351 -0.063002  0.078609 -0.019157   \n",
       "1 -0.007141  0.102240  0.077724  0.108877 -0.137661  0.252389  0.178327   \n",
       "2 -0.085716  0.040413 -0.011065  0.076396 -0.062790  0.072354  0.130806   \n",
       "3 -0.086707  0.019600 -0.027758  0.063932 -0.095097  0.111422  0.115529   \n",
       "4  0.022853  0.020545  0.033012  0.096078 -0.058596  0.077099  0.073410   \n",
       "\n",
       "      emb_7     emb_8     emb_9  ...   emb_390   emb_391   emb_392   emb_393  \\\n",
       "0 -0.263661 -0.056502  0.060781  ... -0.029279  0.030145  0.000772 -0.022329   \n",
       "1 -0.303554 -0.056530  0.218585  ... -0.027753  0.096219  0.143482  0.030739   \n",
       "2 -0.230223 -0.025018  0.145530  ... -0.069291  0.092060  0.037058 -0.018767   \n",
       "3 -0.280657 -0.011152  0.288354  ... -0.016040  0.120837  0.021732 -0.021828   \n",
       "4 -0.141959 -0.099903  0.259655  ... -0.130409  0.157641 -0.089053  0.005738   \n",
       "\n",
       "    emb_394   emb_395   emb_396   emb_397   emb_398   emb_399  \n",
       "0 -0.015441  0.146932 -0.002725  0.012273 -0.039030 -0.056550  \n",
       "1  0.027956  0.027284 -0.045246  0.081974  0.277017 -0.061442  \n",
       "2  0.018342  0.047074 -0.030460  0.030900  0.130267 -0.053380  \n",
       "3 -0.026521  0.106014 -0.014085  0.052972  0.203015  0.036189  \n",
       "4 -0.076420  0.065658  0.050418 -0.017438  0.026353  0.070412  \n",
       "\n",
       "[5 rows x 400 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_text_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 6091, 1522)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(embedded_text_train_df), len(embedded_text_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>emb_9</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_390</th>\n",
       "      <th>emb_391</th>\n",
       "      <th>emb_392</th>\n",
       "      <th>emb_393</th>\n",
       "      <th>emb_394</th>\n",
       "      <th>emb_395</th>\n",
       "      <th>emb_396</th>\n",
       "      <th>emb_397</th>\n",
       "      <th>emb_398</th>\n",
       "      <th>emb_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.034183</td>\n",
       "      <td>-0.028445</td>\n",
       "      <td>0.026426</td>\n",
       "      <td>0.070351</td>\n",
       "      <td>-0.063002</td>\n",
       "      <td>0.078609</td>\n",
       "      <td>-0.019157</td>\n",
       "      <td>-0.263661</td>\n",
       "      <td>-0.056502</td>\n",
       "      <td>0.060781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029279</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>-0.022329</td>\n",
       "      <td>-0.015441</td>\n",
       "      <td>0.146932</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>0.012273</td>\n",
       "      <td>-0.039030</td>\n",
       "      <td>-0.056550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.007141</td>\n",
       "      <td>0.102240</td>\n",
       "      <td>0.077724</td>\n",
       "      <td>0.108877</td>\n",
       "      <td>-0.137661</td>\n",
       "      <td>0.252389</td>\n",
       "      <td>0.178327</td>\n",
       "      <td>-0.303554</td>\n",
       "      <td>-0.056530</td>\n",
       "      <td>0.218585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027753</td>\n",
       "      <td>0.096219</td>\n",
       "      <td>0.143482</td>\n",
       "      <td>0.030739</td>\n",
       "      <td>0.027956</td>\n",
       "      <td>0.027284</td>\n",
       "      <td>-0.045246</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.277017</td>\n",
       "      <td>-0.061442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.085716</td>\n",
       "      <td>0.040413</td>\n",
       "      <td>-0.011065</td>\n",
       "      <td>0.076396</td>\n",
       "      <td>-0.062790</td>\n",
       "      <td>0.072354</td>\n",
       "      <td>0.130806</td>\n",
       "      <td>-0.230223</td>\n",
       "      <td>-0.025018</td>\n",
       "      <td>0.145530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069291</td>\n",
       "      <td>0.092060</td>\n",
       "      <td>0.037058</td>\n",
       "      <td>-0.018767</td>\n",
       "      <td>0.018342</td>\n",
       "      <td>0.047074</td>\n",
       "      <td>-0.030460</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.130267</td>\n",
       "      <td>-0.053380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.086707</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>-0.027758</td>\n",
       "      <td>0.063932</td>\n",
       "      <td>-0.095097</td>\n",
       "      <td>0.111422</td>\n",
       "      <td>0.115529</td>\n",
       "      <td>-0.280657</td>\n",
       "      <td>-0.011152</td>\n",
       "      <td>0.288354</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016040</td>\n",
       "      <td>0.120837</td>\n",
       "      <td>0.021732</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>0.106014</td>\n",
       "      <td>-0.014085</td>\n",
       "      <td>0.052972</td>\n",
       "      <td>0.203015</td>\n",
       "      <td>0.036189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022853</td>\n",
       "      <td>0.020545</td>\n",
       "      <td>0.033012</td>\n",
       "      <td>0.096078</td>\n",
       "      <td>-0.058596</td>\n",
       "      <td>0.077099</td>\n",
       "      <td>0.073410</td>\n",
       "      <td>-0.141959</td>\n",
       "      <td>-0.099903</td>\n",
       "      <td>0.259655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130409</td>\n",
       "      <td>0.157641</td>\n",
       "      <td>-0.089053</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>-0.076420</td>\n",
       "      <td>0.065658</td>\n",
       "      <td>0.050418</td>\n",
       "      <td>-0.017438</td>\n",
       "      <td>0.026353</td>\n",
       "      <td>0.070412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emb_0     emb_1     emb_2     emb_3     emb_4     emb_5     emb_6  \\\n",
       "0 -0.034183 -0.028445  0.026426  0.070351 -0.063002  0.078609 -0.019157   \n",
       "1 -0.007141  0.102240  0.077724  0.108877 -0.137661  0.252389  0.178327   \n",
       "2 -0.085716  0.040413 -0.011065  0.076396 -0.062790  0.072354  0.130806   \n",
       "3 -0.086707  0.019600 -0.027758  0.063932 -0.095097  0.111422  0.115529   \n",
       "4  0.022853  0.020545  0.033012  0.096078 -0.058596  0.077099  0.073410   \n",
       "\n",
       "      emb_7     emb_8     emb_9  ...   emb_390   emb_391   emb_392   emb_393  \\\n",
       "0 -0.263661 -0.056502  0.060781  ... -0.029279  0.030145  0.000772 -0.022329   \n",
       "1 -0.303554 -0.056530  0.218585  ... -0.027753  0.096219  0.143482  0.030739   \n",
       "2 -0.230223 -0.025018  0.145530  ... -0.069291  0.092060  0.037058 -0.018767   \n",
       "3 -0.280657 -0.011152  0.288354  ... -0.016040  0.120837  0.021732 -0.021828   \n",
       "4 -0.141959 -0.099903  0.259655  ... -0.130409  0.157641 -0.089053  0.005738   \n",
       "\n",
       "    emb_394   emb_395   emb_396   emb_397   emb_398   emb_399  \n",
       "0 -0.015441  0.146932 -0.002725  0.012273 -0.039030 -0.056550  \n",
       "1  0.027956  0.027284 -0.045246  0.081974  0.277017 -0.061442  \n",
       "2  0.018342  0.047074 -0.030460  0.030900  0.130267 -0.053380  \n",
       "3 -0.026521  0.106014 -0.014085  0.052972  0.203015  0.036189  \n",
       "4 -0.076420  0.065658  0.050418 -0.017438  0.026353  0.070412  \n",
       "\n",
       "[5 rows x 400 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_text_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>emb_9</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_390</th>\n",
       "      <th>emb_391</th>\n",
       "      <th>emb_392</th>\n",
       "      <th>emb_393</th>\n",
       "      <th>emb_394</th>\n",
       "      <th>emb_395</th>\n",
       "      <th>emb_396</th>\n",
       "      <th>emb_397</th>\n",
       "      <th>emb_398</th>\n",
       "      <th>emb_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.034183</td>\n",
       "      <td>-0.028445</td>\n",
       "      <td>0.026426</td>\n",
       "      <td>0.070351</td>\n",
       "      <td>-0.063002</td>\n",
       "      <td>0.078609</td>\n",
       "      <td>-0.019157</td>\n",
       "      <td>-0.263661</td>\n",
       "      <td>-0.056502</td>\n",
       "      <td>0.060781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029279</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>-0.022329</td>\n",
       "      <td>-0.015441</td>\n",
       "      <td>0.146932</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>0.012273</td>\n",
       "      <td>-0.039030</td>\n",
       "      <td>-0.056550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.007141</td>\n",
       "      <td>0.102240</td>\n",
       "      <td>0.077724</td>\n",
       "      <td>0.108877</td>\n",
       "      <td>-0.137661</td>\n",
       "      <td>0.252389</td>\n",
       "      <td>0.178327</td>\n",
       "      <td>-0.303554</td>\n",
       "      <td>-0.056530</td>\n",
       "      <td>0.218585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027753</td>\n",
       "      <td>0.096219</td>\n",
       "      <td>0.143482</td>\n",
       "      <td>0.030739</td>\n",
       "      <td>0.027956</td>\n",
       "      <td>0.027284</td>\n",
       "      <td>-0.045246</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.277017</td>\n",
       "      <td>-0.061442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.085716</td>\n",
       "      <td>0.040413</td>\n",
       "      <td>-0.011065</td>\n",
       "      <td>0.076396</td>\n",
       "      <td>-0.062790</td>\n",
       "      <td>0.072354</td>\n",
       "      <td>0.130806</td>\n",
       "      <td>-0.230223</td>\n",
       "      <td>-0.025018</td>\n",
       "      <td>0.145530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069291</td>\n",
       "      <td>0.092060</td>\n",
       "      <td>0.037058</td>\n",
       "      <td>-0.018767</td>\n",
       "      <td>0.018342</td>\n",
       "      <td>0.047074</td>\n",
       "      <td>-0.030460</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.130267</td>\n",
       "      <td>-0.053380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.086707</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>-0.027758</td>\n",
       "      <td>0.063932</td>\n",
       "      <td>-0.095097</td>\n",
       "      <td>0.111422</td>\n",
       "      <td>0.115529</td>\n",
       "      <td>-0.280657</td>\n",
       "      <td>-0.011152</td>\n",
       "      <td>0.288354</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016040</td>\n",
       "      <td>0.120837</td>\n",
       "      <td>0.021732</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>0.106014</td>\n",
       "      <td>-0.014085</td>\n",
       "      <td>0.052972</td>\n",
       "      <td>0.203015</td>\n",
       "      <td>0.036189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022853</td>\n",
       "      <td>0.020545</td>\n",
       "      <td>0.033012</td>\n",
       "      <td>0.096078</td>\n",
       "      <td>-0.058596</td>\n",
       "      <td>0.077099</td>\n",
       "      <td>0.073410</td>\n",
       "      <td>-0.141959</td>\n",
       "      <td>-0.099903</td>\n",
       "      <td>0.259655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130409</td>\n",
       "      <td>0.157641</td>\n",
       "      <td>-0.089053</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>-0.076420</td>\n",
       "      <td>0.065658</td>\n",
       "      <td>0.050418</td>\n",
       "      <td>-0.017438</td>\n",
       "      <td>0.026353</td>\n",
       "      <td>0.070412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>0.082430</td>\n",
       "      <td>0.036495</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>-0.079622</td>\n",
       "      <td>0.057562</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>-0.196848</td>\n",
       "      <td>-0.062231</td>\n",
       "      <td>0.092967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110739</td>\n",
       "      <td>0.072549</td>\n",
       "      <td>0.115268</td>\n",
       "      <td>-0.087309</td>\n",
       "      <td>-0.105231</td>\n",
       "      <td>0.015131</td>\n",
       "      <td>-0.014992</td>\n",
       "      <td>0.042365</td>\n",
       "      <td>0.062360</td>\n",
       "      <td>0.028131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>0.105432</td>\n",
       "      <td>-0.025051</td>\n",
       "      <td>0.024132</td>\n",
       "      <td>0.075988</td>\n",
       "      <td>-0.043869</td>\n",
       "      <td>0.193176</td>\n",
       "      <td>0.140545</td>\n",
       "      <td>-0.266746</td>\n",
       "      <td>-0.085535</td>\n",
       "      <td>0.094690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016935</td>\n",
       "      <td>0.083398</td>\n",
       "      <td>0.131557</td>\n",
       "      <td>0.030973</td>\n",
       "      <td>0.049750</td>\n",
       "      <td>-0.120442</td>\n",
       "      <td>-0.051325</td>\n",
       "      <td>0.115229</td>\n",
       "      <td>0.097282</td>\n",
       "      <td>-0.131696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>0.036151</td>\n",
       "      <td>0.014805</td>\n",
       "      <td>-0.015719</td>\n",
       "      <td>0.086286</td>\n",
       "      <td>0.053458</td>\n",
       "      <td>0.032042</td>\n",
       "      <td>0.037615</td>\n",
       "      <td>-0.216149</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.177951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037571</td>\n",
       "      <td>0.217078</td>\n",
       "      <td>0.008346</td>\n",
       "      <td>-0.013909</td>\n",
       "      <td>-0.031338</td>\n",
       "      <td>-0.020781</td>\n",
       "      <td>-0.021993</td>\n",
       "      <td>-0.021915</td>\n",
       "      <td>-0.143232</td>\n",
       "      <td>0.038254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>-0.068939</td>\n",
       "      <td>-0.061271</td>\n",
       "      <td>0.078244</td>\n",
       "      <td>0.070861</td>\n",
       "      <td>-0.005734</td>\n",
       "      <td>0.065245</td>\n",
       "      <td>0.131465</td>\n",
       "      <td>-0.213985</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>0.253771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073032</td>\n",
       "      <td>0.129164</td>\n",
       "      <td>0.033144</td>\n",
       "      <td>-0.015675</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.079133</td>\n",
       "      <td>-0.019993</td>\n",
       "      <td>0.009030</td>\n",
       "      <td>0.072349</td>\n",
       "      <td>0.129819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>0.108923</td>\n",
       "      <td>-0.056835</td>\n",
       "      <td>-0.110851</td>\n",
       "      <td>-0.102374</td>\n",
       "      <td>0.087767</td>\n",
       "      <td>0.086956</td>\n",
       "      <td>-0.014679</td>\n",
       "      <td>-0.173234</td>\n",
       "      <td>0.167506</td>\n",
       "      <td>0.136231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019031</td>\n",
       "      <td>0.150610</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>-0.038958</td>\n",
       "      <td>-0.053582</td>\n",
       "      <td>-0.044347</td>\n",
       "      <td>0.055916</td>\n",
       "      <td>-0.052702</td>\n",
       "      <td>0.075153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         emb_0     emb_1     emb_2     emb_3     emb_4     emb_5     emb_6  \\\n",
       "0    -0.034183 -0.028445  0.026426  0.070351 -0.063002  0.078609 -0.019157   \n",
       "1    -0.007141  0.102240  0.077724  0.108877 -0.137661  0.252389  0.178327   \n",
       "2    -0.085716  0.040413 -0.011065  0.076396 -0.062790  0.072354  0.130806   \n",
       "3    -0.086707  0.019600 -0.027758  0.063932 -0.095097  0.111422  0.115529   \n",
       "4     0.022853  0.020545  0.033012  0.096078 -0.058596  0.077099  0.073410   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1517  0.082430  0.036495 -0.000544  0.049103 -0.079622  0.057562 -0.000563   \n",
       "1518  0.105432 -0.025051  0.024132  0.075988 -0.043869  0.193176  0.140545   \n",
       "1519  0.036151  0.014805 -0.015719  0.086286  0.053458  0.032042  0.037615   \n",
       "1520 -0.068939 -0.061271  0.078244  0.070861 -0.005734  0.065245  0.131465   \n",
       "1521  0.108923 -0.056835 -0.110851 -0.102374  0.087767  0.086956 -0.014679   \n",
       "\n",
       "         emb_7     emb_8     emb_9  ...   emb_390   emb_391   emb_392  \\\n",
       "0    -0.263661 -0.056502  0.060781  ... -0.029279  0.030145  0.000772   \n",
       "1    -0.303554 -0.056530  0.218585  ... -0.027753  0.096219  0.143482   \n",
       "2    -0.230223 -0.025018  0.145530  ... -0.069291  0.092060  0.037058   \n",
       "3    -0.280657 -0.011152  0.288354  ... -0.016040  0.120837  0.021732   \n",
       "4    -0.141959 -0.099903  0.259655  ... -0.130409  0.157641 -0.089053   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1517 -0.196848 -0.062231  0.092967  ...  0.110739  0.072549  0.115268   \n",
       "1518 -0.266746 -0.085535  0.094690  ... -0.016935  0.083398  0.131557   \n",
       "1519 -0.216149  0.055952  0.177951  ... -0.037571  0.217078  0.008346   \n",
       "1520 -0.213985  0.015492  0.253771  ... -0.073032  0.129164  0.033144   \n",
       "1521 -0.173234  0.167506  0.136231  ... -0.019031  0.150610  0.003819   \n",
       "\n",
       "       emb_393   emb_394   emb_395   emb_396   emb_397   emb_398   emb_399  \n",
       "0    -0.022329 -0.015441  0.146932 -0.002725  0.012273 -0.039030 -0.056550  \n",
       "1     0.030739  0.027956  0.027284 -0.045246  0.081974  0.277017 -0.061442  \n",
       "2    -0.018767  0.018342  0.047074 -0.030460  0.030900  0.130267 -0.053380  \n",
       "3    -0.021828 -0.026521  0.106014 -0.014085  0.052972  0.203015  0.036189  \n",
       "4     0.005738 -0.076420  0.065658  0.050418 -0.017438  0.026353  0.070412  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1517 -0.087309 -0.105231  0.015131 -0.014992  0.042365  0.062360  0.028131  \n",
       "1518  0.030973  0.049750 -0.120442 -0.051325  0.115229  0.097282 -0.131696  \n",
       "1519 -0.013909 -0.031338 -0.020781 -0.021993 -0.021915 -0.143232  0.038254  \n",
       "1520 -0.015675  0.008714  0.079133 -0.019993  0.009030  0.072349  0.129819  \n",
       "1521  0.025230 -0.038958 -0.053582 -0.044347  0.055916 -0.052702  0.075153  \n",
       "\n",
       "[7613 rows x 400 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_text_df = pd.concat([embedded_text_train_df, embedded_text_valid_df])\n",
    "embedded_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 7613)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(embedded_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_390</th>\n",
       "      <th>emb_391</th>\n",
       "      <th>emb_392</th>\n",
       "      <th>emb_393</th>\n",
       "      <th>emb_394</th>\n",
       "      <th>emb_395</th>\n",
       "      <th>emb_396</th>\n",
       "      <th>emb_397</th>\n",
       "      <th>emb_398</th>\n",
       "      <th>emb_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.034183</td>\n",
       "      <td>-0.028445</td>\n",
       "      <td>0.026426</td>\n",
       "      <td>0.070351</td>\n",
       "      <td>-0.063002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029279</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>-0.022329</td>\n",
       "      <td>-0.015441</td>\n",
       "      <td>0.146932</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>0.012273</td>\n",
       "      <td>-0.039030</td>\n",
       "      <td>-0.056550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.007141</td>\n",
       "      <td>0.102240</td>\n",
       "      <td>0.077724</td>\n",
       "      <td>0.108877</td>\n",
       "      <td>-0.137661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027753</td>\n",
       "      <td>0.096219</td>\n",
       "      <td>0.143482</td>\n",
       "      <td>0.030739</td>\n",
       "      <td>0.027956</td>\n",
       "      <td>0.027284</td>\n",
       "      <td>-0.045246</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.277017</td>\n",
       "      <td>-0.061442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.085716</td>\n",
       "      <td>0.040413</td>\n",
       "      <td>-0.011065</td>\n",
       "      <td>0.076396</td>\n",
       "      <td>-0.062790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069291</td>\n",
       "      <td>0.092060</td>\n",
       "      <td>0.037058</td>\n",
       "      <td>-0.018767</td>\n",
       "      <td>0.018342</td>\n",
       "      <td>0.047074</td>\n",
       "      <td>-0.030460</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.130267</td>\n",
       "      <td>-0.053380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.086707</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>-0.027758</td>\n",
       "      <td>0.063932</td>\n",
       "      <td>-0.095097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016040</td>\n",
       "      <td>0.120837</td>\n",
       "      <td>0.021732</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>0.106014</td>\n",
       "      <td>-0.014085</td>\n",
       "      <td>0.052972</td>\n",
       "      <td>0.203015</td>\n",
       "      <td>0.036189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022853</td>\n",
       "      <td>0.020545</td>\n",
       "      <td>0.033012</td>\n",
       "      <td>0.096078</td>\n",
       "      <td>-0.058596</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130409</td>\n",
       "      <td>0.157641</td>\n",
       "      <td>-0.089053</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>-0.076420</td>\n",
       "      <td>0.065658</td>\n",
       "      <td>0.050418</td>\n",
       "      <td>-0.017438</td>\n",
       "      <td>0.026353</td>\n",
       "      <td>0.070412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.082430</td>\n",
       "      <td>0.036495</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>-0.079622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110739</td>\n",
       "      <td>0.072549</td>\n",
       "      <td>0.115268</td>\n",
       "      <td>-0.087309</td>\n",
       "      <td>-0.105231</td>\n",
       "      <td>0.015131</td>\n",
       "      <td>-0.014992</td>\n",
       "      <td>0.042365</td>\n",
       "      <td>0.062360</td>\n",
       "      <td>0.028131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105432</td>\n",
       "      <td>-0.025051</td>\n",
       "      <td>0.024132</td>\n",
       "      <td>0.075988</td>\n",
       "      <td>-0.043869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016935</td>\n",
       "      <td>0.083398</td>\n",
       "      <td>0.131557</td>\n",
       "      <td>0.030973</td>\n",
       "      <td>0.049750</td>\n",
       "      <td>-0.120442</td>\n",
       "      <td>-0.051325</td>\n",
       "      <td>0.115229</td>\n",
       "      <td>0.097282</td>\n",
       "      <td>-0.131696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036151</td>\n",
       "      <td>0.014805</td>\n",
       "      <td>-0.015719</td>\n",
       "      <td>0.086286</td>\n",
       "      <td>0.053458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037571</td>\n",
       "      <td>0.217078</td>\n",
       "      <td>0.008346</td>\n",
       "      <td>-0.013909</td>\n",
       "      <td>-0.031338</td>\n",
       "      <td>-0.020781</td>\n",
       "      <td>-0.021993</td>\n",
       "      <td>-0.021915</td>\n",
       "      <td>-0.143232</td>\n",
       "      <td>0.038254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.068939</td>\n",
       "      <td>-0.061271</td>\n",
       "      <td>0.078244</td>\n",
       "      <td>0.070861</td>\n",
       "      <td>-0.005734</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073032</td>\n",
       "      <td>0.129164</td>\n",
       "      <td>0.033144</td>\n",
       "      <td>-0.015675</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.079133</td>\n",
       "      <td>-0.019993</td>\n",
       "      <td>0.009030</td>\n",
       "      <td>0.072349</td>\n",
       "      <td>0.129819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d</td>\n",
       "      <td>1</td>\n",
       "      <td>0.108923</td>\n",
       "      <td>-0.056835</td>\n",
       "      <td>-0.110851</td>\n",
       "      <td>-0.102374</td>\n",
       "      <td>0.087767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019031</td>\n",
       "      <td>0.150610</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>-0.038958</td>\n",
       "      <td>-0.053582</td>\n",
       "      <td>-0.044347</td>\n",
       "      <td>0.055916</td>\n",
       "      <td>-0.052702</td>\n",
       "      <td>0.075153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 405 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                                                                         Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                        Forest fire near La Ronge Sask. Canada   \n",
       "2         All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                             13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                                      Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "...                                                                                                                                         ...   \n",
       "7608                                                        Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5   \n",
       "7609              @aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.   \n",
       "7610                                                                          M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ   \n",
       "7611  Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.   \n",
       "7612                                             The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d   \n",
       "\n",
       "      target     emb_0     emb_1     emb_2     emb_3     emb_4  ...   emb_390  \\\n",
       "0          1 -0.034183 -0.028445  0.026426  0.070351 -0.063002  ... -0.029279   \n",
       "1          1 -0.007141  0.102240  0.077724  0.108877 -0.137661  ... -0.027753   \n",
       "2          1 -0.085716  0.040413 -0.011065  0.076396 -0.062790  ... -0.069291   \n",
       "3          1 -0.086707  0.019600 -0.027758  0.063932 -0.095097  ... -0.016040   \n",
       "4          1  0.022853  0.020545  0.033012  0.096078 -0.058596  ... -0.130409   \n",
       "...      ...       ...       ...       ...       ...       ...  ...       ...   \n",
       "7608       1  0.082430  0.036495 -0.000544  0.049103 -0.079622  ...  0.110739   \n",
       "7609       1  0.105432 -0.025051  0.024132  0.075988 -0.043869  ... -0.016935   \n",
       "7610       1  0.036151  0.014805 -0.015719  0.086286  0.053458  ... -0.037571   \n",
       "7611       1 -0.068939 -0.061271  0.078244  0.070861 -0.005734  ... -0.073032   \n",
       "7612       1  0.108923 -0.056835 -0.110851 -0.102374  0.087767  ... -0.019031   \n",
       "\n",
       "       emb_391   emb_392   emb_393   emb_394   emb_395   emb_396   emb_397  \\\n",
       "0     0.030145  0.000772 -0.022329 -0.015441  0.146932 -0.002725  0.012273   \n",
       "1     0.096219  0.143482  0.030739  0.027956  0.027284 -0.045246  0.081974   \n",
       "2     0.092060  0.037058 -0.018767  0.018342  0.047074 -0.030460  0.030900   \n",
       "3     0.120837  0.021732 -0.021828 -0.026521  0.106014 -0.014085  0.052972   \n",
       "4     0.157641 -0.089053  0.005738 -0.076420  0.065658  0.050418 -0.017438   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7608  0.072549  0.115268 -0.087309 -0.105231  0.015131 -0.014992  0.042365   \n",
       "7609  0.083398  0.131557  0.030973  0.049750 -0.120442 -0.051325  0.115229   \n",
       "7610  0.217078  0.008346 -0.013909 -0.031338 -0.020781 -0.021993 -0.021915   \n",
       "7611  0.129164  0.033144 -0.015675  0.008714  0.079133 -0.019993  0.009030   \n",
       "7612  0.150610  0.003819  0.025230 -0.038958 -0.053582 -0.044347  0.055916   \n",
       "\n",
       "       emb_398   emb_399  \n",
       "0    -0.039030 -0.056550  \n",
       "1     0.277017 -0.061442  \n",
       "2     0.130267 -0.053380  \n",
       "3     0.203015  0.036189  \n",
       "4     0.026353  0.070412  \n",
       "...        ...       ...  \n",
       "7608  0.062360  0.028131  \n",
       "7609  0.097282 -0.131696  \n",
       "7610 -0.143232  0.038254  \n",
       "7611  0.072349  0.129819  \n",
       "7612 -0.052702  0.075153  \n",
       "\n",
       "[7613 rows x 405 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df and embedded_text_df are your DataFrames\n",
    "\n",
    "# Reset indices to ensure they are unique and aligned\n",
    "df_reset = df.reset_index(drop=True)\n",
    "embedded_text_df_reset = embedded_text_df.reset_index(drop=True)\n",
    "\n",
    "# Concatenate DataFrames along columns\n",
    "df_with_embeds = pd.concat([df_reset, embedded_text_df_reset], axis=1)\n",
    "\n",
    "# Verify the concatenated DataFrame\n",
    "df_with_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target', 'emb_0', 'emb_1',\n",
       "       'emb_2', 'emb_3', 'emb_4',\n",
       "       ...\n",
       "       'emb_390', 'emb_391', 'emb_392', 'emb_393', 'emb_394', 'emb_395',\n",
       "       'emb_396', 'emb_397', 'emb_398', 'emb_399'],\n",
       "      dtype='object', length=405)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_embeds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TabularPandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m conts\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memb_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m400\u001b[39m)]\n\u001b[1;32m      6\u001b[0m dep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m to \u001b[38;5;241m=\u001b[39m \u001b[43mTabularPandas\u001b[49m(\n\u001b[1;32m      9\u001b[0m     df_with_embeds, splits\u001b[38;5;241m=\u001b[39msplits,\n\u001b[1;32m     10\u001b[0m     procs \u001b[38;5;241m=\u001b[39m [Categorify, FillMissing, Normalize],\n\u001b[1;32m     11\u001b[0m     cat_names\u001b[38;5;241m=\u001b[39mcats,\n\u001b[1;32m     12\u001b[0m     cont_names\u001b[38;5;241m=\u001b[39mconts,\n\u001b[1;32m     13\u001b[0m     y_names\u001b[38;5;241m=\u001b[39mdep,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mlen\u001b[39m(to\u001b[38;5;241m.\u001b[39mtrain),\u001b[38;5;28mlen\u001b[39m(to\u001b[38;5;241m.\u001b[39mvalid)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TabularPandas' is not defined"
     ]
    }
   ],
   "source": [
    "from fastai.data.transforms import *\n",
    "\n",
    "splits = RandomSplitter(0.2)(df_with_embeds)\n",
    "cats=[\"keyword\",\"location\",]\n",
    "conts=[f'emb_{i}' for i in range(400)]\n",
    "dep = 'target'\n",
    "\n",
    "to = TabularPandas(\n",
    "    df_with_embeds, splits=splits,\n",
    "    procs = [Categorify, FillMissing, Normalize],\n",
    "    cat_names=cats,\n",
    "    cont_names=conts,\n",
    "    y_names=dep,\n",
    ")\n",
    "\n",
    "len(to.train),len(to.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>emb_9</th>\n",
       "      <th>emb_10</th>\n",
       "      <th>emb_11</th>\n",
       "      <th>emb_12</th>\n",
       "      <th>emb_13</th>\n",
       "      <th>emb_14</th>\n",
       "      <th>emb_15</th>\n",
       "      <th>emb_16</th>\n",
       "      <th>emb_17</th>\n",
       "      <th>emb_18</th>\n",
       "      <th>emb_19</th>\n",
       "      <th>emb_20</th>\n",
       "      <th>emb_21</th>\n",
       "      <th>emb_22</th>\n",
       "      <th>emb_23</th>\n",
       "      <th>emb_24</th>\n",
       "      <th>emb_25</th>\n",
       "      <th>emb_26</th>\n",
       "      <th>emb_27</th>\n",
       "      <th>emb_28</th>\n",
       "      <th>emb_29</th>\n",
       "      <th>emb_30</th>\n",
       "      <th>emb_31</th>\n",
       "      <th>emb_32</th>\n",
       "      <th>emb_33</th>\n",
       "      <th>emb_34</th>\n",
       "      <th>emb_35</th>\n",
       "      <th>emb_36</th>\n",
       "      <th>emb_37</th>\n",
       "      <th>emb_38</th>\n",
       "      <th>emb_39</th>\n",
       "      <th>emb_40</th>\n",
       "      <th>emb_41</th>\n",
       "      <th>emb_42</th>\n",
       "      <th>emb_43</th>\n",
       "      <th>emb_44</th>\n",
       "      <th>emb_45</th>\n",
       "      <th>emb_46</th>\n",
       "      <th>emb_47</th>\n",
       "      <th>emb_48</th>\n",
       "      <th>emb_49</th>\n",
       "      <th>emb_50</th>\n",
       "      <th>emb_51</th>\n",
       "      <th>emb_52</th>\n",
       "      <th>emb_53</th>\n",
       "      <th>emb_54</th>\n",
       "      <th>emb_55</th>\n",
       "      <th>emb_56</th>\n",
       "      <th>emb_57</th>\n",
       "      <th>emb_58</th>\n",
       "      <th>emb_59</th>\n",
       "      <th>emb_60</th>\n",
       "      <th>emb_61</th>\n",
       "      <th>emb_62</th>\n",
       "      <th>emb_63</th>\n",
       "      <th>emb_64</th>\n",
       "      <th>emb_65</th>\n",
       "      <th>emb_66</th>\n",
       "      <th>emb_67</th>\n",
       "      <th>emb_68</th>\n",
       "      <th>emb_69</th>\n",
       "      <th>emb_70</th>\n",
       "      <th>emb_71</th>\n",
       "      <th>emb_72</th>\n",
       "      <th>emb_73</th>\n",
       "      <th>emb_74</th>\n",
       "      <th>emb_75</th>\n",
       "      <th>emb_76</th>\n",
       "      <th>emb_77</th>\n",
       "      <th>emb_78</th>\n",
       "      <th>emb_79</th>\n",
       "      <th>emb_80</th>\n",
       "      <th>emb_81</th>\n",
       "      <th>emb_82</th>\n",
       "      <th>emb_83</th>\n",
       "      <th>emb_84</th>\n",
       "      <th>emb_85</th>\n",
       "      <th>emb_86</th>\n",
       "      <th>emb_87</th>\n",
       "      <th>emb_88</th>\n",
       "      <th>emb_89</th>\n",
       "      <th>emb_90</th>\n",
       "      <th>emb_91</th>\n",
       "      <th>emb_92</th>\n",
       "      <th>emb_93</th>\n",
       "      <th>emb_94</th>\n",
       "      <th>emb_95</th>\n",
       "      <th>emb_96</th>\n",
       "      <th>emb_97</th>\n",
       "      <th>emb_98</th>\n",
       "      <th>emb_99</th>\n",
       "      <th>emb_100</th>\n",
       "      <th>emb_101</th>\n",
       "      <th>emb_102</th>\n",
       "      <th>emb_103</th>\n",
       "      <th>emb_104</th>\n",
       "      <th>emb_105</th>\n",
       "      <th>emb_106</th>\n",
       "      <th>emb_107</th>\n",
       "      <th>emb_108</th>\n",
       "      <th>emb_109</th>\n",
       "      <th>emb_110</th>\n",
       "      <th>emb_111</th>\n",
       "      <th>emb_112</th>\n",
       "      <th>emb_113</th>\n",
       "      <th>emb_114</th>\n",
       "      <th>emb_115</th>\n",
       "      <th>emb_116</th>\n",
       "      <th>emb_117</th>\n",
       "      <th>emb_118</th>\n",
       "      <th>emb_119</th>\n",
       "      <th>emb_120</th>\n",
       "      <th>emb_121</th>\n",
       "      <th>emb_122</th>\n",
       "      <th>emb_123</th>\n",
       "      <th>emb_124</th>\n",
       "      <th>emb_125</th>\n",
       "      <th>emb_126</th>\n",
       "      <th>emb_127</th>\n",
       "      <th>emb_128</th>\n",
       "      <th>emb_129</th>\n",
       "      <th>emb_130</th>\n",
       "      <th>emb_131</th>\n",
       "      <th>emb_132</th>\n",
       "      <th>emb_133</th>\n",
       "      <th>emb_134</th>\n",
       "      <th>emb_135</th>\n",
       "      <th>emb_136</th>\n",
       "      <th>emb_137</th>\n",
       "      <th>emb_138</th>\n",
       "      <th>emb_139</th>\n",
       "      <th>emb_140</th>\n",
       "      <th>emb_141</th>\n",
       "      <th>emb_142</th>\n",
       "      <th>emb_143</th>\n",
       "      <th>emb_144</th>\n",
       "      <th>emb_145</th>\n",
       "      <th>emb_146</th>\n",
       "      <th>emb_147</th>\n",
       "      <th>emb_148</th>\n",
       "      <th>emb_149</th>\n",
       "      <th>emb_150</th>\n",
       "      <th>emb_151</th>\n",
       "      <th>emb_152</th>\n",
       "      <th>emb_153</th>\n",
       "      <th>emb_154</th>\n",
       "      <th>emb_155</th>\n",
       "      <th>emb_156</th>\n",
       "      <th>emb_157</th>\n",
       "      <th>emb_158</th>\n",
       "      <th>emb_159</th>\n",
       "      <th>emb_160</th>\n",
       "      <th>emb_161</th>\n",
       "      <th>emb_162</th>\n",
       "      <th>emb_163</th>\n",
       "      <th>emb_164</th>\n",
       "      <th>emb_165</th>\n",
       "      <th>emb_166</th>\n",
       "      <th>emb_167</th>\n",
       "      <th>emb_168</th>\n",
       "      <th>emb_169</th>\n",
       "      <th>emb_170</th>\n",
       "      <th>emb_171</th>\n",
       "      <th>emb_172</th>\n",
       "      <th>emb_173</th>\n",
       "      <th>emb_174</th>\n",
       "      <th>emb_175</th>\n",
       "      <th>emb_176</th>\n",
       "      <th>emb_177</th>\n",
       "      <th>emb_178</th>\n",
       "      <th>emb_179</th>\n",
       "      <th>emb_180</th>\n",
       "      <th>emb_181</th>\n",
       "      <th>emb_182</th>\n",
       "      <th>emb_183</th>\n",
       "      <th>emb_184</th>\n",
       "      <th>emb_185</th>\n",
       "      <th>emb_186</th>\n",
       "      <th>emb_187</th>\n",
       "      <th>emb_188</th>\n",
       "      <th>emb_189</th>\n",
       "      <th>emb_190</th>\n",
       "      <th>emb_191</th>\n",
       "      <th>emb_192</th>\n",
       "      <th>emb_193</th>\n",
       "      <th>emb_194</th>\n",
       "      <th>emb_195</th>\n",
       "      <th>emb_196</th>\n",
       "      <th>emb_197</th>\n",
       "      <th>emb_198</th>\n",
       "      <th>emb_199</th>\n",
       "      <th>emb_200</th>\n",
       "      <th>emb_201</th>\n",
       "      <th>emb_202</th>\n",
       "      <th>emb_203</th>\n",
       "      <th>emb_204</th>\n",
       "      <th>emb_205</th>\n",
       "      <th>emb_206</th>\n",
       "      <th>emb_207</th>\n",
       "      <th>emb_208</th>\n",
       "      <th>emb_209</th>\n",
       "      <th>emb_210</th>\n",
       "      <th>emb_211</th>\n",
       "      <th>emb_212</th>\n",
       "      <th>emb_213</th>\n",
       "      <th>emb_214</th>\n",
       "      <th>emb_215</th>\n",
       "      <th>emb_216</th>\n",
       "      <th>emb_217</th>\n",
       "      <th>emb_218</th>\n",
       "      <th>emb_219</th>\n",
       "      <th>emb_220</th>\n",
       "      <th>emb_221</th>\n",
       "      <th>emb_222</th>\n",
       "      <th>emb_223</th>\n",
       "      <th>emb_224</th>\n",
       "      <th>emb_225</th>\n",
       "      <th>emb_226</th>\n",
       "      <th>emb_227</th>\n",
       "      <th>emb_228</th>\n",
       "      <th>emb_229</th>\n",
       "      <th>emb_230</th>\n",
       "      <th>emb_231</th>\n",
       "      <th>emb_232</th>\n",
       "      <th>emb_233</th>\n",
       "      <th>emb_234</th>\n",
       "      <th>emb_235</th>\n",
       "      <th>emb_236</th>\n",
       "      <th>emb_237</th>\n",
       "      <th>emb_238</th>\n",
       "      <th>emb_239</th>\n",
       "      <th>emb_240</th>\n",
       "      <th>emb_241</th>\n",
       "      <th>emb_242</th>\n",
       "      <th>emb_243</th>\n",
       "      <th>emb_244</th>\n",
       "      <th>emb_245</th>\n",
       "      <th>emb_246</th>\n",
       "      <th>emb_247</th>\n",
       "      <th>emb_248</th>\n",
       "      <th>emb_249</th>\n",
       "      <th>emb_250</th>\n",
       "      <th>emb_251</th>\n",
       "      <th>emb_252</th>\n",
       "      <th>emb_253</th>\n",
       "      <th>emb_254</th>\n",
       "      <th>emb_255</th>\n",
       "      <th>emb_256</th>\n",
       "      <th>emb_257</th>\n",
       "      <th>emb_258</th>\n",
       "      <th>emb_259</th>\n",
       "      <th>emb_260</th>\n",
       "      <th>emb_261</th>\n",
       "      <th>emb_262</th>\n",
       "      <th>emb_263</th>\n",
       "      <th>emb_264</th>\n",
       "      <th>emb_265</th>\n",
       "      <th>emb_266</th>\n",
       "      <th>emb_267</th>\n",
       "      <th>emb_268</th>\n",
       "      <th>emb_269</th>\n",
       "      <th>emb_270</th>\n",
       "      <th>emb_271</th>\n",
       "      <th>emb_272</th>\n",
       "      <th>emb_273</th>\n",
       "      <th>emb_274</th>\n",
       "      <th>emb_275</th>\n",
       "      <th>emb_276</th>\n",
       "      <th>emb_277</th>\n",
       "      <th>emb_278</th>\n",
       "      <th>emb_279</th>\n",
       "      <th>emb_280</th>\n",
       "      <th>emb_281</th>\n",
       "      <th>emb_282</th>\n",
       "      <th>emb_283</th>\n",
       "      <th>emb_284</th>\n",
       "      <th>emb_285</th>\n",
       "      <th>emb_286</th>\n",
       "      <th>emb_287</th>\n",
       "      <th>emb_288</th>\n",
       "      <th>emb_289</th>\n",
       "      <th>emb_290</th>\n",
       "      <th>emb_291</th>\n",
       "      <th>emb_292</th>\n",
       "      <th>emb_293</th>\n",
       "      <th>emb_294</th>\n",
       "      <th>emb_295</th>\n",
       "      <th>emb_296</th>\n",
       "      <th>emb_297</th>\n",
       "      <th>emb_298</th>\n",
       "      <th>emb_299</th>\n",
       "      <th>emb_300</th>\n",
       "      <th>emb_301</th>\n",
       "      <th>emb_302</th>\n",
       "      <th>emb_303</th>\n",
       "      <th>emb_304</th>\n",
       "      <th>emb_305</th>\n",
       "      <th>emb_306</th>\n",
       "      <th>emb_307</th>\n",
       "      <th>emb_308</th>\n",
       "      <th>emb_309</th>\n",
       "      <th>emb_310</th>\n",
       "      <th>emb_311</th>\n",
       "      <th>emb_312</th>\n",
       "      <th>emb_313</th>\n",
       "      <th>emb_314</th>\n",
       "      <th>emb_315</th>\n",
       "      <th>emb_316</th>\n",
       "      <th>emb_317</th>\n",
       "      <th>emb_318</th>\n",
       "      <th>emb_319</th>\n",
       "      <th>emb_320</th>\n",
       "      <th>emb_321</th>\n",
       "      <th>emb_322</th>\n",
       "      <th>emb_323</th>\n",
       "      <th>emb_324</th>\n",
       "      <th>emb_325</th>\n",
       "      <th>emb_326</th>\n",
       "      <th>emb_327</th>\n",
       "      <th>emb_328</th>\n",
       "      <th>emb_329</th>\n",
       "      <th>emb_330</th>\n",
       "      <th>emb_331</th>\n",
       "      <th>emb_332</th>\n",
       "      <th>emb_333</th>\n",
       "      <th>emb_334</th>\n",
       "      <th>emb_335</th>\n",
       "      <th>emb_336</th>\n",
       "      <th>emb_337</th>\n",
       "      <th>emb_338</th>\n",
       "      <th>emb_339</th>\n",
       "      <th>emb_340</th>\n",
       "      <th>emb_341</th>\n",
       "      <th>emb_342</th>\n",
       "      <th>emb_343</th>\n",
       "      <th>emb_344</th>\n",
       "      <th>emb_345</th>\n",
       "      <th>emb_346</th>\n",
       "      <th>emb_347</th>\n",
       "      <th>emb_348</th>\n",
       "      <th>emb_349</th>\n",
       "      <th>emb_350</th>\n",
       "      <th>emb_351</th>\n",
       "      <th>emb_352</th>\n",
       "      <th>emb_353</th>\n",
       "      <th>emb_354</th>\n",
       "      <th>emb_355</th>\n",
       "      <th>emb_356</th>\n",
       "      <th>emb_357</th>\n",
       "      <th>emb_358</th>\n",
       "      <th>emb_359</th>\n",
       "      <th>emb_360</th>\n",
       "      <th>emb_361</th>\n",
       "      <th>emb_362</th>\n",
       "      <th>emb_363</th>\n",
       "      <th>emb_364</th>\n",
       "      <th>emb_365</th>\n",
       "      <th>emb_366</th>\n",
       "      <th>emb_367</th>\n",
       "      <th>emb_368</th>\n",
       "      <th>emb_369</th>\n",
       "      <th>emb_370</th>\n",
       "      <th>emb_371</th>\n",
       "      <th>emb_372</th>\n",
       "      <th>emb_373</th>\n",
       "      <th>emb_374</th>\n",
       "      <th>emb_375</th>\n",
       "      <th>emb_376</th>\n",
       "      <th>emb_377</th>\n",
       "      <th>emb_378</th>\n",
       "      <th>emb_379</th>\n",
       "      <th>emb_380</th>\n",
       "      <th>emb_381</th>\n",
       "      <th>emb_382</th>\n",
       "      <th>emb_383</th>\n",
       "      <th>emb_384</th>\n",
       "      <th>emb_385</th>\n",
       "      <th>emb_386</th>\n",
       "      <th>emb_387</th>\n",
       "      <th>emb_388</th>\n",
       "      <th>emb_389</th>\n",
       "      <th>emb_390</th>\n",
       "      <th>emb_391</th>\n",
       "      <th>emb_392</th>\n",
       "      <th>emb_393</th>\n",
       "      <th>emb_394</th>\n",
       "      <th>emb_395</th>\n",
       "      <th>emb_396</th>\n",
       "      <th>emb_397</th>\n",
       "      <th>emb_398</th>\n",
       "      <th>emb_399</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6192</th>\n",
       "      <td>sirens</td>\n",
       "      <td>#na#</td>\n",
       "      <td>0.037508</td>\n",
       "      <td>0.055060</td>\n",
       "      <td>0.010673</td>\n",
       "      <td>0.006946</td>\n",
       "      <td>-0.008763</td>\n",
       "      <td>0.026151</td>\n",
       "      <td>0.051637</td>\n",
       "      <td>-0.219186</td>\n",
       "      <td>-0.069657</td>\n",
       "      <td>0.288112</td>\n",
       "      <td>-0.040087</td>\n",
       "      <td>-0.145560</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.055728</td>\n",
       "      <td>-0.023232</td>\n",
       "      <td>-0.031818</td>\n",
       "      <td>0.040715</td>\n",
       "      <td>-0.128729</td>\n",
       "      <td>0.164664</td>\n",
       "      <td>0.026755</td>\n",
       "      <td>-0.065879</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.082089</td>\n",
       "      <td>-0.025161</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>-0.093488</td>\n",
       "      <td>-0.148472</td>\n",
       "      <td>0.009366</td>\n",
       "      <td>0.068209</td>\n",
       "      <td>0.084896</td>\n",
       "      <td>0.093744</td>\n",
       "      <td>-0.028086</td>\n",
       "      <td>-0.141958</td>\n",
       "      <td>0.023407</td>\n",
       "      <td>-0.070239</td>\n",
       "      <td>-0.017203</td>\n",
       "      <td>0.150015</td>\n",
       "      <td>0.031332</td>\n",
       "      <td>0.081929</td>\n",
       "      <td>-0.031675</td>\n",
       "      <td>-0.066437</td>\n",
       "      <td>0.274757</td>\n",
       "      <td>0.023057</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.023158</td>\n",
       "      <td>-0.125546</td>\n",
       "      <td>-0.024842</td>\n",
       "      <td>0.170423</td>\n",
       "      <td>0.152998</td>\n",
       "      <td>-0.030957</td>\n",
       "      <td>-0.169797</td>\n",
       "      <td>0.259338</td>\n",
       "      <td>-0.021894</td>\n",
       "      <td>0.083339</td>\n",
       "      <td>0.090397</td>\n",
       "      <td>-0.055111</td>\n",
       "      <td>0.165681</td>\n",
       "      <td>-0.006837</td>\n",
       "      <td>-0.111347</td>\n",
       "      <td>-0.055022</td>\n",
       "      <td>0.031371</td>\n",
       "      <td>-0.013222</td>\n",
       "      <td>-0.108863</td>\n",
       "      <td>-0.147274</td>\n",
       "      <td>-0.056070</td>\n",
       "      <td>0.111384</td>\n",
       "      <td>0.050492</td>\n",
       "      <td>0.045759</td>\n",
       "      <td>0.148347</td>\n",
       "      <td>-0.119919</td>\n",
       "      <td>-0.001030</td>\n",
       "      <td>-0.011712</td>\n",
       "      <td>-0.151017</td>\n",
       "      <td>0.021616</td>\n",
       "      <td>-0.077219</td>\n",
       "      <td>0.028097</td>\n",
       "      <td>0.012725</td>\n",
       "      <td>-0.064401</td>\n",
       "      <td>-0.072953</td>\n",
       "      <td>-0.134233</td>\n",
       "      <td>-0.257821</td>\n",
       "      <td>0.048635</td>\n",
       "      <td>0.030048</td>\n",
       "      <td>0.123441</td>\n",
       "      <td>0.052961</td>\n",
       "      <td>-0.019909</td>\n",
       "      <td>0.192906</td>\n",
       "      <td>0.023149</td>\n",
       "      <td>-0.146892</td>\n",
       "      <td>-0.030802</td>\n",
       "      <td>0.008137</td>\n",
       "      <td>0.038071</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>-0.064612</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.038917</td>\n",
       "      <td>0.047206</td>\n",
       "      <td>0.034952</td>\n",
       "      <td>0.066005</td>\n",
       "      <td>-0.055167</td>\n",
       "      <td>-0.079867</td>\n",
       "      <td>0.037677</td>\n",
       "      <td>0.161609</td>\n",
       "      <td>-0.040842</td>\n",
       "      <td>-0.385791</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>-0.031600</td>\n",
       "      <td>-0.059844</td>\n",
       "      <td>-0.142066</td>\n",
       "      <td>-0.032429</td>\n",
       "      <td>0.049971</td>\n",
       "      <td>0.156796</td>\n",
       "      <td>0.018983</td>\n",
       "      <td>0.075027</td>\n",
       "      <td>-0.044106</td>\n",
       "      <td>0.013920</td>\n",
       "      <td>-0.152152</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.039039</td>\n",
       "      <td>0.088183</td>\n",
       "      <td>-0.116057</td>\n",
       "      <td>-0.019435</td>\n",
       "      <td>-0.049735</td>\n",
       "      <td>0.122181</td>\n",
       "      <td>0.059963</td>\n",
       "      <td>-0.010520</td>\n",
       "      <td>0.041929</td>\n",
       "      <td>0.009138</td>\n",
       "      <td>0.091025</td>\n",
       "      <td>0.016771</td>\n",
       "      <td>-0.052470</td>\n",
       "      <td>-0.167617</td>\n",
       "      <td>-0.093278</td>\n",
       "      <td>-0.140108</td>\n",
       "      <td>-0.165105</td>\n",
       "      <td>-0.070785</td>\n",
       "      <td>-0.101102</td>\n",
       "      <td>0.062778</td>\n",
       "      <td>-0.036228</td>\n",
       "      <td>0.136952</td>\n",
       "      <td>0.228832</td>\n",
       "      <td>0.054293</td>\n",
       "      <td>-0.007303</td>\n",
       "      <td>-0.052370</td>\n",
       "      <td>0.058846</td>\n",
       "      <td>0.182624</td>\n",
       "      <td>-0.084888</td>\n",
       "      <td>-0.047726</td>\n",
       "      <td>0.070670</td>\n",
       "      <td>-0.048521</td>\n",
       "      <td>-0.008546</td>\n",
       "      <td>-0.059200</td>\n",
       "      <td>0.106732</td>\n",
       "      <td>0.248693</td>\n",
       "      <td>0.021073</td>\n",
       "      <td>-0.105391</td>\n",
       "      <td>0.301290</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>-0.088605</td>\n",
       "      <td>-0.099961</td>\n",
       "      <td>-0.071311</td>\n",
       "      <td>0.072123</td>\n",
       "      <td>-0.109423</td>\n",
       "      <td>-0.078336</td>\n",
       "      <td>-0.071823</td>\n",
       "      <td>0.121757</td>\n",
       "      <td>0.069121</td>\n",
       "      <td>-0.076219</td>\n",
       "      <td>-0.095243</td>\n",
       "      <td>0.071703</td>\n",
       "      <td>0.059106</td>\n",
       "      <td>0.044182</td>\n",
       "      <td>-0.036436</td>\n",
       "      <td>0.125367</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>-0.099310</td>\n",
       "      <td>0.149961</td>\n",
       "      <td>0.104358</td>\n",
       "      <td>-0.057449</td>\n",
       "      <td>-0.071124</td>\n",
       "      <td>0.153865</td>\n",
       "      <td>-0.083751</td>\n",
       "      <td>0.161515</td>\n",
       "      <td>-0.063574</td>\n",
       "      <td>-0.079647</td>\n",
       "      <td>-0.011463</td>\n",
       "      <td>0.015410</td>\n",
       "      <td>0.041755</td>\n",
       "      <td>0.187061</td>\n",
       "      <td>0.118851</td>\n",
       "      <td>0.100854</td>\n",
       "      <td>0.014201</td>\n",
       "      <td>0.024451</td>\n",
       "      <td>-0.034055</td>\n",
       "      <td>-0.007525</td>\n",
       "      <td>0.063476</td>\n",
       "      <td>0.074799</td>\n",
       "      <td>-0.022220</td>\n",
       "      <td>0.048040</td>\n",
       "      <td>0.040232</td>\n",
       "      <td>0.205003</td>\n",
       "      <td>0.009972</td>\n",
       "      <td>0.016874</td>\n",
       "      <td>-0.092039</td>\n",
       "      <td>-0.060061</td>\n",
       "      <td>-0.001118</td>\n",
       "      <td>0.010169</td>\n",
       "      <td>0.036660</td>\n",
       "      <td>-0.023588</td>\n",
       "      <td>-0.008768</td>\n",
       "      <td>-0.028327</td>\n",
       "      <td>-0.026096</td>\n",
       "      <td>-0.168704</td>\n",
       "      <td>-0.288823</td>\n",
       "      <td>-0.018004</td>\n",
       "      <td>-0.210121</td>\n",
       "      <td>0.023571</td>\n",
       "      <td>0.056366</td>\n",
       "      <td>0.155404</td>\n",
       "      <td>-0.271505</td>\n",
       "      <td>-0.078952</td>\n",
       "      <td>0.067756</td>\n",
       "      <td>-0.051584</td>\n",
       "      <td>0.028480</td>\n",
       "      <td>-0.209598</td>\n",
       "      <td>0.070621</td>\n",
       "      <td>-0.350707</td>\n",
       "      <td>-0.001340</td>\n",
       "      <td>0.281084</td>\n",
       "      <td>-0.002594</td>\n",
       "      <td>0.099108</td>\n",
       "      <td>-0.122636</td>\n",
       "      <td>-0.197872</td>\n",
       "      <td>-0.115873</td>\n",
       "      <td>-0.132840</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>0.070412</td>\n",
       "      <td>0.146989</td>\n",
       "      <td>0.072520</td>\n",
       "      <td>-0.040304</td>\n",
       "      <td>0.071974</td>\n",
       "      <td>-0.018766</td>\n",
       "      <td>0.115002</td>\n",
       "      <td>-0.034802</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>-0.219338</td>\n",
       "      <td>-0.205505</td>\n",
       "      <td>0.144639</td>\n",
       "      <td>-0.151934</td>\n",
       "      <td>0.228596</td>\n",
       "      <td>0.027949</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>-0.173353</td>\n",
       "      <td>0.018398</td>\n",
       "      <td>-0.141789</td>\n",
       "      <td>0.031459</td>\n",
       "      <td>0.087604</td>\n",
       "      <td>0.057668</td>\n",
       "      <td>0.085085</td>\n",
       "      <td>-0.057572</td>\n",
       "      <td>0.074843</td>\n",
       "      <td>0.142508</td>\n",
       "      <td>0.021054</td>\n",
       "      <td>0.032389</td>\n",
       "      <td>0.086190</td>\n",
       "      <td>0.105595</td>\n",
       "      <td>-0.186475</td>\n",
       "      <td>-0.356969</td>\n",
       "      <td>-0.078072</td>\n",
       "      <td>0.140912</td>\n",
       "      <td>0.039308</td>\n",
       "      <td>-0.053903</td>\n",
       "      <td>0.021079</td>\n",
       "      <td>0.162720</td>\n",
       "      <td>0.160632</td>\n",
       "      <td>0.021557</td>\n",
       "      <td>-0.046209</td>\n",
       "      <td>-0.174469</td>\n",
       "      <td>0.036274</td>\n",
       "      <td>-0.002393</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>-0.182708</td>\n",
       "      <td>0.057872</td>\n",
       "      <td>0.014040</td>\n",
       "      <td>-0.154219</td>\n",
       "      <td>0.119263</td>\n",
       "      <td>-0.035476</td>\n",
       "      <td>0.089962</td>\n",
       "      <td>-0.005005</td>\n",
       "      <td>-0.089255</td>\n",
       "      <td>0.078573</td>\n",
       "      <td>-0.002499</td>\n",
       "      <td>0.026737</td>\n",
       "      <td>0.055641</td>\n",
       "      <td>0.261358</td>\n",
       "      <td>-0.023807</td>\n",
       "      <td>-0.008202</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>-0.126477</td>\n",
       "      <td>-0.184773</td>\n",
       "      <td>0.279970</td>\n",
       "      <td>0.026369</td>\n",
       "      <td>-0.010827</td>\n",
       "      <td>-0.042378</td>\n",
       "      <td>-0.042705</td>\n",
       "      <td>0.131519</td>\n",
       "      <td>-0.105752</td>\n",
       "      <td>0.272706</td>\n",
       "      <td>0.072015</td>\n",
       "      <td>0.042525</td>\n",
       "      <td>0.136527</td>\n",
       "      <td>-0.053072</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-0.002034</td>\n",
       "      <td>-0.048696</td>\n",
       "      <td>0.052110</td>\n",
       "      <td>0.171986</td>\n",
       "      <td>0.095038</td>\n",
       "      <td>0.094045</td>\n",
       "      <td>-0.028557</td>\n",
       "      <td>-0.011535</td>\n",
       "      <td>0.340998</td>\n",
       "      <td>-0.009249</td>\n",
       "      <td>-0.137278</td>\n",
       "      <td>-0.076901</td>\n",
       "      <td>-0.028088</td>\n",
       "      <td>-0.017278</td>\n",
       "      <td>0.048745</td>\n",
       "      <td>0.207630</td>\n",
       "      <td>0.150199</td>\n",
       "      <td>-0.021769</td>\n",
       "      <td>0.013553</td>\n",
       "      <td>-0.067669</td>\n",
       "      <td>-0.070676</td>\n",
       "      <td>0.138271</td>\n",
       "      <td>0.051470</td>\n",
       "      <td>0.127199</td>\n",
       "      <td>-0.008129</td>\n",
       "      <td>-0.038272</td>\n",
       "      <td>-0.233107</td>\n",
       "      <td>-0.275039</td>\n",
       "      <td>-0.047883</td>\n",
       "      <td>-0.034582</td>\n",
       "      <td>-0.122204</td>\n",
       "      <td>0.038670</td>\n",
       "      <td>-0.055875</td>\n",
       "      <td>0.020114</td>\n",
       "      <td>0.043183</td>\n",
       "      <td>-0.131982</td>\n",
       "      <td>-0.036996</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>-0.067967</td>\n",
       "      <td>0.102758</td>\n",
       "      <td>0.248470</td>\n",
       "      <td>0.124126</td>\n",
       "      <td>0.059493</td>\n",
       "      <td>0.082085</td>\n",
       "      <td>-0.032502</td>\n",
       "      <td>-0.209334</td>\n",
       "      <td>-0.005393</td>\n",
       "      <td>-0.029678</td>\n",
       "      <td>0.033364</td>\n",
       "      <td>-0.032391</td>\n",
       "      <td>0.100857</td>\n",
       "      <td>0.027245</td>\n",
       "      <td>-0.032674</td>\n",
       "      <td>-0.111192</td>\n",
       "      <td>-0.020948</td>\n",
       "      <td>-0.010957</td>\n",
       "      <td>-0.075843</td>\n",
       "      <td>0.051049</td>\n",
       "      <td>-0.188161</td>\n",
       "      <td>0.065992</td>\n",
       "      <td>-0.006477</td>\n",
       "      <td>-0.179287</td>\n",
       "      <td>-0.057650</td>\n",
       "      <td>0.045552</td>\n",
       "      <td>-0.064181</td>\n",
       "      <td>0.040202</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>0.050199</td>\n",
       "      <td>-0.063731</td>\n",
       "      <td>-0.040250</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>-0.082394</td>\n",
       "      <td>-0.155738</td>\n",
       "      <td>-0.058622</td>\n",
       "      <td>-0.144662</td>\n",
       "      <td>0.082669</td>\n",
       "      <td>0.057297</td>\n",
       "      <td>0.014276</td>\n",
       "      <td>0.136305</td>\n",
       "      <td>-0.054000</td>\n",
       "      <td>-0.088347</td>\n",
       "      <td>-0.020471</td>\n",
       "      <td>-0.005486</td>\n",
       "      <td>-0.013983</td>\n",
       "      <td>0.025120</td>\n",
       "      <td>0.110265</td>\n",
       "      <td>-0.007783</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>police</td>\n",
       "      <td>USA</td>\n",
       "      <td>0.112898</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.095674</td>\n",
       "      <td>0.027466</td>\n",
       "      <td>-0.053297</td>\n",
       "      <td>0.271229</td>\n",
       "      <td>0.152836</td>\n",
       "      <td>-0.215035</td>\n",
       "      <td>-0.140109</td>\n",
       "      <td>0.090577</td>\n",
       "      <td>0.061928</td>\n",
       "      <td>-0.072727</td>\n",
       "      <td>0.036998</td>\n",
       "      <td>0.036870</td>\n",
       "      <td>0.028001</td>\n",
       "      <td>0.161397</td>\n",
       "      <td>-0.059826</td>\n",
       "      <td>-0.106922</td>\n",
       "      <td>0.166523</td>\n",
       "      <td>-0.031543</td>\n",
       "      <td>0.030816</td>\n",
       "      <td>-0.075498</td>\n",
       "      <td>-0.094905</td>\n",
       "      <td>0.145043</td>\n",
       "      <td>0.043564</td>\n",
       "      <td>0.125319</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>0.044993</td>\n",
       "      <td>0.083337</td>\n",
       "      <td>0.014207</td>\n",
       "      <td>0.024168</td>\n",
       "      <td>-0.126497</td>\n",
       "      <td>-0.139551</td>\n",
       "      <td>0.190516</td>\n",
       "      <td>-0.064788</td>\n",
       "      <td>0.087001</td>\n",
       "      <td>0.031390</td>\n",
       "      <td>0.043177</td>\n",
       "      <td>0.274622</td>\n",
       "      <td>-0.109947</td>\n",
       "      <td>-0.150783</td>\n",
       "      <td>0.066094</td>\n",
       "      <td>-0.071636</td>\n",
       "      <td>-0.166014</td>\n",
       "      <td>0.043024</td>\n",
       "      <td>0.045798</td>\n",
       "      <td>-0.058107</td>\n",
       "      <td>0.135607</td>\n",
       "      <td>0.024629</td>\n",
       "      <td>-0.030888</td>\n",
       "      <td>-0.190149</td>\n",
       "      <td>0.115405</td>\n",
       "      <td>-0.042541</td>\n",
       "      <td>-0.061940</td>\n",
       "      <td>0.046699</td>\n",
       "      <td>0.097073</td>\n",
       "      <td>0.187544</td>\n",
       "      <td>-0.080332</td>\n",
       "      <td>-0.100585</td>\n",
       "      <td>0.094819</td>\n",
       "      <td>-0.084466</td>\n",
       "      <td>-0.105145</td>\n",
       "      <td>-0.047003</td>\n",
       "      <td>-0.139958</td>\n",
       "      <td>-0.090053</td>\n",
       "      <td>0.176382</td>\n",
       "      <td>-0.010822</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>0.187792</td>\n",
       "      <td>-0.249240</td>\n",
       "      <td>-0.067685</td>\n",
       "      <td>-0.065478</td>\n",
       "      <td>-0.213596</td>\n",
       "      <td>-0.087357</td>\n",
       "      <td>-0.003724</td>\n",
       "      <td>-0.109372</td>\n",
       "      <td>0.012420</td>\n",
       "      <td>-0.040749</td>\n",
       "      <td>0.065192</td>\n",
       "      <td>-0.082943</td>\n",
       "      <td>-0.140004</td>\n",
       "      <td>0.031189</td>\n",
       "      <td>-0.034408</td>\n",
       "      <td>0.036583</td>\n",
       "      <td>-0.068967</td>\n",
       "      <td>0.045965</td>\n",
       "      <td>0.044021</td>\n",
       "      <td>0.128049</td>\n",
       "      <td>-0.101307</td>\n",
       "      <td>0.013970</td>\n",
       "      <td>-0.048565</td>\n",
       "      <td>-0.039935</td>\n",
       "      <td>-0.045529</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>-0.001839</td>\n",
       "      <td>0.107964</td>\n",
       "      <td>0.181443</td>\n",
       "      <td>-0.031311</td>\n",
       "      <td>0.053305</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.226522</td>\n",
       "      <td>0.137110</td>\n",
       "      <td>-0.017099</td>\n",
       "      <td>-0.169091</td>\n",
       "      <td>-0.122665</td>\n",
       "      <td>0.083270</td>\n",
       "      <td>-0.089733</td>\n",
       "      <td>-0.020715</td>\n",
       "      <td>-0.264808</td>\n",
       "      <td>-0.158739</td>\n",
       "      <td>0.090200</td>\n",
       "      <td>0.037423</td>\n",
       "      <td>0.044316</td>\n",
       "      <td>-0.006968</td>\n",
       "      <td>0.032916</td>\n",
       "      <td>0.031347</td>\n",
       "      <td>-0.072517</td>\n",
       "      <td>0.254591</td>\n",
       "      <td>0.118661</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>-0.089655</td>\n",
       "      <td>0.008061</td>\n",
       "      <td>-0.070765</td>\n",
       "      <td>-0.043276</td>\n",
       "      <td>0.176751</td>\n",
       "      <td>-0.021843</td>\n",
       "      <td>0.054913</td>\n",
       "      <td>0.063431</td>\n",
       "      <td>-0.083017</td>\n",
       "      <td>0.057142</td>\n",
       "      <td>0.138537</td>\n",
       "      <td>-0.145702</td>\n",
       "      <td>-0.152001</td>\n",
       "      <td>-0.095696</td>\n",
       "      <td>-0.141399</td>\n",
       "      <td>0.152655</td>\n",
       "      <td>-0.011297</td>\n",
       "      <td>0.027230</td>\n",
       "      <td>-0.069986</td>\n",
       "      <td>0.213979</td>\n",
       "      <td>0.121733</td>\n",
       "      <td>0.142021</td>\n",
       "      <td>-0.076308</td>\n",
       "      <td>0.096581</td>\n",
       "      <td>0.114053</td>\n",
       "      <td>-0.073213</td>\n",
       "      <td>-0.161394</td>\n",
       "      <td>-0.062225</td>\n",
       "      <td>-0.213541</td>\n",
       "      <td>0.125124</td>\n",
       "      <td>-0.025110</td>\n",
       "      <td>-0.032306</td>\n",
       "      <td>0.110153</td>\n",
       "      <td>0.246089</td>\n",
       "      <td>0.070927</td>\n",
       "      <td>-0.106414</td>\n",
       "      <td>0.202314</td>\n",
       "      <td>-0.018640</td>\n",
       "      <td>-0.174799</td>\n",
       "      <td>0.044533</td>\n",
       "      <td>-0.171350</td>\n",
       "      <td>0.095172</td>\n",
       "      <td>0.095640</td>\n",
       "      <td>-0.106964</td>\n",
       "      <td>-0.060019</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.135740</td>\n",
       "      <td>-0.045549</td>\n",
       "      <td>-0.068265</td>\n",
       "      <td>0.040944</td>\n",
       "      <td>0.029336</td>\n",
       "      <td>0.261425</td>\n",
       "      <td>-0.127923</td>\n",
       "      <td>0.122825</td>\n",
       "      <td>0.012849</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.096626</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>-0.077312</td>\n",
       "      <td>-0.027855</td>\n",
       "      <td>0.036898</td>\n",
       "      <td>0.016365</td>\n",
       "      <td>-0.090904</td>\n",
       "      <td>-0.030485</td>\n",
       "      <td>0.061476</td>\n",
       "      <td>0.077118</td>\n",
       "      <td>0.097816</td>\n",
       "      <td>0.166529</td>\n",
       "      <td>0.260490</td>\n",
       "      <td>0.063801</td>\n",
       "      <td>-0.159681</td>\n",
       "      <td>-0.066132</td>\n",
       "      <td>0.049322</td>\n",
       "      <td>0.104970</td>\n",
       "      <td>0.031617</td>\n",
       "      <td>0.036993</td>\n",
       "      <td>0.019649</td>\n",
       "      <td>0.063330</td>\n",
       "      <td>-0.049782</td>\n",
       "      <td>0.160785</td>\n",
       "      <td>0.132729</td>\n",
       "      <td>-0.150095</td>\n",
       "      <td>-0.124441</td>\n",
       "      <td>-0.094000</td>\n",
       "      <td>0.056865</td>\n",
       "      <td>-0.053947</td>\n",
       "      <td>0.025911</td>\n",
       "      <td>-0.022517</td>\n",
       "      <td>0.025598</td>\n",
       "      <td>0.023522</td>\n",
       "      <td>-0.188440</td>\n",
       "      <td>-0.014419</td>\n",
       "      <td>-0.271544</td>\n",
       "      <td>0.045196</td>\n",
       "      <td>-0.212486</td>\n",
       "      <td>0.123841</td>\n",
       "      <td>0.094451</td>\n",
       "      <td>0.083292</td>\n",
       "      <td>-0.242268</td>\n",
       "      <td>-0.138853</td>\n",
       "      <td>-0.134020</td>\n",
       "      <td>0.166461</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>-0.162440</td>\n",
       "      <td>0.006961</td>\n",
       "      <td>-0.303127</td>\n",
       "      <td>0.033616</td>\n",
       "      <td>0.049763</td>\n",
       "      <td>-0.093429</td>\n",
       "      <td>0.032783</td>\n",
       "      <td>-0.044810</td>\n",
       "      <td>-0.250556</td>\n",
       "      <td>-0.013322</td>\n",
       "      <td>0.011379</td>\n",
       "      <td>0.045894</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.226182</td>\n",
       "      <td>-0.113433</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>0.017177</td>\n",
       "      <td>-0.041773</td>\n",
       "      <td>0.053578</td>\n",
       "      <td>-0.061792</td>\n",
       "      <td>0.056178</td>\n",
       "      <td>-0.254421</td>\n",
       "      <td>0.159654</td>\n",
       "      <td>-0.094320</td>\n",
       "      <td>-0.262293</td>\n",
       "      <td>0.203419</td>\n",
       "      <td>0.111769</td>\n",
       "      <td>0.161747</td>\n",
       "      <td>-0.118946</td>\n",
       "      <td>-0.053369</td>\n",
       "      <td>-0.002552</td>\n",
       "      <td>0.010489</td>\n",
       "      <td>0.163404</td>\n",
       "      <td>-0.036467</td>\n",
       "      <td>-0.014577</td>\n",
       "      <td>-0.133826</td>\n",
       "      <td>-0.111948</td>\n",
       "      <td>0.091971</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>0.035127</td>\n",
       "      <td>0.014972</td>\n",
       "      <td>0.070873</td>\n",
       "      <td>-0.159118</td>\n",
       "      <td>-0.237353</td>\n",
       "      <td>-0.001927</td>\n",
       "      <td>0.368569</td>\n",
       "      <td>0.049451</td>\n",
       "      <td>-0.161065</td>\n",
       "      <td>-0.009498</td>\n",
       "      <td>0.081681</td>\n",
       "      <td>0.103858</td>\n",
       "      <td>0.128603</td>\n",
       "      <td>-0.054656</td>\n",
       "      <td>-0.097429</td>\n",
       "      <td>0.074501</td>\n",
       "      <td>-0.147931</td>\n",
       "      <td>-0.167537</td>\n",
       "      <td>-0.286804</td>\n",
       "      <td>0.201339</td>\n",
       "      <td>0.134561</td>\n",
       "      <td>-0.132346</td>\n",
       "      <td>0.093103</td>\n",
       "      <td>-0.064704</td>\n",
       "      <td>-0.024951</td>\n",
       "      <td>0.017234</td>\n",
       "      <td>-0.006706</td>\n",
       "      <td>0.119180</td>\n",
       "      <td>0.032037</td>\n",
       "      <td>-0.085232</td>\n",
       "      <td>0.093342</td>\n",
       "      <td>0.180195</td>\n",
       "      <td>-0.031959</td>\n",
       "      <td>-0.001350</td>\n",
       "      <td>0.007728</td>\n",
       "      <td>-0.032314</td>\n",
       "      <td>-0.148289</td>\n",
       "      <td>0.197651</td>\n",
       "      <td>-0.088858</td>\n",
       "      <td>-0.048320</td>\n",
       "      <td>-0.019547</td>\n",
       "      <td>-0.114752</td>\n",
       "      <td>-0.027081</td>\n",
       "      <td>-0.252420</td>\n",
       "      <td>0.167419</td>\n",
       "      <td>0.046687</td>\n",
       "      <td>0.075852</td>\n",
       "      <td>0.160158</td>\n",
       "      <td>0.070964</td>\n",
       "      <td>-0.079957</td>\n",
       "      <td>-0.046331</td>\n",
       "      <td>-0.158972</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.114120</td>\n",
       "      <td>-0.197959</td>\n",
       "      <td>-0.011070</td>\n",
       "      <td>-0.071051</td>\n",
       "      <td>-0.014641</td>\n",
       "      <td>0.262220</td>\n",
       "      <td>-0.166532</td>\n",
       "      <td>0.022337</td>\n",
       "      <td>-0.018375</td>\n",
       "      <td>-0.087322</td>\n",
       "      <td>-0.140432</td>\n",
       "      <td>-0.070912</td>\n",
       "      <td>-0.007920</td>\n",
       "      <td>0.109697</td>\n",
       "      <td>-0.040822</td>\n",
       "      <td>0.044855</td>\n",
       "      <td>0.052449</td>\n",
       "      <td>0.106817</td>\n",
       "      <td>-0.074841</td>\n",
       "      <td>-0.094502</td>\n",
       "      <td>0.164625</td>\n",
       "      <td>-0.034119</td>\n",
       "      <td>0.071506</td>\n",
       "      <td>-0.117788</td>\n",
       "      <td>-0.255965</td>\n",
       "      <td>0.118744</td>\n",
       "      <td>0.123743</td>\n",
       "      <td>-0.165049</td>\n",
       "      <td>0.050448</td>\n",
       "      <td>-0.068246</td>\n",
       "      <td>0.010057</td>\n",
       "      <td>-0.106428</td>\n",
       "      <td>-0.132181</td>\n",
       "      <td>-0.052784</td>\n",
       "      <td>-0.246122</td>\n",
       "      <td>-0.002716</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.273692</td>\n",
       "      <td>0.151634</td>\n",
       "      <td>-0.032980</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>-0.028018</td>\n",
       "      <td>-0.039732</td>\n",
       "      <td>-0.056149</td>\n",
       "      <td>0.072412</td>\n",
       "      <td>-0.007252</td>\n",
       "      <td>0.108638</td>\n",
       "      <td>-0.022384</td>\n",
       "      <td>0.011537</td>\n",
       "      <td>-0.078946</td>\n",
       "      <td>-0.135493</td>\n",
       "      <td>-0.101475</td>\n",
       "      <td>-0.093896</td>\n",
       "      <td>-0.042803</td>\n",
       "      <td>0.051744</td>\n",
       "      <td>-0.120511</td>\n",
       "      <td>0.031217</td>\n",
       "      <td>0.224899</td>\n",
       "      <td>-0.135153</td>\n",
       "      <td>-0.042282</td>\n",
       "      <td>-0.098094</td>\n",
       "      <td>-0.133625</td>\n",
       "      <td>0.033502</td>\n",
       "      <td>0.069094</td>\n",
       "      <td>0.081392</td>\n",
       "      <td>-0.003043</td>\n",
       "      <td>-0.110670</td>\n",
       "      <td>-0.011268</td>\n",
       "      <td>0.056013</td>\n",
       "      <td>-0.096712</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.135003</td>\n",
       "      <td>0.101851</td>\n",
       "      <td>-0.021134</td>\n",
       "      <td>0.044574</td>\n",
       "      <td>0.116753</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>0.021026</td>\n",
       "      <td>-0.039478</td>\n",
       "      <td>-0.005339</td>\n",
       "      <td>0.079044</td>\n",
       "      <td>0.222914</td>\n",
       "      <td>-0.120942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>mass%20murderer</td>\n",
       "      <td>Haysville, KS</td>\n",
       "      <td>-0.019414</td>\n",
       "      <td>-0.087513</td>\n",
       "      <td>0.182088</td>\n",
       "      <td>0.019102</td>\n",
       "      <td>-0.154542</td>\n",
       "      <td>0.214701</td>\n",
       "      <td>0.104244</td>\n",
       "      <td>-0.194652</td>\n",
       "      <td>-0.166407</td>\n",
       "      <td>0.042626</td>\n",
       "      <td>0.138191</td>\n",
       "      <td>-0.034452</td>\n",
       "      <td>0.048175</td>\n",
       "      <td>0.014309</td>\n",
       "      <td>-0.159233</td>\n",
       "      <td>0.249255</td>\n",
       "      <td>0.079979</td>\n",
       "      <td>-0.080505</td>\n",
       "      <td>0.060769</td>\n",
       "      <td>-0.127035</td>\n",
       "      <td>-0.196640</td>\n",
       "      <td>-0.141735</td>\n",
       "      <td>-0.109923</td>\n",
       "      <td>0.175613</td>\n",
       "      <td>-0.044957</td>\n",
       "      <td>0.186366</td>\n",
       "      <td>0.012360</td>\n",
       "      <td>0.103502</td>\n",
       "      <td>-0.040414</td>\n",
       "      <td>-0.148785</td>\n",
       "      <td>0.098435</td>\n",
       "      <td>-0.042524</td>\n",
       "      <td>-0.079379</td>\n",
       "      <td>0.009982</td>\n",
       "      <td>-0.107949</td>\n",
       "      <td>0.117535</td>\n",
       "      <td>-0.212322</td>\n",
       "      <td>-0.094290</td>\n",
       "      <td>0.024542</td>\n",
       "      <td>-0.113490</td>\n",
       "      <td>-0.167122</td>\n",
       "      <td>0.233677</td>\n",
       "      <td>0.012782</td>\n",
       "      <td>-0.093298</td>\n",
       "      <td>-0.107465</td>\n",
       "      <td>0.325287</td>\n",
       "      <td>0.182469</td>\n",
       "      <td>0.184330</td>\n",
       "      <td>-0.034942</td>\n",
       "      <td>-0.033150</td>\n",
       "      <td>-0.147048</td>\n",
       "      <td>-0.084448</td>\n",
       "      <td>-0.001281</td>\n",
       "      <td>0.087393</td>\n",
       "      <td>0.041298</td>\n",
       "      <td>-0.008694</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>-0.173822</td>\n",
       "      <td>-0.016550</td>\n",
       "      <td>0.126940</td>\n",
       "      <td>0.073256</td>\n",
       "      <td>-0.142641</td>\n",
       "      <td>0.061391</td>\n",
       "      <td>-0.058024</td>\n",
       "      <td>-0.099124</td>\n",
       "      <td>-0.183917</td>\n",
       "      <td>0.135608</td>\n",
       "      <td>-0.006400</td>\n",
       "      <td>0.243040</td>\n",
       "      <td>0.024251</td>\n",
       "      <td>-0.113897</td>\n",
       "      <td>-0.079960</td>\n",
       "      <td>-0.251900</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>-0.174757</td>\n",
       "      <td>-0.003864</td>\n",
       "      <td>-0.055150</td>\n",
       "      <td>-0.154384</td>\n",
       "      <td>0.093898</td>\n",
       "      <td>-0.023555</td>\n",
       "      <td>0.025221</td>\n",
       "      <td>-0.021264</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>-0.218115</td>\n",
       "      <td>0.062752</td>\n",
       "      <td>0.069389</td>\n",
       "      <td>-0.099786</td>\n",
       "      <td>0.104292</td>\n",
       "      <td>-0.032472</td>\n",
       "      <td>0.047408</td>\n",
       "      <td>0.094012</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>-0.078876</td>\n",
       "      <td>-0.228535</td>\n",
       "      <td>-0.016474</td>\n",
       "      <td>0.145014</td>\n",
       "      <td>-0.078294</td>\n",
       "      <td>0.011977</td>\n",
       "      <td>0.106739</td>\n",
       "      <td>-0.018976</td>\n",
       "      <td>0.074203</td>\n",
       "      <td>0.086930</td>\n",
       "      <td>-0.089687</td>\n",
       "      <td>0.127859</td>\n",
       "      <td>0.082727</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.052567</td>\n",
       "      <td>-0.137341</td>\n",
       "      <td>-0.206173</td>\n",
       "      <td>0.266780</td>\n",
       "      <td>-0.047137</td>\n",
       "      <td>0.079040</td>\n",
       "      <td>0.055941</td>\n",
       "      <td>0.066619</td>\n",
       "      <td>-0.129290</td>\n",
       "      <td>-0.056673</td>\n",
       "      <td>0.058133</td>\n",
       "      <td>0.013450</td>\n",
       "      <td>-0.000341</td>\n",
       "      <td>-0.103722</td>\n",
       "      <td>0.039279</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>-0.198360</td>\n",
       "      <td>0.089530</td>\n",
       "      <td>-0.014357</td>\n",
       "      <td>-0.090088</td>\n",
       "      <td>-0.110385</td>\n",
       "      <td>-0.132945</td>\n",
       "      <td>0.055340</td>\n",
       "      <td>0.127001</td>\n",
       "      <td>0.121839</td>\n",
       "      <td>-0.093459</td>\n",
       "      <td>0.109014</td>\n",
       "      <td>-0.099166</td>\n",
       "      <td>0.164435</td>\n",
       "      <td>0.104245</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>-0.206719</td>\n",
       "      <td>0.174077</td>\n",
       "      <td>0.156428</td>\n",
       "      <td>0.250168</td>\n",
       "      <td>-0.071211</td>\n",
       "      <td>-0.142050</td>\n",
       "      <td>0.298848</td>\n",
       "      <td>0.020464</td>\n",
       "      <td>-0.212786</td>\n",
       "      <td>0.016559</td>\n",
       "      <td>0.115429</td>\n",
       "      <td>0.041728</td>\n",
       "      <td>-0.070047</td>\n",
       "      <td>0.085647</td>\n",
       "      <td>0.034370</td>\n",
       "      <td>0.253975</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>-0.135952</td>\n",
       "      <td>0.058759</td>\n",
       "      <td>-0.049450</td>\n",
       "      <td>-0.043173</td>\n",
       "      <td>-0.044498</td>\n",
       "      <td>-0.169043</td>\n",
       "      <td>-0.048835</td>\n",
       "      <td>0.043926</td>\n",
       "      <td>-0.073857</td>\n",
       "      <td>-0.037723</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.366376</td>\n",
       "      <td>-0.081365</td>\n",
       "      <td>-0.188128</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>-0.025652</td>\n",
       "      <td>0.102524</td>\n",
       "      <td>-0.146439</td>\n",
       "      <td>0.050802</td>\n",
       "      <td>0.106169</td>\n",
       "      <td>-0.073909</td>\n",
       "      <td>0.015084</td>\n",
       "      <td>-0.026253</td>\n",
       "      <td>-0.089132</td>\n",
       "      <td>0.068688</td>\n",
       "      <td>0.036385</td>\n",
       "      <td>0.186135</td>\n",
       "      <td>-0.015192</td>\n",
       "      <td>0.203649</td>\n",
       "      <td>0.040554</td>\n",
       "      <td>0.214555</td>\n",
       "      <td>0.135881</td>\n",
       "      <td>0.077381</td>\n",
       "      <td>0.061249</td>\n",
       "      <td>0.171405</td>\n",
       "      <td>-0.024455</td>\n",
       "      <td>-0.252858</td>\n",
       "      <td>0.011868</td>\n",
       "      <td>0.013797</td>\n",
       "      <td>-0.040934</td>\n",
       "      <td>0.026164</td>\n",
       "      <td>-0.064392</td>\n",
       "      <td>-0.051006</td>\n",
       "      <td>-0.057623</td>\n",
       "      <td>0.009144</td>\n",
       "      <td>-0.012082</td>\n",
       "      <td>0.072799</td>\n",
       "      <td>0.155026</td>\n",
       "      <td>-0.030479</td>\n",
       "      <td>-0.125845</td>\n",
       "      <td>-0.094981</td>\n",
       "      <td>-0.101405</td>\n",
       "      <td>0.035784</td>\n",
       "      <td>-0.189482</td>\n",
       "      <td>0.146675</td>\n",
       "      <td>-0.062459</td>\n",
       "      <td>-0.284756</td>\n",
       "      <td>0.070802</td>\n",
       "      <td>-0.222648</td>\n",
       "      <td>-0.055765</td>\n",
       "      <td>-0.002441</td>\n",
       "      <td>0.249433</td>\n",
       "      <td>-0.017099</td>\n",
       "      <td>0.050994</td>\n",
       "      <td>-0.182466</td>\n",
       "      <td>0.032620</td>\n",
       "      <td>-0.033744</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>0.063427</td>\n",
       "      <td>-0.144151</td>\n",
       "      <td>-0.011040</td>\n",
       "      <td>-0.175818</td>\n",
       "      <td>0.071597</td>\n",
       "      <td>0.123039</td>\n",
       "      <td>0.059335</td>\n",
       "      <td>-0.023228</td>\n",
       "      <td>0.025643</td>\n",
       "      <td>-0.254167</td>\n",
       "      <td>0.088691</td>\n",
       "      <td>0.010836</td>\n",
       "      <td>0.076875</td>\n",
       "      <td>-0.019138</td>\n",
       "      <td>0.016744</td>\n",
       "      <td>-0.120834</td>\n",
       "      <td>0.012758</td>\n",
       "      <td>0.046954</td>\n",
       "      <td>0.031315</td>\n",
       "      <td>-0.005171</td>\n",
       "      <td>-0.063318</td>\n",
       "      <td>-0.078219</td>\n",
       "      <td>-0.233497</td>\n",
       "      <td>0.281344</td>\n",
       "      <td>-0.036546</td>\n",
       "      <td>-0.234314</td>\n",
       "      <td>0.512969</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>0.164549</td>\n",
       "      <td>-0.051829</td>\n",
       "      <td>-0.163138</td>\n",
       "      <td>-0.006128</td>\n",
       "      <td>-0.078703</td>\n",
       "      <td>0.048292</td>\n",
       "      <td>-0.118734</td>\n",
       "      <td>-0.118298</td>\n",
       "      <td>-0.101975</td>\n",
       "      <td>-0.175908</td>\n",
       "      <td>0.037732</td>\n",
       "      <td>0.151352</td>\n",
       "      <td>-0.002027</td>\n",
       "      <td>0.089212</td>\n",
       "      <td>0.059780</td>\n",
       "      <td>-0.163545</td>\n",
       "      <td>-0.030554</td>\n",
       "      <td>-0.023919</td>\n",
       "      <td>0.106213</td>\n",
       "      <td>-0.114563</td>\n",
       "      <td>-0.015900</td>\n",
       "      <td>0.073052</td>\n",
       "      <td>-0.186102</td>\n",
       "      <td>-0.043167</td>\n",
       "      <td>0.221882</td>\n",
       "      <td>0.076160</td>\n",
       "      <td>-0.104558</td>\n",
       "      <td>0.092273</td>\n",
       "      <td>-0.168838</td>\n",
       "      <td>-0.098928</td>\n",
       "      <td>-0.097306</td>\n",
       "      <td>0.056527</td>\n",
       "      <td>0.173420</td>\n",
       "      <td>-0.128929</td>\n",
       "      <td>-0.087863</td>\n",
       "      <td>-0.078142</td>\n",
       "      <td>-0.050697</td>\n",
       "      <td>-0.006116</td>\n",
       "      <td>-0.027707</td>\n",
       "      <td>-0.005384</td>\n",
       "      <td>0.053616</td>\n",
       "      <td>-0.109919</td>\n",
       "      <td>-0.041402</td>\n",
       "      <td>0.248914</td>\n",
       "      <td>-0.125144</td>\n",
       "      <td>-0.063198</td>\n",
       "      <td>0.026725</td>\n",
       "      <td>-0.007703</td>\n",
       "      <td>-0.053255</td>\n",
       "      <td>0.422109</td>\n",
       "      <td>-0.125451</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>-0.022147</td>\n",
       "      <td>-0.098182</td>\n",
       "      <td>-0.079729</td>\n",
       "      <td>0.083323</td>\n",
       "      <td>0.261642</td>\n",
       "      <td>0.174864</td>\n",
       "      <td>0.075354</td>\n",
       "      <td>0.076559</td>\n",
       "      <td>0.070362</td>\n",
       "      <td>-0.031354</td>\n",
       "      <td>-0.057400</td>\n",
       "      <td>0.261949</td>\n",
       "      <td>0.181345</td>\n",
       "      <td>0.063998</td>\n",
       "      <td>0.009802</td>\n",
       "      <td>-0.050977</td>\n",
       "      <td>-0.031049</td>\n",
       "      <td>-0.032671</td>\n",
       "      <td>0.248295</td>\n",
       "      <td>-0.002536</td>\n",
       "      <td>-0.093106</td>\n",
       "      <td>-0.086777</td>\n",
       "      <td>0.034363</td>\n",
       "      <td>-0.004177</td>\n",
       "      <td>0.040386</td>\n",
       "      <td>-0.149717</td>\n",
       "      <td>0.024890</td>\n",
       "      <td>-0.095281</td>\n",
       "      <td>0.147814</td>\n",
       "      <td>-0.123123</td>\n",
       "      <td>-0.003139</td>\n",
       "      <td>0.081864</td>\n",
       "      <td>-0.096845</td>\n",
       "      <td>-0.021098</td>\n",
       "      <td>-0.055105</td>\n",
       "      <td>0.149776</td>\n",
       "      <td>-0.193501</td>\n",
       "      <td>-0.114941</td>\n",
       "      <td>-0.078351</td>\n",
       "      <td>0.027979</td>\n",
       "      <td>-0.091030</td>\n",
       "      <td>-0.071595</td>\n",
       "      <td>-0.127972</td>\n",
       "      <td>0.025581</td>\n",
       "      <td>-0.006656</td>\n",
       "      <td>-0.317233</td>\n",
       "      <td>-0.139544</td>\n",
       "      <td>0.037242</td>\n",
       "      <td>0.106768</td>\n",
       "      <td>-0.029459</td>\n",
       "      <td>0.425889</td>\n",
       "      <td>0.067482</td>\n",
       "      <td>0.075136</td>\n",
       "      <td>0.026982</td>\n",
       "      <td>-0.078885</td>\n",
       "      <td>0.101285</td>\n",
       "      <td>0.087272</td>\n",
       "      <td>0.071513</td>\n",
       "      <td>-0.012909</td>\n",
       "      <td>0.055875</td>\n",
       "      <td>0.135219</td>\n",
       "      <td>0.049055</td>\n",
       "      <td>-0.072129</td>\n",
       "      <td>0.067042</td>\n",
       "      <td>-0.108816</td>\n",
       "      <td>-0.181198</td>\n",
       "      <td>0.099039</td>\n",
       "      <td>0.017198</td>\n",
       "      <td>-0.019343</td>\n",
       "      <td>-0.074788</td>\n",
       "      <td>0.089717</td>\n",
       "      <td>-0.039612</td>\n",
       "      <td>0.028827</td>\n",
       "      <td>0.013360</td>\n",
       "      <td>-0.143565</td>\n",
       "      <td>-0.150968</td>\n",
       "      <td>0.229106</td>\n",
       "      <td>-0.024370</td>\n",
       "      <td>-0.004649</td>\n",
       "      <td>-0.182534</td>\n",
       "      <td>0.073710</td>\n",
       "      <td>-0.061701</td>\n",
       "      <td>-0.013210</td>\n",
       "      <td>-0.009350</td>\n",
       "      <td>0.079325</td>\n",
       "      <td>-0.039029</td>\n",
       "      <td>0.099431</td>\n",
       "      <td>-0.083337</td>\n",
       "      <td>0.037316</td>\n",
       "      <td>0.152315</td>\n",
       "      <td>0.098875</td>\n",
       "      <td>0.086047</td>\n",
       "      <td>-0.069216</td>\n",
       "      <td>-0.115149</td>\n",
       "      <td>0.039374</td>\n",
       "      <td>0.377374</td>\n",
       "      <td>0.014803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>detonation</td>\n",
       "      <td>#na#</td>\n",
       "      <td>-0.107448</td>\n",
       "      <td>-0.006279</td>\n",
       "      <td>-0.067999</td>\n",
       "      <td>0.039616</td>\n",
       "      <td>-0.036072</td>\n",
       "      <td>0.007058</td>\n",
       "      <td>0.022725</td>\n",
       "      <td>-0.152529</td>\n",
       "      <td>-0.047920</td>\n",
       "      <td>0.155072</td>\n",
       "      <td>-0.084948</td>\n",
       "      <td>0.006656</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.018395</td>\n",
       "      <td>-0.031748</td>\n",
       "      <td>-0.006204</td>\n",
       "      <td>-0.057792</td>\n",
       "      <td>0.013362</td>\n",
       "      <td>0.094087</td>\n",
       "      <td>0.049074</td>\n",
       "      <td>0.070172</td>\n",
       "      <td>-0.007380</td>\n",
       "      <td>0.029956</td>\n",
       "      <td>0.029694</td>\n",
       "      <td>-0.008922</td>\n",
       "      <td>-0.151170</td>\n",
       "      <td>-0.024343</td>\n",
       "      <td>-0.078024</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.187538</td>\n",
       "      <td>0.061567</td>\n",
       "      <td>0.081218</td>\n",
       "      <td>-0.267655</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>-0.046599</td>\n",
       "      <td>0.104464</td>\n",
       "      <td>0.090145</td>\n",
       "      <td>-0.072532</td>\n",
       "      <td>0.128523</td>\n",
       "      <td>-0.033901</td>\n",
       "      <td>0.040031</td>\n",
       "      <td>0.145571</td>\n",
       "      <td>0.021217</td>\n",
       "      <td>-0.007112</td>\n",
       "      <td>0.184248</td>\n",
       "      <td>-0.037739</td>\n",
       "      <td>0.033886</td>\n",
       "      <td>0.045966</td>\n",
       "      <td>0.086613</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>-0.108547</td>\n",
       "      <td>0.162344</td>\n",
       "      <td>0.063931</td>\n",
       "      <td>-0.017461</td>\n",
       "      <td>0.021195</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>0.210855</td>\n",
       "      <td>-0.161386</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>-0.048647</td>\n",
       "      <td>0.091294</td>\n",
       "      <td>0.080597</td>\n",
       "      <td>-0.021760</td>\n",
       "      <td>-0.142458</td>\n",
       "      <td>-0.089401</td>\n",
       "      <td>0.079988</td>\n",
       "      <td>0.044268</td>\n",
       "      <td>0.023918</td>\n",
       "      <td>0.055166</td>\n",
       "      <td>-0.156863</td>\n",
       "      <td>0.023349</td>\n",
       "      <td>-0.007256</td>\n",
       "      <td>-0.209249</td>\n",
       "      <td>0.033818</td>\n",
       "      <td>-0.050444</td>\n",
       "      <td>-0.025666</td>\n",
       "      <td>0.080472</td>\n",
       "      <td>-0.012134</td>\n",
       "      <td>-0.040308</td>\n",
       "      <td>-0.137525</td>\n",
       "      <td>-0.096665</td>\n",
       "      <td>0.030511</td>\n",
       "      <td>0.072876</td>\n",
       "      <td>0.036903</td>\n",
       "      <td>0.033492</td>\n",
       "      <td>-0.041212</td>\n",
       "      <td>0.211322</td>\n",
       "      <td>0.117919</td>\n",
       "      <td>-0.150918</td>\n",
       "      <td>0.044767</td>\n",
       "      <td>-0.002726</td>\n",
       "      <td>0.029759</td>\n",
       "      <td>0.013540</td>\n",
       "      <td>-0.131418</td>\n",
       "      <td>-0.104736</td>\n",
       "      <td>0.034174</td>\n",
       "      <td>0.050819</td>\n",
       "      <td>0.018442</td>\n",
       "      <td>0.077781</td>\n",
       "      <td>0.046125</td>\n",
       "      <td>-0.111147</td>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.033659</td>\n",
       "      <td>-0.008592</td>\n",
       "      <td>-0.456482</td>\n",
       "      <td>0.117502</td>\n",
       "      <td>-0.005938</td>\n",
       "      <td>-0.025376</td>\n",
       "      <td>-0.246166</td>\n",
       "      <td>0.051839</td>\n",
       "      <td>0.154401</td>\n",
       "      <td>0.015551</td>\n",
       "      <td>-0.053432</td>\n",
       "      <td>-0.005057</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>-0.034056</td>\n",
       "      <td>0.040365</td>\n",
       "      <td>0.315636</td>\n",
       "      <td>0.085639</td>\n",
       "      <td>0.072792</td>\n",
       "      <td>-0.299569</td>\n",
       "      <td>0.024816</td>\n",
       "      <td>-0.020253</td>\n",
       "      <td>0.155911</td>\n",
       "      <td>0.063741</td>\n",
       "      <td>0.099955</td>\n",
       "      <td>-0.016189</td>\n",
       "      <td>-0.076691</td>\n",
       "      <td>0.027566</td>\n",
       "      <td>-0.097203</td>\n",
       "      <td>0.145902</td>\n",
       "      <td>-0.068788</td>\n",
       "      <td>-0.006728</td>\n",
       "      <td>-0.144340</td>\n",
       "      <td>-0.119464</td>\n",
       "      <td>0.066691</td>\n",
       "      <td>-0.004603</td>\n",
       "      <td>0.021563</td>\n",
       "      <td>-0.104213</td>\n",
       "      <td>0.227402</td>\n",
       "      <td>0.082774</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>-0.010251</td>\n",
       "      <td>-0.065571</td>\n",
       "      <td>0.092814</td>\n",
       "      <td>0.179859</td>\n",
       "      <td>0.036130</td>\n",
       "      <td>-0.081410</td>\n",
       "      <td>-0.001822</td>\n",
       "      <td>-0.055357</td>\n",
       "      <td>-0.003584</td>\n",
       "      <td>-0.077941</td>\n",
       "      <td>0.082180</td>\n",
       "      <td>-0.046561</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>-0.033371</td>\n",
       "      <td>0.337482</td>\n",
       "      <td>-0.051495</td>\n",
       "      <td>-0.162720</td>\n",
       "      <td>-0.005749</td>\n",
       "      <td>-0.072619</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.036710</td>\n",
       "      <td>-0.048915</td>\n",
       "      <td>0.041892</td>\n",
       "      <td>0.027089</td>\n",
       "      <td>-0.219781</td>\n",
       "      <td>0.030675</td>\n",
       "      <td>-0.023006</td>\n",
       "      <td>-0.089872</td>\n",
       "      <td>0.360061</td>\n",
       "      <td>-0.053082</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>0.094880</td>\n",
       "      <td>0.015317</td>\n",
       "      <td>-0.096937</td>\n",
       "      <td>0.018943</td>\n",
       "      <td>-0.003969</td>\n",
       "      <td>-0.035984</td>\n",
       "      <td>-0.048369</td>\n",
       "      <td>0.060998</td>\n",
       "      <td>0.073692</td>\n",
       "      <td>0.061715</td>\n",
       "      <td>0.111058</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>-0.040368</td>\n",
       "      <td>0.043723</td>\n",
       "      <td>0.100284</td>\n",
       "      <td>0.033970</td>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.026999</td>\n",
       "      <td>-0.055821</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>-0.007683</td>\n",
       "      <td>0.025906</td>\n",
       "      <td>-0.025369</td>\n",
       "      <td>-0.006467</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>-0.021949</td>\n",
       "      <td>0.429622</td>\n",
       "      <td>-0.033201</td>\n",
       "      <td>-0.030904</td>\n",
       "      <td>-0.059534</td>\n",
       "      <td>-0.032022</td>\n",
       "      <td>0.095458</td>\n",
       "      <td>0.019393</td>\n",
       "      <td>0.090257</td>\n",
       "      <td>-0.073353</td>\n",
       "      <td>-0.028819</td>\n",
       "      <td>-0.229775</td>\n",
       "      <td>-0.043473</td>\n",
       "      <td>-0.121500</td>\n",
       "      <td>-0.148785</td>\n",
       "      <td>0.075916</td>\n",
       "      <td>-0.283117</td>\n",
       "      <td>-0.022991</td>\n",
       "      <td>0.020785</td>\n",
       "      <td>0.016990</td>\n",
       "      <td>-0.196026</td>\n",
       "      <td>-0.010207</td>\n",
       "      <td>-0.051163</td>\n",
       "      <td>-0.075333</td>\n",
       "      <td>-0.086511</td>\n",
       "      <td>-0.166143</td>\n",
       "      <td>0.080344</td>\n",
       "      <td>-0.558563</td>\n",
       "      <td>0.053560</td>\n",
       "      <td>0.145832</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>0.126499</td>\n",
       "      <td>-0.094352</td>\n",
       "      <td>-0.125322</td>\n",
       "      <td>-0.116429</td>\n",
       "      <td>-0.130909</td>\n",
       "      <td>-0.035671</td>\n",
       "      <td>-0.072572</td>\n",
       "      <td>0.052242</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>-0.081681</td>\n",
       "      <td>0.087877</td>\n",
       "      <td>-0.084990</td>\n",
       "      <td>0.020652</td>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.095561</td>\n",
       "      <td>-0.402014</td>\n",
       "      <td>-0.192151</td>\n",
       "      <td>0.022985</td>\n",
       "      <td>-0.229041</td>\n",
       "      <td>0.200222</td>\n",
       "      <td>0.044868</td>\n",
       "      <td>0.061355</td>\n",
       "      <td>0.040485</td>\n",
       "      <td>-0.056594</td>\n",
       "      <td>-0.025824</td>\n",
       "      <td>0.025394</td>\n",
       "      <td>0.083090</td>\n",
       "      <td>-0.027917</td>\n",
       "      <td>-0.023553</td>\n",
       "      <td>0.030809</td>\n",
       "      <td>0.071935</td>\n",
       "      <td>0.090839</td>\n",
       "      <td>0.080706</td>\n",
       "      <td>-0.068875</td>\n",
       "      <td>-0.016737</td>\n",
       "      <td>0.101205</td>\n",
       "      <td>-0.222836</td>\n",
       "      <td>-0.167577</td>\n",
       "      <td>-0.026063</td>\n",
       "      <td>0.279652</td>\n",
       "      <td>-0.027509</td>\n",
       "      <td>-0.150861</td>\n",
       "      <td>-0.008252</td>\n",
       "      <td>-0.080945</td>\n",
       "      <td>0.073188</td>\n",
       "      <td>-0.018839</td>\n",
       "      <td>0.022912</td>\n",
       "      <td>-0.117120</td>\n",
       "      <td>0.075245</td>\n",
       "      <td>0.035026</td>\n",
       "      <td>0.029149</td>\n",
       "      <td>-0.134059</td>\n",
       "      <td>-0.083952</td>\n",
       "      <td>0.047504</td>\n",
       "      <td>-0.065258</td>\n",
       "      <td>0.084923</td>\n",
       "      <td>0.030477</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>-0.158138</td>\n",
       "      <td>-0.095859</td>\n",
       "      <td>-0.023190</td>\n",
       "      <td>0.021039</td>\n",
       "      <td>-0.047644</td>\n",
       "      <td>0.005802</td>\n",
       "      <td>0.265743</td>\n",
       "      <td>0.027347</td>\n",
       "      <td>0.055783</td>\n",
       "      <td>-0.086370</td>\n",
       "      <td>-0.043560</td>\n",
       "      <td>-0.230913</td>\n",
       "      <td>0.162414</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>-0.075178</td>\n",
       "      <td>0.025439</td>\n",
       "      <td>-0.004177</td>\n",
       "      <td>0.091818</td>\n",
       "      <td>-0.193978</td>\n",
       "      <td>0.180201</td>\n",
       "      <td>0.017736</td>\n",
       "      <td>-0.053249</td>\n",
       "      <td>0.203157</td>\n",
       "      <td>-0.077929</td>\n",
       "      <td>0.047468</td>\n",
       "      <td>-0.069396</td>\n",
       "      <td>-0.093250</td>\n",
       "      <td>-0.032134</td>\n",
       "      <td>0.148642</td>\n",
       "      <td>-0.052570</td>\n",
       "      <td>-0.027195</td>\n",
       "      <td>0.061382</td>\n",
       "      <td>-0.111911</td>\n",
       "      <td>0.335627</td>\n",
       "      <td>-0.021817</td>\n",
       "      <td>-0.062280</td>\n",
       "      <td>-0.233400</td>\n",
       "      <td>-0.101821</td>\n",
       "      <td>-0.044094</td>\n",
       "      <td>-0.102319</td>\n",
       "      <td>0.346466</td>\n",
       "      <td>0.121806</td>\n",
       "      <td>0.137745</td>\n",
       "      <td>0.028848</td>\n",
       "      <td>0.010625</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.085197</td>\n",
       "      <td>0.050229</td>\n",
       "      <td>0.028222</td>\n",
       "      <td>-0.068229</td>\n",
       "      <td>0.014853</td>\n",
       "      <td>-0.043004</td>\n",
       "      <td>-0.276517</td>\n",
       "      <td>0.067562</td>\n",
       "      <td>-0.047147</td>\n",
       "      <td>-0.124056</td>\n",
       "      <td>0.064848</td>\n",
       "      <td>-0.115610</td>\n",
       "      <td>0.066159</td>\n",
       "      <td>0.051632</td>\n",
       "      <td>-0.059845</td>\n",
       "      <td>-0.087777</td>\n",
       "      <td>-0.037988</td>\n",
       "      <td>-0.106196</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.042743</td>\n",
       "      <td>0.096123</td>\n",
       "      <td>-0.009415</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>-0.018174</td>\n",
       "      <td>0.069503</td>\n",
       "      <td>0.007109</td>\n",
       "      <td>-0.031834</td>\n",
       "      <td>0.093077</td>\n",
       "      <td>0.023096</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>-0.022090</td>\n",
       "      <td>-0.081217</td>\n",
       "      <td>-0.005056</td>\n",
       "      <td>-0.091049</td>\n",
       "      <td>-0.038942</td>\n",
       "      <td>-0.041402</td>\n",
       "      <td>-0.198143</td>\n",
       "      <td>0.014318</td>\n",
       "      <td>0.057298</td>\n",
       "      <td>-0.025789</td>\n",
       "      <td>-0.125352</td>\n",
       "      <td>0.085072</td>\n",
       "      <td>0.054986</td>\n",
       "      <td>0.033365</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>0.083461</td>\n",
       "      <td>0.056320</td>\n",
       "      <td>-0.076958</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>-0.162161</td>\n",
       "      <td>-0.026603</td>\n",
       "      <td>0.074886</td>\n",
       "      <td>-0.087528</td>\n",
       "      <td>-0.091121</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>-0.031144</td>\n",
       "      <td>0.047869</td>\n",
       "      <td>-0.066627</td>\n",
       "      <td>-0.031664</td>\n",
       "      <td>-0.075227</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.048740</td>\n",
       "      <td>0.018969</td>\n",
       "      <td>0.079155</td>\n",
       "      <td>0.046572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>detonate</td>\n",
       "      <td>Ottawa,Ontario Canada</td>\n",
       "      <td>0.076772</td>\n",
       "      <td>-0.009422</td>\n",
       "      <td>0.061870</td>\n",
       "      <td>-0.025135</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.221031</td>\n",
       "      <td>0.119283</td>\n",
       "      <td>-0.201717</td>\n",
       "      <td>0.033278</td>\n",
       "      <td>0.041373</td>\n",
       "      <td>0.030023</td>\n",
       "      <td>-0.016307</td>\n",
       "      <td>-0.016455</td>\n",
       "      <td>-0.007681</td>\n",
       "      <td>0.008890</td>\n",
       "      <td>-0.128569</td>\n",
       "      <td>0.049977</td>\n",
       "      <td>-0.001403</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>-0.029866</td>\n",
       "      <td>-0.043106</td>\n",
       "      <td>-0.018664</td>\n",
       "      <td>-0.061879</td>\n",
       "      <td>0.127563</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>-0.074483</td>\n",
       "      <td>0.033540</td>\n",
       "      <td>-0.053097</td>\n",
       "      <td>0.003718</td>\n",
       "      <td>0.024809</td>\n",
       "      <td>-0.029477</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>-0.032646</td>\n",
       "      <td>0.083991</td>\n",
       "      <td>0.010166</td>\n",
       "      <td>0.100944</td>\n",
       "      <td>0.012365</td>\n",
       "      <td>-0.036715</td>\n",
       "      <td>0.086565</td>\n",
       "      <td>-0.027134</td>\n",
       "      <td>-0.024769</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>-0.028257</td>\n",
       "      <td>-0.110640</td>\n",
       "      <td>-0.002861</td>\n",
       "      <td>0.022619</td>\n",
       "      <td>-0.041292</td>\n",
       "      <td>0.019838</td>\n",
       "      <td>0.034354</td>\n",
       "      <td>-0.025519</td>\n",
       "      <td>-0.113131</td>\n",
       "      <td>0.081798</td>\n",
       "      <td>-0.003396</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>0.075293</td>\n",
       "      <td>0.023393</td>\n",
       "      <td>0.096405</td>\n",
       "      <td>-0.004213</td>\n",
       "      <td>-0.045124</td>\n",
       "      <td>0.088837</td>\n",
       "      <td>-0.044486</td>\n",
       "      <td>-0.001491</td>\n",
       "      <td>-0.040362</td>\n",
       "      <td>-0.069464</td>\n",
       "      <td>-0.036309</td>\n",
       "      <td>0.117349</td>\n",
       "      <td>0.043321</td>\n",
       "      <td>0.029023</td>\n",
       "      <td>0.077086</td>\n",
       "      <td>-0.067572</td>\n",
       "      <td>-0.062553</td>\n",
       "      <td>-0.021399</td>\n",
       "      <td>0.036511</td>\n",
       "      <td>-0.099444</td>\n",
       "      <td>-0.107931</td>\n",
       "      <td>-0.058386</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>-0.016492</td>\n",
       "      <td>0.033839</td>\n",
       "      <td>0.191649</td>\n",
       "      <td>-0.059045</td>\n",
       "      <td>-0.023928</td>\n",
       "      <td>-0.020682</td>\n",
       "      <td>-0.055026</td>\n",
       "      <td>0.021425</td>\n",
       "      <td>0.024035</td>\n",
       "      <td>0.047317</td>\n",
       "      <td>0.091284</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.007633</td>\n",
       "      <td>0.051004</td>\n",
       "      <td>-0.056929</td>\n",
       "      <td>-0.019809</td>\n",
       "      <td>0.032183</td>\n",
       "      <td>0.016204</td>\n",
       "      <td>-0.013540</td>\n",
       "      <td>0.016958</td>\n",
       "      <td>-0.044711</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>-0.009641</td>\n",
       "      <td>0.055412</td>\n",
       "      <td>0.102085</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>-0.056485</td>\n",
       "      <td>-0.103716</td>\n",
       "      <td>0.022407</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.024395</td>\n",
       "      <td>0.135860</td>\n",
       "      <td>-0.093317</td>\n",
       "      <td>0.081330</td>\n",
       "      <td>-0.011681</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>0.075054</td>\n",
       "      <td>0.003573</td>\n",
       "      <td>-0.120285</td>\n",
       "      <td>0.179881</td>\n",
       "      <td>0.096787</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>-0.101660</td>\n",
       "      <td>-0.043092</td>\n",
       "      <td>-0.030881</td>\n",
       "      <td>0.026051</td>\n",
       "      <td>0.079291</td>\n",
       "      <td>-0.002108</td>\n",
       "      <td>-0.031002</td>\n",
       "      <td>0.043863</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>0.042727</td>\n",
       "      <td>0.203049</td>\n",
       "      <td>-0.013365</td>\n",
       "      <td>-0.142056</td>\n",
       "      <td>-0.167149</td>\n",
       "      <td>-0.026748</td>\n",
       "      <td>0.071655</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.081047</td>\n",
       "      <td>-0.038267</td>\n",
       "      <td>-0.165394</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.089230</td>\n",
       "      <td>-0.059361</td>\n",
       "      <td>-0.038215</td>\n",
       "      <td>0.055699</td>\n",
       "      <td>0.067036</td>\n",
       "      <td>-0.113201</td>\n",
       "      <td>-0.109533</td>\n",
       "      <td>-0.021301</td>\n",
       "      <td>-0.078344</td>\n",
       "      <td>-0.042202</td>\n",
       "      <td>0.063652</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.040017</td>\n",
       "      <td>-0.005956</td>\n",
       "      <td>-0.114823</td>\n",
       "      <td>0.100061</td>\n",
       "      <td>0.027152</td>\n",
       "      <td>-0.070969</td>\n",
       "      <td>0.134752</td>\n",
       "      <td>-0.086503</td>\n",
       "      <td>0.109754</td>\n",
       "      <td>-0.036543</td>\n",
       "      <td>-0.043200</td>\n",
       "      <td>-0.005840</td>\n",
       "      <td>0.042280</td>\n",
       "      <td>0.053347</td>\n",
       "      <td>-0.015775</td>\n",
       "      <td>-0.084986</td>\n",
       "      <td>0.051919</td>\n",
       "      <td>0.114763</td>\n",
       "      <td>0.092262</td>\n",
       "      <td>-0.020201</td>\n",
       "      <td>0.041950</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.043280</td>\n",
       "      <td>0.014517</td>\n",
       "      <td>-0.015101</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>0.070998</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>-0.036263</td>\n",
       "      <td>-0.027543</td>\n",
       "      <td>0.052016</td>\n",
       "      <td>0.056564</td>\n",
       "      <td>-0.006345</td>\n",
       "      <td>0.090476</td>\n",
       "      <td>0.048256</td>\n",
       "      <td>0.071483</td>\n",
       "      <td>-0.106441</td>\n",
       "      <td>0.016749</td>\n",
       "      <td>-0.039913</td>\n",
       "      <td>0.056311</td>\n",
       "      <td>-0.047304</td>\n",
       "      <td>0.013537</td>\n",
       "      <td>-0.034271</td>\n",
       "      <td>-0.009718</td>\n",
       "      <td>-0.013153</td>\n",
       "      <td>0.091932</td>\n",
       "      <td>0.032270</td>\n",
       "      <td>-0.057416</td>\n",
       "      <td>-0.186979</td>\n",
       "      <td>-0.022420</td>\n",
       "      <td>-0.005539</td>\n",
       "      <td>-0.023661</td>\n",
       "      <td>-0.048488</td>\n",
       "      <td>0.011585</td>\n",
       "      <td>0.044182</td>\n",
       "      <td>0.045509</td>\n",
       "      <td>-0.125807</td>\n",
       "      <td>-0.035248</td>\n",
       "      <td>-0.220456</td>\n",
       "      <td>0.087313</td>\n",
       "      <td>-0.112814</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.065266</td>\n",
       "      <td>0.059890</td>\n",
       "      <td>-0.179772</td>\n",
       "      <td>-0.045213</td>\n",
       "      <td>-0.051858</td>\n",
       "      <td>0.015914</td>\n",
       "      <td>-0.030674</td>\n",
       "      <td>-0.043773</td>\n",
       "      <td>0.062341</td>\n",
       "      <td>0.035543</td>\n",
       "      <td>-0.038183</td>\n",
       "      <td>0.061389</td>\n",
       "      <td>0.013797</td>\n",
       "      <td>-0.037642</td>\n",
       "      <td>-0.024403</td>\n",
       "      <td>-0.117666</td>\n",
       "      <td>0.017088</td>\n",
       "      <td>-0.020961</td>\n",
       "      <td>0.029008</td>\n",
       "      <td>-0.012717</td>\n",
       "      <td>0.093314</td>\n",
       "      <td>-0.000692</td>\n",
       "      <td>-0.080334</td>\n",
       "      <td>0.022392</td>\n",
       "      <td>-0.072224</td>\n",
       "      <td>0.058365</td>\n",
       "      <td>-0.019727</td>\n",
       "      <td>0.092210</td>\n",
       "      <td>-0.041305</td>\n",
       "      <td>0.143757</td>\n",
       "      <td>-0.120874</td>\n",
       "      <td>-0.038706</td>\n",
       "      <td>0.113877</td>\n",
       "      <td>0.033809</td>\n",
       "      <td>-0.025668</td>\n",
       "      <td>-0.069246</td>\n",
       "      <td>-0.035354</td>\n",
       "      <td>0.011508</td>\n",
       "      <td>-0.043784</td>\n",
       "      <td>0.084439</td>\n",
       "      <td>-0.022049</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.029447</td>\n",
       "      <td>-0.057165</td>\n",
       "      <td>0.020126</td>\n",
       "      <td>0.048529</td>\n",
       "      <td>0.024742</td>\n",
       "      <td>0.133450</td>\n",
       "      <td>0.084162</td>\n",
       "      <td>-0.066288</td>\n",
       "      <td>-0.188160</td>\n",
       "      <td>0.033978</td>\n",
       "      <td>0.307596</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>-0.072351</td>\n",
       "      <td>0.023614</td>\n",
       "      <td>-0.022777</td>\n",
       "      <td>0.079656</td>\n",
       "      <td>0.032366</td>\n",
       "      <td>-0.040396</td>\n",
       "      <td>-0.113505</td>\n",
       "      <td>0.069414</td>\n",
       "      <td>-0.067685</td>\n",
       "      <td>-0.079141</td>\n",
       "      <td>-0.193989</td>\n",
       "      <td>-0.018060</td>\n",
       "      <td>0.117052</td>\n",
       "      <td>-0.050058</td>\n",
       "      <td>-0.024707</td>\n",
       "      <td>-0.038166</td>\n",
       "      <td>0.010074</td>\n",
       "      <td>0.254532</td>\n",
       "      <td>0.037029</td>\n",
       "      <td>-0.006374</td>\n",
       "      <td>-0.022722</td>\n",
       "      <td>-0.079466</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>-0.168873</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>-0.032920</td>\n",
       "      <td>-0.117892</td>\n",
       "      <td>0.086632</td>\n",
       "      <td>-0.091635</td>\n",
       "      <td>0.183175</td>\n",
       "      <td>-0.011716</td>\n",
       "      <td>-0.046603</td>\n",
       "      <td>0.028512</td>\n",
       "      <td>-0.061112</td>\n",
       "      <td>-0.056140</td>\n",
       "      <td>-0.060802</td>\n",
       "      <td>0.129775</td>\n",
       "      <td>0.057223</td>\n",
       "      <td>0.066347</td>\n",
       "      <td>0.029194</td>\n",
       "      <td>0.009913</td>\n",
       "      <td>0.015222</td>\n",
       "      <td>0.052693</td>\n",
       "      <td>-0.175844</td>\n",
       "      <td>-0.011733</td>\n",
       "      <td>0.081210</td>\n",
       "      <td>-0.068074</td>\n",
       "      <td>0.030599</td>\n",
       "      <td>-0.046508</td>\n",
       "      <td>-0.004292</td>\n",
       "      <td>0.199841</td>\n",
       "      <td>-0.049878</td>\n",
       "      <td>-0.004792</td>\n",
       "      <td>-0.039171</td>\n",
       "      <td>0.044562</td>\n",
       "      <td>0.051587</td>\n",
       "      <td>-0.018043</td>\n",
       "      <td>-0.074874</td>\n",
       "      <td>0.011548</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.020170</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>0.046067</td>\n",
       "      <td>0.033544</td>\n",
       "      <td>-0.035378</td>\n",
       "      <td>-0.070080</td>\n",
       "      <td>0.059558</td>\n",
       "      <td>-0.096781</td>\n",
       "      <td>-0.231131</td>\n",
       "      <td>0.034348</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>-0.126753</td>\n",
       "      <td>-0.023826</td>\n",
       "      <td>-0.104802</td>\n",
       "      <td>0.014942</td>\n",
       "      <td>-0.069139</td>\n",
       "      <td>-0.163629</td>\n",
       "      <td>-0.119735</td>\n",
       "      <td>-0.095954</td>\n",
       "      <td>0.043693</td>\n",
       "      <td>0.035128</td>\n",
       "      <td>0.173974</td>\n",
       "      <td>0.031327</td>\n",
       "      <td>-0.032793</td>\n",
       "      <td>-0.032260</td>\n",
       "      <td>-0.006919</td>\n",
       "      <td>0.016717</td>\n",
       "      <td>-0.079024</td>\n",
       "      <td>0.036072</td>\n",
       "      <td>-0.000882</td>\n",
       "      <td>0.100534</td>\n",
       "      <td>0.096244</td>\n",
       "      <td>0.050173</td>\n",
       "      <td>-0.092602</td>\n",
       "      <td>0.041574</td>\n",
       "      <td>-0.017257</td>\n",
       "      <td>-0.085330</td>\n",
       "      <td>0.008313</td>\n",
       "      <td>-0.008547</td>\n",
       "      <td>-0.009856</td>\n",
       "      <td>0.052860</td>\n",
       "      <td>0.208742</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>-0.010635</td>\n",
       "      <td>0.013093</td>\n",
       "      <td>-0.067793</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.062929</td>\n",
       "      <td>0.015322</td>\n",
       "      <td>-0.050748</td>\n",
       "      <td>0.010210</td>\n",
       "      <td>-0.003890</td>\n",
       "      <td>-0.139615</td>\n",
       "      <td>-0.087335</td>\n",
       "      <td>0.017447</td>\n",
       "      <td>-0.081990</td>\n",
       "      <td>-0.044522</td>\n",
       "      <td>0.033681</td>\n",
       "      <td>-0.032722</td>\n",
       "      <td>0.083178</td>\n",
       "      <td>0.064989</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>-0.045619</td>\n",
       "      <td>-0.065288</td>\n",
       "      <td>-0.023953</td>\n",
       "      <td>0.013140</td>\n",
       "      <td>-0.058820</td>\n",
       "      <td>-0.089803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7327</th>\n",
       "      <td>wildfire</td>\n",
       "      <td>Get our App</td>\n",
       "      <td>0.052147</td>\n",
       "      <td>0.090394</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>0.161467</td>\n",
       "      <td>-0.142861</td>\n",
       "      <td>0.250441</td>\n",
       "      <td>0.138730</td>\n",
       "      <td>-0.252038</td>\n",
       "      <td>-0.165468</td>\n",
       "      <td>0.078597</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>-0.078334</td>\n",
       "      <td>0.118265</td>\n",
       "      <td>0.022107</td>\n",
       "      <td>-0.071972</td>\n",
       "      <td>0.125892</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>-0.101900</td>\n",
       "      <td>0.215773</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.031172</td>\n",
       "      <td>-0.010763</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.138467</td>\n",
       "      <td>0.071947</td>\n",
       "      <td>0.062064</td>\n",
       "      <td>-0.011624</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>0.022902</td>\n",
       "      <td>0.020493</td>\n",
       "      <td>-0.016222</td>\n",
       "      <td>-0.120535</td>\n",
       "      <td>-0.128217</td>\n",
       "      <td>0.221550</td>\n",
       "      <td>-0.013615</td>\n",
       "      <td>-0.013066</td>\n",
       "      <td>0.124005</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.197091</td>\n",
       "      <td>-0.152057</td>\n",
       "      <td>-0.094180</td>\n",
       "      <td>0.106186</td>\n",
       "      <td>-0.078210</td>\n",
       "      <td>-0.074427</td>\n",
       "      <td>-0.023324</td>\n",
       "      <td>-0.028359</td>\n",
       "      <td>-0.077021</td>\n",
       "      <td>0.091876</td>\n",
       "      <td>0.020583</td>\n",
       "      <td>-0.085111</td>\n",
       "      <td>-0.086037</td>\n",
       "      <td>0.220461</td>\n",
       "      <td>-0.048812</td>\n",
       "      <td>0.009426</td>\n",
       "      <td>0.068172</td>\n",
       "      <td>0.047805</td>\n",
       "      <td>0.184328</td>\n",
       "      <td>-0.108393</td>\n",
       "      <td>-0.047176</td>\n",
       "      <td>0.073170</td>\n",
       "      <td>-0.034816</td>\n",
       "      <td>-0.047624</td>\n",
       "      <td>-0.105405</td>\n",
       "      <td>-0.144970</td>\n",
       "      <td>-0.144752</td>\n",
       "      <td>0.135917</td>\n",
       "      <td>0.108882</td>\n",
       "      <td>-0.015440</td>\n",
       "      <td>0.166591</td>\n",
       "      <td>-0.195019</td>\n",
       "      <td>-0.050716</td>\n",
       "      <td>-0.009296</td>\n",
       "      <td>-0.347309</td>\n",
       "      <td>-0.013923</td>\n",
       "      <td>-0.075162</td>\n",
       "      <td>-0.093800</td>\n",
       "      <td>0.061701</td>\n",
       "      <td>-0.087544</td>\n",
       "      <td>-0.070024</td>\n",
       "      <td>-0.085788</td>\n",
       "      <td>-0.264058</td>\n",
       "      <td>0.028723</td>\n",
       "      <td>-0.076834</td>\n",
       "      <td>-0.064000</td>\n",
       "      <td>0.162530</td>\n",
       "      <td>0.128846</td>\n",
       "      <td>0.081804</td>\n",
       "      <td>0.101113</td>\n",
       "      <td>-0.106543</td>\n",
       "      <td>0.083207</td>\n",
       "      <td>-0.043866</td>\n",
       "      <td>0.009763</td>\n",
       "      <td>-0.068339</td>\n",
       "      <td>-0.009455</td>\n",
       "      <td>-0.040249</td>\n",
       "      <td>0.106594</td>\n",
       "      <td>0.069571</td>\n",
       "      <td>0.007635</td>\n",
       "      <td>-0.048255</td>\n",
       "      <td>-0.002226</td>\n",
       "      <td>0.054911</td>\n",
       "      <td>0.116202</td>\n",
       "      <td>-0.068188</td>\n",
       "      <td>-0.183328</td>\n",
       "      <td>-0.117134</td>\n",
       "      <td>0.116497</td>\n",
       "      <td>-0.039680</td>\n",
       "      <td>0.023109</td>\n",
       "      <td>-0.313292</td>\n",
       "      <td>-0.099692</td>\n",
       "      <td>0.229336</td>\n",
       "      <td>0.077413</td>\n",
       "      <td>-0.032113</td>\n",
       "      <td>0.025033</td>\n",
       "      <td>0.051325</td>\n",
       "      <td>0.023646</td>\n",
       "      <td>-0.134149</td>\n",
       "      <td>0.314615</td>\n",
       "      <td>0.075463</td>\n",
       "      <td>0.131048</td>\n",
       "      <td>-0.227757</td>\n",
       "      <td>-0.009252</td>\n",
       "      <td>-0.055758</td>\n",
       "      <td>0.066253</td>\n",
       "      <td>0.111625</td>\n",
       "      <td>-0.061908</td>\n",
       "      <td>0.075852</td>\n",
       "      <td>0.023028</td>\n",
       "      <td>0.030712</td>\n",
       "      <td>0.054786</td>\n",
       "      <td>0.128736</td>\n",
       "      <td>-0.055650</td>\n",
       "      <td>-0.175433</td>\n",
       "      <td>-0.047186</td>\n",
       "      <td>-0.128244</td>\n",
       "      <td>0.035030</td>\n",
       "      <td>-0.048177</td>\n",
       "      <td>0.047822</td>\n",
       "      <td>-0.105063</td>\n",
       "      <td>0.264091</td>\n",
       "      <td>0.155056</td>\n",
       "      <td>0.091512</td>\n",
       "      <td>-0.024461</td>\n",
       "      <td>-0.072652</td>\n",
       "      <td>0.073239</td>\n",
       "      <td>0.183841</td>\n",
       "      <td>-0.149928</td>\n",
       "      <td>-0.047980</td>\n",
       "      <td>-0.103283</td>\n",
       "      <td>0.071798</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>0.056020</td>\n",
       "      <td>0.051052</td>\n",
       "      <td>0.303570</td>\n",
       "      <td>0.054536</td>\n",
       "      <td>-0.118511</td>\n",
       "      <td>0.255546</td>\n",
       "      <td>0.054136</td>\n",
       "      <td>-0.192075</td>\n",
       "      <td>0.082022</td>\n",
       "      <td>-0.188672</td>\n",
       "      <td>0.047393</td>\n",
       "      <td>0.053273</td>\n",
       "      <td>-0.069027</td>\n",
       "      <td>-0.066434</td>\n",
       "      <td>0.013831</td>\n",
       "      <td>0.136238</td>\n",
       "      <td>-0.057112</td>\n",
       "      <td>-0.011880</td>\n",
       "      <td>0.025733</td>\n",
       "      <td>0.116344</td>\n",
       "      <td>0.233226</td>\n",
       "      <td>-0.073614</td>\n",
       "      <td>0.163673</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>0.033126</td>\n",
       "      <td>0.063925</td>\n",
       "      <td>0.070427</td>\n",
       "      <td>-0.044817</td>\n",
       "      <td>-0.035937</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.086719</td>\n",
       "      <td>0.028757</td>\n",
       "      <td>0.012157</td>\n",
       "      <td>0.051671</td>\n",
       "      <td>0.043216</td>\n",
       "      <td>0.054570</td>\n",
       "      <td>0.127827</td>\n",
       "      <td>0.225465</td>\n",
       "      <td>0.275024</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>-0.106282</td>\n",
       "      <td>-0.061039</td>\n",
       "      <td>-0.017668</td>\n",
       "      <td>0.039733</td>\n",
       "      <td>0.033504</td>\n",
       "      <td>-0.054369</td>\n",
       "      <td>0.108110</td>\n",
       "      <td>-0.048179</td>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.231308</td>\n",
       "      <td>0.073776</td>\n",
       "      <td>-0.001994</td>\n",
       "      <td>-0.056409</td>\n",
       "      <td>-0.078925</td>\n",
       "      <td>0.004652</td>\n",
       "      <td>-0.111710</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>-0.023877</td>\n",
       "      <td>0.131118</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>-0.317899</td>\n",
       "      <td>-0.028272</td>\n",
       "      <td>-0.312378</td>\n",
       "      <td>-0.035981</td>\n",
       "      <td>-0.250712</td>\n",
       "      <td>0.203697</td>\n",
       "      <td>0.017527</td>\n",
       "      <td>0.138979</td>\n",
       "      <td>-0.364084</td>\n",
       "      <td>-0.065865</td>\n",
       "      <td>-0.074282</td>\n",
       "      <td>0.045063</td>\n",
       "      <td>-0.028241</td>\n",
       "      <td>-0.188848</td>\n",
       "      <td>0.024797</td>\n",
       "      <td>-0.403424</td>\n",
       "      <td>-0.026914</td>\n",
       "      <td>0.137284</td>\n",
       "      <td>-0.081136</td>\n",
       "      <td>0.009143</td>\n",
       "      <td>-0.049258</td>\n",
       "      <td>-0.263930</td>\n",
       "      <td>-0.058526</td>\n",
       "      <td>0.024715</td>\n",
       "      <td>0.037658</td>\n",
       "      <td>-0.034710</td>\n",
       "      <td>0.183285</td>\n",
       "      <td>-0.044158</td>\n",
       "      <td>-0.083744</td>\n",
       "      <td>-0.049790</td>\n",
       "      <td>-0.128675</td>\n",
       "      <td>0.124303</td>\n",
       "      <td>-0.073716</td>\n",
       "      <td>0.033981</td>\n",
       "      <td>-0.361086</td>\n",
       "      <td>0.086361</td>\n",
       "      <td>0.022701</td>\n",
       "      <td>-0.324658</td>\n",
       "      <td>0.360100</td>\n",
       "      <td>0.046320</td>\n",
       "      <td>0.231939</td>\n",
       "      <td>-0.134205</td>\n",
       "      <td>-0.012160</td>\n",
       "      <td>-0.064244</td>\n",
       "      <td>-0.016295</td>\n",
       "      <td>0.125519</td>\n",
       "      <td>0.048745</td>\n",
       "      <td>-0.034116</td>\n",
       "      <td>-0.071038</td>\n",
       "      <td>-0.049975</td>\n",
       "      <td>0.127318</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.009611</td>\n",
       "      <td>0.060309</td>\n",
       "      <td>0.093200</td>\n",
       "      <td>-0.121046</td>\n",
       "      <td>-0.239241</td>\n",
       "      <td>-0.118868</td>\n",
       "      <td>0.312144</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>-0.176653</td>\n",
       "      <td>-0.063413</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.112068</td>\n",
       "      <td>0.086570</td>\n",
       "      <td>-0.030476</td>\n",
       "      <td>-0.124310</td>\n",
       "      <td>0.103256</td>\n",
       "      <td>-0.071358</td>\n",
       "      <td>-0.122346</td>\n",
       "      <td>-0.185804</td>\n",
       "      <td>0.046178</td>\n",
       "      <td>0.042529</td>\n",
       "      <td>-0.185285</td>\n",
       "      <td>0.105276</td>\n",
       "      <td>-0.033871</td>\n",
       "      <td>-0.105983</td>\n",
       "      <td>-0.052381</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>0.079107</td>\n",
       "      <td>-0.016631</td>\n",
       "      <td>-0.023949</td>\n",
       "      <td>-0.005141</td>\n",
       "      <td>0.253887</td>\n",
       "      <td>0.011397</td>\n",
       "      <td>-0.065441</td>\n",
       "      <td>0.042139</td>\n",
       "      <td>-0.019822</td>\n",
       "      <td>-0.164667</td>\n",
       "      <td>0.339570</td>\n",
       "      <td>-0.041138</td>\n",
       "      <td>-0.105374</td>\n",
       "      <td>0.021269</td>\n",
       "      <td>-0.065300</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>-0.194524</td>\n",
       "      <td>0.295086</td>\n",
       "      <td>-0.012515</td>\n",
       "      <td>0.103842</td>\n",
       "      <td>0.269902</td>\n",
       "      <td>0.044185</td>\n",
       "      <td>-0.028330</td>\n",
       "      <td>-0.042090</td>\n",
       "      <td>-0.079859</td>\n",
       "      <td>0.106976</td>\n",
       "      <td>0.083266</td>\n",
       "      <td>-0.036975</td>\n",
       "      <td>0.029714</td>\n",
       "      <td>-0.092788</td>\n",
       "      <td>-0.042043</td>\n",
       "      <td>0.328900</td>\n",
       "      <td>-0.164841</td>\n",
       "      <td>-0.021572</td>\n",
       "      <td>-0.132315</td>\n",
       "      <td>0.039459</td>\n",
       "      <td>-0.050048</td>\n",
       "      <td>0.017940</td>\n",
       "      <td>0.130960</td>\n",
       "      <td>0.209079</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.133440</td>\n",
       "      <td>-0.038668</td>\n",
       "      <td>0.004475</td>\n",
       "      <td>0.066080</td>\n",
       "      <td>-0.020742</td>\n",
       "      <td>0.130055</td>\n",
       "      <td>-0.058617</td>\n",
       "      <td>0.033791</td>\n",
       "      <td>-0.186176</td>\n",
       "      <td>-0.309000</td>\n",
       "      <td>0.055795</td>\n",
       "      <td>0.066269</td>\n",
       "      <td>-0.179551</td>\n",
       "      <td>0.041571</td>\n",
       "      <td>-0.175064</td>\n",
       "      <td>-0.028395</td>\n",
       "      <td>-0.126166</td>\n",
       "      <td>-0.164619</td>\n",
       "      <td>-0.180996</td>\n",
       "      <td>-0.255245</td>\n",
       "      <td>0.049988</td>\n",
       "      <td>0.008129</td>\n",
       "      <td>0.362056</td>\n",
       "      <td>0.163455</td>\n",
       "      <td>0.013765</td>\n",
       "      <td>0.139762</td>\n",
       "      <td>-0.057764</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>-0.066871</td>\n",
       "      <td>-0.024643</td>\n",
       "      <td>-0.089311</td>\n",
       "      <td>-0.017157</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>0.060749</td>\n",
       "      <td>-0.031982</td>\n",
       "      <td>-0.135342</td>\n",
       "      <td>-0.093948</td>\n",
       "      <td>-0.139607</td>\n",
       "      <td>-0.043546</td>\n",
       "      <td>-0.005756</td>\n",
       "      <td>-0.135909</td>\n",
       "      <td>0.053447</td>\n",
       "      <td>0.266613</td>\n",
       "      <td>-0.138522</td>\n",
       "      <td>-0.124166</td>\n",
       "      <td>-0.152047</td>\n",
       "      <td>-0.104829</td>\n",
       "      <td>-0.026241</td>\n",
       "      <td>0.091146</td>\n",
       "      <td>0.056282</td>\n",
       "      <td>-0.017280</td>\n",
       "      <td>-0.131336</td>\n",
       "      <td>-0.137343</td>\n",
       "      <td>0.081862</td>\n",
       "      <td>-0.177457</td>\n",
       "      <td>-0.006035</td>\n",
       "      <td>-0.037148</td>\n",
       "      <td>0.100874</td>\n",
       "      <td>0.060145</td>\n",
       "      <td>-0.088073</td>\n",
       "      <td>0.017432</td>\n",
       "      <td>0.237457</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.017306</td>\n",
       "      <td>-0.084485</td>\n",
       "      <td>-0.062813</td>\n",
       "      <td>0.079334</td>\n",
       "      <td>0.276254</td>\n",
       "      <td>-0.071629</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>bridge%20collapse</td>\n",
       "      <td>#na#</td>\n",
       "      <td>0.076157</td>\n",
       "      <td>0.046072</td>\n",
       "      <td>-0.044841</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>0.106724</td>\n",
       "      <td>0.036105</td>\n",
       "      <td>-0.038526</td>\n",
       "      <td>-0.199657</td>\n",
       "      <td>-0.138474</td>\n",
       "      <td>-0.012390</td>\n",
       "      <td>0.042626</td>\n",
       "      <td>-0.143135</td>\n",
       "      <td>-0.110817</td>\n",
       "      <td>0.027772</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.293875</td>\n",
       "      <td>0.057075</td>\n",
       "      <td>-0.024148</td>\n",
       "      <td>0.030901</td>\n",
       "      <td>0.059134</td>\n",
       "      <td>0.044989</td>\n",
       "      <td>-0.038601</td>\n",
       "      <td>-0.062257</td>\n",
       "      <td>0.075570</td>\n",
       "      <td>-0.068666</td>\n",
       "      <td>0.071949</td>\n",
       "      <td>-0.083693</td>\n",
       "      <td>-0.043764</td>\n",
       "      <td>0.153285</td>\n",
       "      <td>-0.090170</td>\n",
       "      <td>-0.109058</td>\n",
       "      <td>0.039317</td>\n",
       "      <td>-0.108658</td>\n",
       "      <td>-0.004070</td>\n",
       "      <td>0.040528</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>-0.018129</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>-0.174670</td>\n",
       "      <td>-0.154966</td>\n",
       "      <td>0.106858</td>\n",
       "      <td>0.174022</td>\n",
       "      <td>-0.032018</td>\n",
       "      <td>-0.012950</td>\n",
       "      <td>0.057767</td>\n",
       "      <td>0.210306</td>\n",
       "      <td>0.039637</td>\n",
       "      <td>0.098416</td>\n",
       "      <td>0.081849</td>\n",
       "      <td>-0.094543</td>\n",
       "      <td>-0.120487</td>\n",
       "      <td>0.169401</td>\n",
       "      <td>-0.006240</td>\n",
       "      <td>0.138731</td>\n",
       "      <td>0.027297</td>\n",
       "      <td>-0.086962</td>\n",
       "      <td>-0.064491</td>\n",
       "      <td>-0.103368</td>\n",
       "      <td>-0.142048</td>\n",
       "      <td>0.087953</td>\n",
       "      <td>0.033641</td>\n",
       "      <td>-0.095292</td>\n",
       "      <td>0.039676</td>\n",
       "      <td>-0.099926</td>\n",
       "      <td>0.017661</td>\n",
       "      <td>-0.048095</td>\n",
       "      <td>0.100092</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>-0.204829</td>\n",
       "      <td>0.021614</td>\n",
       "      <td>-0.037125</td>\n",
       "      <td>-0.096369</td>\n",
       "      <td>-0.082469</td>\n",
       "      <td>-0.087878</td>\n",
       "      <td>0.041962</td>\n",
       "      <td>0.026663</td>\n",
       "      <td>-0.136855</td>\n",
       "      <td>0.088652</td>\n",
       "      <td>-0.029931</td>\n",
       "      <td>-0.054056</td>\n",
       "      <td>-0.003205</td>\n",
       "      <td>-0.195866</td>\n",
       "      <td>0.050954</td>\n",
       "      <td>0.029326</td>\n",
       "      <td>-0.036530</td>\n",
       "      <td>-0.153918</td>\n",
       "      <td>0.101796</td>\n",
       "      <td>-0.130579</td>\n",
       "      <td>0.070396</td>\n",
       "      <td>0.022571</td>\n",
       "      <td>-0.095896</td>\n",
       "      <td>-0.072800</td>\n",
       "      <td>0.021590</td>\n",
       "      <td>0.033249</td>\n",
       "      <td>-0.007708</td>\n",
       "      <td>-0.025478</td>\n",
       "      <td>-0.024938</td>\n",
       "      <td>0.096282</td>\n",
       "      <td>0.090081</td>\n",
       "      <td>-0.055158</td>\n",
       "      <td>0.158666</td>\n",
       "      <td>-0.026524</td>\n",
       "      <td>-0.069272</td>\n",
       "      <td>0.099055</td>\n",
       "      <td>0.092168</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>0.015191</td>\n",
       "      <td>0.222844</td>\n",
       "      <td>-0.032421</td>\n",
       "      <td>-0.014357</td>\n",
       "      <td>0.084397</td>\n",
       "      <td>-0.018161</td>\n",
       "      <td>-0.072707</td>\n",
       "      <td>0.020958</td>\n",
       "      <td>-0.042648</td>\n",
       "      <td>-0.102020</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>-0.007807</td>\n",
       "      <td>0.117124</td>\n",
       "      <td>-0.005372</td>\n",
       "      <td>0.117712</td>\n",
       "      <td>-0.123838</td>\n",
       "      <td>0.183296</td>\n",
       "      <td>0.075648</td>\n",
       "      <td>-0.062829</td>\n",
       "      <td>0.172454</td>\n",
       "      <td>-0.142891</td>\n",
       "      <td>-0.209559</td>\n",
       "      <td>0.186989</td>\n",
       "      <td>0.078552</td>\n",
       "      <td>-0.120031</td>\n",
       "      <td>-0.133607</td>\n",
       "      <td>-0.117011</td>\n",
       "      <td>0.020515</td>\n",
       "      <td>-0.025608</td>\n",
       "      <td>0.063562</td>\n",
       "      <td>-0.034180</td>\n",
       "      <td>-0.138238</td>\n",
       "      <td>0.042997</td>\n",
       "      <td>0.070472</td>\n",
       "      <td>-0.077825</td>\n",
       "      <td>0.089135</td>\n",
       "      <td>0.323033</td>\n",
       "      <td>0.012917</td>\n",
       "      <td>0.110297</td>\n",
       "      <td>-0.008093</td>\n",
       "      <td>0.117527</td>\n",
       "      <td>-0.030785</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>0.103272</td>\n",
       "      <td>0.018570</td>\n",
       "      <td>0.048701</td>\n",
       "      <td>-0.003783</td>\n",
       "      <td>-0.170865</td>\n",
       "      <td>0.114759</td>\n",
       "      <td>-0.005387</td>\n",
       "      <td>-0.113251</td>\n",
       "      <td>-0.151169</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>-0.151886</td>\n",
       "      <td>-0.084469</td>\n",
       "      <td>0.044613</td>\n",
       "      <td>-0.010240</td>\n",
       "      <td>0.039388</td>\n",
       "      <td>-0.033588</td>\n",
       "      <td>-0.187045</td>\n",
       "      <td>-0.075660</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.028756</td>\n",
       "      <td>0.059860</td>\n",
       "      <td>-0.104375</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.160491</td>\n",
       "      <td>0.041606</td>\n",
       "      <td>0.019096</td>\n",
       "      <td>-0.084176</td>\n",
       "      <td>-0.013129</td>\n",
       "      <td>-0.075956</td>\n",
       "      <td>0.042526</td>\n",
       "      <td>-0.004277</td>\n",
       "      <td>-0.046161</td>\n",
       "      <td>0.032182</td>\n",
       "      <td>0.020329</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>-0.017837</td>\n",
       "      <td>-0.012807</td>\n",
       "      <td>0.015610</td>\n",
       "      <td>0.069269</td>\n",
       "      <td>0.107928</td>\n",
       "      <td>-0.105288</td>\n",
       "      <td>0.158021</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.051887</td>\n",
       "      <td>0.077040</td>\n",
       "      <td>0.097750</td>\n",
       "      <td>0.009718</td>\n",
       "      <td>-0.018740</td>\n",
       "      <td>0.087561</td>\n",
       "      <td>-0.085802</td>\n",
       "      <td>0.110411</td>\n",
       "      <td>0.048286</td>\n",
       "      <td>-0.069467</td>\n",
       "      <td>-0.021803</td>\n",
       "      <td>-0.101310</td>\n",
       "      <td>0.036135</td>\n",
       "      <td>0.056836</td>\n",
       "      <td>0.073946</td>\n",
       "      <td>0.078733</td>\n",
       "      <td>-0.018306</td>\n",
       "      <td>-0.161086</td>\n",
       "      <td>-0.032669</td>\n",
       "      <td>-0.097361</td>\n",
       "      <td>-0.018640</td>\n",
       "      <td>0.040365</td>\n",
       "      <td>0.034039</td>\n",
       "      <td>0.114879</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>-0.128178</td>\n",
       "      <td>-0.011083</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.010790</td>\n",
       "      <td>0.092231</td>\n",
       "      <td>-0.066612</td>\n",
       "      <td>-0.029219</td>\n",
       "      <td>-0.015520</td>\n",
       "      <td>-0.029885</td>\n",
       "      <td>0.130964</td>\n",
       "      <td>0.024145</td>\n",
       "      <td>-0.071336</td>\n",
       "      <td>-0.029979</td>\n",
       "      <td>-0.103909</td>\n",
       "      <td>-0.089567</td>\n",
       "      <td>-0.051592</td>\n",
       "      <td>-0.041335</td>\n",
       "      <td>0.061668</td>\n",
       "      <td>-0.136177</td>\n",
       "      <td>-0.025057</td>\n",
       "      <td>-0.102071</td>\n",
       "      <td>0.109930</td>\n",
       "      <td>-0.016247</td>\n",
       "      <td>-0.087660</td>\n",
       "      <td>-0.063681</td>\n",
       "      <td>0.139063</td>\n",
       "      <td>-0.056172</td>\n",
       "      <td>0.197308</td>\n",
       "      <td>-0.148626</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.169714</td>\n",
       "      <td>-0.089685</td>\n",
       "      <td>-0.035014</td>\n",
       "      <td>0.102423</td>\n",
       "      <td>0.073415</td>\n",
       "      <td>0.236146</td>\n",
       "      <td>-0.048325</td>\n",
       "      <td>-0.069980</td>\n",
       "      <td>0.050421</td>\n",
       "      <td>0.110015</td>\n",
       "      <td>0.039152</td>\n",
       "      <td>-0.145187</td>\n",
       "      <td>-0.075351</td>\n",
       "      <td>0.097911</td>\n",
       "      <td>0.145855</td>\n",
       "      <td>-0.038829</td>\n",
       "      <td>0.125558</td>\n",
       "      <td>-0.038062</td>\n",
       "      <td>-0.067217</td>\n",
       "      <td>0.018951</td>\n",
       "      <td>0.222004</td>\n",
       "      <td>0.078664</td>\n",
       "      <td>-0.091455</td>\n",
       "      <td>0.035969</td>\n",
       "      <td>0.016317</td>\n",
       "      <td>0.013944</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>-0.034568</td>\n",
       "      <td>0.008830</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.044172</td>\n",
       "      <td>0.088183</td>\n",
       "      <td>-0.087698</td>\n",
       "      <td>-0.028175</td>\n",
       "      <td>-0.005677</td>\n",
       "      <td>-0.140092</td>\n",
       "      <td>-0.072878</td>\n",
       "      <td>-0.039663</td>\n",
       "      <td>0.098559</td>\n",
       "      <td>0.213364</td>\n",
       "      <td>0.022357</td>\n",
       "      <td>-0.015242</td>\n",
       "      <td>-0.036273</td>\n",
       "      <td>0.200838</td>\n",
       "      <td>0.082106</td>\n",
       "      <td>0.103499</td>\n",
       "      <td>0.037613</td>\n",
       "      <td>-0.053850</td>\n",
       "      <td>-0.104209</td>\n",
       "      <td>0.097099</td>\n",
       "      <td>-0.087050</td>\n",
       "      <td>0.047939</td>\n",
       "      <td>-0.226683</td>\n",
       "      <td>-0.008008</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>-0.150566</td>\n",
       "      <td>-0.106503</td>\n",
       "      <td>0.108156</td>\n",
       "      <td>0.109333</td>\n",
       "      <td>-0.008702</td>\n",
       "      <td>0.029368</td>\n",
       "      <td>0.184676</td>\n",
       "      <td>0.078827</td>\n",
       "      <td>-0.084082</td>\n",
       "      <td>-0.100233</td>\n",
       "      <td>-0.171551</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>-0.082194</td>\n",
       "      <td>-0.045228</td>\n",
       "      <td>0.086921</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.030134</td>\n",
       "      <td>0.048703</td>\n",
       "      <td>-0.064814</td>\n",
       "      <td>-0.074215</td>\n",
       "      <td>0.063652</td>\n",
       "      <td>0.166027</td>\n",
       "      <td>0.068488</td>\n",
       "      <td>-0.133359</td>\n",
       "      <td>-0.177459</td>\n",
       "      <td>-0.094844</td>\n",
       "      <td>-0.226329</td>\n",
       "      <td>0.268662</td>\n",
       "      <td>-0.020056</td>\n",
       "      <td>-0.133383</td>\n",
       "      <td>0.133624</td>\n",
       "      <td>-0.005336</td>\n",
       "      <td>-0.175415</td>\n",
       "      <td>0.055012</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>-0.113676</td>\n",
       "      <td>-0.128984</td>\n",
       "      <td>-0.100965</td>\n",
       "      <td>-0.131628</td>\n",
       "      <td>-0.045595</td>\n",
       "      <td>0.143403</td>\n",
       "      <td>-0.045307</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>-0.112167</td>\n",
       "      <td>-0.018855</td>\n",
       "      <td>-0.031585</td>\n",
       "      <td>-0.098380</td>\n",
       "      <td>0.142013</td>\n",
       "      <td>-0.024153</td>\n",
       "      <td>0.132486</td>\n",
       "      <td>-0.040659</td>\n",
       "      <td>-0.021737</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>-0.047627</td>\n",
       "      <td>0.043053</td>\n",
       "      <td>-0.042993</td>\n",
       "      <td>0.081873</td>\n",
       "      <td>-0.017066</td>\n",
       "      <td>0.034993</td>\n",
       "      <td>0.183152</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>-0.141626</td>\n",
       "      <td>0.069985</td>\n",
       "      <td>0.008282</td>\n",
       "      <td>0.067334</td>\n",
       "      <td>-0.043318</td>\n",
       "      <td>-0.133857</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.033527</td>\n",
       "      <td>-0.090704</td>\n",
       "      <td>0.030341</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.011746</td>\n",
       "      <td>-0.193774</td>\n",
       "      <td>-0.014963</td>\n",
       "      <td>0.028749</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>-0.102207</td>\n",
       "      <td>-0.031076</td>\n",
       "      <td>0.113209</td>\n",
       "      <td>-0.143475</td>\n",
       "      <td>0.318326</td>\n",
       "      <td>-0.074452</td>\n",
       "      <td>-0.086477</td>\n",
       "      <td>-0.076224</td>\n",
       "      <td>0.202623</td>\n",
       "      <td>0.050249</td>\n",
       "      <td>-0.011104</td>\n",
       "      <td>0.074688</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>-0.068442</td>\n",
       "      <td>-0.130258</td>\n",
       "      <td>0.036524</td>\n",
       "      <td>0.044110</td>\n",
       "      <td>-0.045445</td>\n",
       "      <td>-0.116180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6204</th>\n",
       "      <td>smoke</td>\n",
       "      <td>#na#</td>\n",
       "      <td>-0.109571</td>\n",
       "      <td>0.080203</td>\n",
       "      <td>0.010659</td>\n",
       "      <td>0.139558</td>\n",
       "      <td>-0.084731</td>\n",
       "      <td>0.158707</td>\n",
       "      <td>0.109660</td>\n",
       "      <td>-0.328806</td>\n",
       "      <td>-0.090339</td>\n",
       "      <td>0.172090</td>\n",
       "      <td>0.155653</td>\n",
       "      <td>0.030816</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>-0.022595</td>\n",
       "      <td>-0.116938</td>\n",
       "      <td>-0.045542</td>\n",
       "      <td>0.024813</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>-0.011273</td>\n",
       "      <td>-0.033552</td>\n",
       "      <td>0.086389</td>\n",
       "      <td>-0.038980</td>\n",
       "      <td>0.017126</td>\n",
       "      <td>0.130844</td>\n",
       "      <td>-0.002031</td>\n",
       "      <td>-0.099841</td>\n",
       "      <td>0.126279</td>\n",
       "      <td>-0.154905</td>\n",
       "      <td>0.038561</td>\n",
       "      <td>0.199439</td>\n",
       "      <td>0.030630</td>\n",
       "      <td>0.067019</td>\n",
       "      <td>-0.247566</td>\n",
       "      <td>-0.015010</td>\n",
       "      <td>-0.061718</td>\n",
       "      <td>0.068412</td>\n",
       "      <td>-0.142631</td>\n",
       "      <td>-0.097947</td>\n",
       "      <td>-0.048778</td>\n",
       "      <td>-0.015180</td>\n",
       "      <td>0.008241</td>\n",
       "      <td>0.249323</td>\n",
       "      <td>-0.080589</td>\n",
       "      <td>-0.023049</td>\n",
       "      <td>-0.033217</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>0.093263</td>\n",
       "      <td>0.183050</td>\n",
       "      <td>0.196748</td>\n",
       "      <td>-0.057467</td>\n",
       "      <td>-0.136886</td>\n",
       "      <td>0.071653</td>\n",
       "      <td>0.142443</td>\n",
       "      <td>-0.026587</td>\n",
       "      <td>0.099693</td>\n",
       "      <td>-0.085712</td>\n",
       "      <td>0.138293</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>-0.156760</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>0.031582</td>\n",
       "      <td>-0.042841</td>\n",
       "      <td>0.265938</td>\n",
       "      <td>-0.170969</td>\n",
       "      <td>0.111393</td>\n",
       "      <td>-0.027862</td>\n",
       "      <td>-0.001768</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>0.183442</td>\n",
       "      <td>0.095767</td>\n",
       "      <td>-0.038681</td>\n",
       "      <td>-0.014827</td>\n",
       "      <td>-0.134672</td>\n",
       "      <td>0.034048</td>\n",
       "      <td>-0.065250</td>\n",
       "      <td>0.086794</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>-0.256039</td>\n",
       "      <td>0.018510</td>\n",
       "      <td>-0.025198</td>\n",
       "      <td>-0.117142</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.098772</td>\n",
       "      <td>-0.019750</td>\n",
       "      <td>0.072055</td>\n",
       "      <td>0.023816</td>\n",
       "      <td>0.087927</td>\n",
       "      <td>0.131848</td>\n",
       "      <td>0.064541</td>\n",
       "      <td>-0.001467</td>\n",
       "      <td>0.052575</td>\n",
       "      <td>-0.052555</td>\n",
       "      <td>0.019138</td>\n",
       "      <td>-0.013555</td>\n",
       "      <td>-0.144181</td>\n",
       "      <td>-0.039740</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.003655</td>\n",
       "      <td>0.092853</td>\n",
       "      <td>0.165865</td>\n",
       "      <td>-0.069105</td>\n",
       "      <td>0.005712</td>\n",
       "      <td>0.153603</td>\n",
       "      <td>-0.043550</td>\n",
       "      <td>-0.230930</td>\n",
       "      <td>0.078505</td>\n",
       "      <td>-0.011617</td>\n",
       "      <td>-0.020862</td>\n",
       "      <td>-0.020835</td>\n",
       "      <td>-0.091574</td>\n",
       "      <td>0.016059</td>\n",
       "      <td>-0.020286</td>\n",
       "      <td>-0.026782</td>\n",
       "      <td>-0.168413</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>-0.186020</td>\n",
       "      <td>-0.144494</td>\n",
       "      <td>0.158867</td>\n",
       "      <td>-0.044591</td>\n",
       "      <td>0.154294</td>\n",
       "      <td>-0.066651</td>\n",
       "      <td>0.038210</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>-0.159016</td>\n",
       "      <td>-0.016009</td>\n",
       "      <td>0.047586</td>\n",
       "      <td>-0.046696</td>\n",
       "      <td>-0.036877</td>\n",
       "      <td>-0.047314</td>\n",
       "      <td>-0.034766</td>\n",
       "      <td>0.170372</td>\n",
       "      <td>0.034867</td>\n",
       "      <td>-0.139171</td>\n",
       "      <td>-0.039934</td>\n",
       "      <td>-0.005142</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>-0.020220</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>-0.068581</td>\n",
       "      <td>-0.030270</td>\n",
       "      <td>0.069522</td>\n",
       "      <td>0.089998</td>\n",
       "      <td>-0.143487</td>\n",
       "      <td>-0.236634</td>\n",
       "      <td>0.175186</td>\n",
       "      <td>0.151177</td>\n",
       "      <td>-0.088679</td>\n",
       "      <td>-0.237587</td>\n",
       "      <td>0.066289</td>\n",
       "      <td>-0.036923</td>\n",
       "      <td>-0.022748</td>\n",
       "      <td>0.131708</td>\n",
       "      <td>0.055798</td>\n",
       "      <td>0.152579</td>\n",
       "      <td>-0.009236</td>\n",
       "      <td>-0.161716</td>\n",
       "      <td>0.221436</td>\n",
       "      <td>-0.061865</td>\n",
       "      <td>0.021598</td>\n",
       "      <td>-0.172372</td>\n",
       "      <td>-0.044058</td>\n",
       "      <td>0.037272</td>\n",
       "      <td>-0.069708</td>\n",
       "      <td>-0.198973</td>\n",
       "      <td>-0.006615</td>\n",
       "      <td>0.070724</td>\n",
       "      <td>0.181233</td>\n",
       "      <td>-0.068914</td>\n",
       "      <td>0.054507</td>\n",
       "      <td>-0.009064</td>\n",
       "      <td>0.225203</td>\n",
       "      <td>-0.084607</td>\n",
       "      <td>-0.023671</td>\n",
       "      <td>-0.035789</td>\n",
       "      <td>0.108828</td>\n",
       "      <td>-0.062934</td>\n",
       "      <td>-0.005740</td>\n",
       "      <td>0.110318</td>\n",
       "      <td>-0.069510</td>\n",
       "      <td>-0.048510</td>\n",
       "      <td>-0.014742</td>\n",
       "      <td>0.168129</td>\n",
       "      <td>0.071541</td>\n",
       "      <td>0.197550</td>\n",
       "      <td>0.201844</td>\n",
       "      <td>0.032751</td>\n",
       "      <td>0.075960</td>\n",
       "      <td>0.181861</td>\n",
       "      <td>0.122119</td>\n",
       "      <td>-0.016223</td>\n",
       "      <td>0.041461</td>\n",
       "      <td>-0.266188</td>\n",
       "      <td>0.049927</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.092677</td>\n",
       "      <td>-0.079087</td>\n",
       "      <td>-0.011220</td>\n",
       "      <td>-0.010516</td>\n",
       "      <td>-0.041310</td>\n",
       "      <td>-0.066683</td>\n",
       "      <td>0.045342</td>\n",
       "      <td>-0.120163</td>\n",
       "      <td>0.118659</td>\n",
       "      <td>-0.002010</td>\n",
       "      <td>0.042428</td>\n",
       "      <td>0.030304</td>\n",
       "      <td>-0.020003</td>\n",
       "      <td>-0.112315</td>\n",
       "      <td>-0.129164</td>\n",
       "      <td>-0.019864</td>\n",
       "      <td>-0.026157</td>\n",
       "      <td>-0.009350</td>\n",
       "      <td>0.054708</td>\n",
       "      <td>-0.244696</td>\n",
       "      <td>-0.026549</td>\n",
       "      <td>-0.110338</td>\n",
       "      <td>-0.027016</td>\n",
       "      <td>0.034985</td>\n",
       "      <td>0.162559</td>\n",
       "      <td>-0.244653</td>\n",
       "      <td>-0.009336</td>\n",
       "      <td>0.045409</td>\n",
       "      <td>-0.056630</td>\n",
       "      <td>-0.049714</td>\n",
       "      <td>-0.002590</td>\n",
       "      <td>-0.009109</td>\n",
       "      <td>-0.153354</td>\n",
       "      <td>-0.064210</td>\n",
       "      <td>0.234605</td>\n",
       "      <td>-0.008312</td>\n",
       "      <td>-0.054223</td>\n",
       "      <td>-0.063401</td>\n",
       "      <td>-0.123812</td>\n",
       "      <td>-0.119616</td>\n",
       "      <td>-0.123330</td>\n",
       "      <td>0.051739</td>\n",
       "      <td>0.036790</td>\n",
       "      <td>-0.130820</td>\n",
       "      <td>-0.038933</td>\n",
       "      <td>-0.021616</td>\n",
       "      <td>0.027752</td>\n",
       "      <td>-0.117120</td>\n",
       "      <td>-0.050470</td>\n",
       "      <td>0.024209</td>\n",
       "      <td>-0.068796</td>\n",
       "      <td>-0.317151</td>\n",
       "      <td>0.166114</td>\n",
       "      <td>0.108829</td>\n",
       "      <td>-0.152394</td>\n",
       "      <td>0.501626</td>\n",
       "      <td>0.010576</td>\n",
       "      <td>0.156876</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.081896</td>\n",
       "      <td>-0.062954</td>\n",
       "      <td>-0.042410</td>\n",
       "      <td>0.137425</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.029020</td>\n",
       "      <td>-0.073379</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.018606</td>\n",
       "      <td>0.162410</td>\n",
       "      <td>-0.029309</td>\n",
       "      <td>0.066301</td>\n",
       "      <td>0.068337</td>\n",
       "      <td>-0.294529</td>\n",
       "      <td>-0.035214</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>0.174051</td>\n",
       "      <td>-0.230817</td>\n",
       "      <td>-0.029245</td>\n",
       "      <td>0.154711</td>\n",
       "      <td>-0.399170</td>\n",
       "      <td>0.085708</td>\n",
       "      <td>0.181091</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.008989</td>\n",
       "      <td>0.049840</td>\n",
       "      <td>-0.027042</td>\n",
       "      <td>0.032060</td>\n",
       "      <td>-0.202390</td>\n",
       "      <td>-0.117266</td>\n",
       "      <td>0.029390</td>\n",
       "      <td>-0.048379</td>\n",
       "      <td>-0.130172</td>\n",
       "      <td>-0.087228</td>\n",
       "      <td>0.100429</td>\n",
       "      <td>0.094000</td>\n",
       "      <td>-0.115611</td>\n",
       "      <td>-0.137478</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>-0.010488</td>\n",
       "      <td>-0.030523</td>\n",
       "      <td>-0.129386</td>\n",
       "      <td>0.031646</td>\n",
       "      <td>0.043443</td>\n",
       "      <td>-0.123273</td>\n",
       "      <td>-0.049135</td>\n",
       "      <td>-0.127523</td>\n",
       "      <td>0.491136</td>\n",
       "      <td>-0.132008</td>\n",
       "      <td>-0.118403</td>\n",
       "      <td>0.048528</td>\n",
       "      <td>-0.117640</td>\n",
       "      <td>0.005887</td>\n",
       "      <td>0.095438</td>\n",
       "      <td>0.372867</td>\n",
       "      <td>0.247900</td>\n",
       "      <td>0.029227</td>\n",
       "      <td>-0.037019</td>\n",
       "      <td>-0.030807</td>\n",
       "      <td>-0.075480</td>\n",
       "      <td>0.203657</td>\n",
       "      <td>0.268339</td>\n",
       "      <td>0.156321</td>\n",
       "      <td>-0.084037</td>\n",
       "      <td>0.020744</td>\n",
       "      <td>0.065878</td>\n",
       "      <td>0.103615</td>\n",
       "      <td>-0.066243</td>\n",
       "      <td>0.155842</td>\n",
       "      <td>0.023177</td>\n",
       "      <td>-0.106569</td>\n",
       "      <td>-0.195076</td>\n",
       "      <td>0.013533</td>\n",
       "      <td>0.054395</td>\n",
       "      <td>-0.028370</td>\n",
       "      <td>-0.297454</td>\n",
       "      <td>-0.064792</td>\n",
       "      <td>0.098684</td>\n",
       "      <td>0.040687</td>\n",
       "      <td>-0.138230</td>\n",
       "      <td>-0.020008</td>\n",
       "      <td>0.236063</td>\n",
       "      <td>0.039886</td>\n",
       "      <td>-0.032195</td>\n",
       "      <td>-0.104934</td>\n",
       "      <td>-0.015654</td>\n",
       "      <td>-0.133936</td>\n",
       "      <td>-0.218210</td>\n",
       "      <td>-0.015633</td>\n",
       "      <td>-0.053356</td>\n",
       "      <td>-0.035344</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>-0.097216</td>\n",
       "      <td>0.229725</td>\n",
       "      <td>-0.107893</td>\n",
       "      <td>-0.135884</td>\n",
       "      <td>-0.157216</td>\n",
       "      <td>0.093305</td>\n",
       "      <td>0.133488</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>0.347299</td>\n",
       "      <td>0.025628</td>\n",
       "      <td>0.018132</td>\n",
       "      <td>0.031602</td>\n",
       "      <td>-0.076408</td>\n",
       "      <td>-0.071833</td>\n",
       "      <td>0.077541</td>\n",
       "      <td>0.035984</td>\n",
       "      <td>-0.007251</td>\n",
       "      <td>0.115826</td>\n",
       "      <td>0.049459</td>\n",
       "      <td>-0.008029</td>\n",
       "      <td>-0.109621</td>\n",
       "      <td>0.084565</td>\n",
       "      <td>-0.091571</td>\n",
       "      <td>-0.220581</td>\n",
       "      <td>0.044561</td>\n",
       "      <td>0.117253</td>\n",
       "      <td>-0.128492</td>\n",
       "      <td>0.071453</td>\n",
       "      <td>0.034587</td>\n",
       "      <td>-0.014483</td>\n",
       "      <td>-0.079585</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.057568</td>\n",
       "      <td>-0.063075</td>\n",
       "      <td>0.110885</td>\n",
       "      <td>0.056798</td>\n",
       "      <td>0.036001</td>\n",
       "      <td>-0.090397</td>\n",
       "      <td>-0.010322</td>\n",
       "      <td>-0.234610</td>\n",
       "      <td>-0.092104</td>\n",
       "      <td>0.035483</td>\n",
       "      <td>-0.065757</td>\n",
       "      <td>-0.208826</td>\n",
       "      <td>0.031659</td>\n",
       "      <td>-0.038692</td>\n",
       "      <td>0.018394</td>\n",
       "      <td>0.013719</td>\n",
       "      <td>-0.032323</td>\n",
       "      <td>-0.020136</td>\n",
       "      <td>0.215614</td>\n",
       "      <td>0.024351</td>\n",
       "      <td>0.010974</td>\n",
       "      <td>0.135121</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5137</th>\n",
       "      <td>nuclear%20reactor</td>\n",
       "      <td>#na#</td>\n",
       "      <td>0.016610</td>\n",
       "      <td>-0.087318</td>\n",
       "      <td>-0.066170</td>\n",
       "      <td>0.046714</td>\n",
       "      <td>-0.022622</td>\n",
       "      <td>0.085347</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>-0.132802</td>\n",
       "      <td>0.035191</td>\n",
       "      <td>0.125952</td>\n",
       "      <td>0.050570</td>\n",
       "      <td>0.006261</td>\n",
       "      <td>0.082256</td>\n",
       "      <td>0.027413</td>\n",
       "      <td>-0.073125</td>\n",
       "      <td>-0.115346</td>\n",
       "      <td>-0.046912</td>\n",
       "      <td>0.013246</td>\n",
       "      <td>0.051730</td>\n",
       "      <td>-0.043493</td>\n",
       "      <td>0.061502</td>\n",
       "      <td>0.067472</td>\n",
       "      <td>-0.040920</td>\n",
       "      <td>0.064470</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>-0.159592</td>\n",
       "      <td>0.159050</td>\n",
       "      <td>-0.056436</td>\n",
       "      <td>0.059384</td>\n",
       "      <td>0.082783</td>\n",
       "      <td>-0.015783</td>\n",
       "      <td>0.084382</td>\n",
       "      <td>-0.113787</td>\n",
       "      <td>-0.016802</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>0.029332</td>\n",
       "      <td>-0.032220</td>\n",
       "      <td>-0.058006</td>\n",
       "      <td>0.043799</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.084604</td>\n",
       "      <td>0.135619</td>\n",
       "      <td>0.110523</td>\n",
       "      <td>0.023432</td>\n",
       "      <td>0.198573</td>\n",
       "      <td>0.021516</td>\n",
       "      <td>0.016255</td>\n",
       "      <td>0.043911</td>\n",
       "      <td>0.168893</td>\n",
       "      <td>0.035183</td>\n",
       "      <td>-0.032286</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.147039</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.035110</td>\n",
       "      <td>0.011098</td>\n",
       "      <td>0.144686</td>\n",
       "      <td>0.047843</td>\n",
       "      <td>-0.020408</td>\n",
       "      <td>-0.045748</td>\n",
       "      <td>0.091322</td>\n",
       "      <td>-0.030787</td>\n",
       "      <td>0.103446</td>\n",
       "      <td>-0.057960</td>\n",
       "      <td>0.047302</td>\n",
       "      <td>0.073563</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>0.043556</td>\n",
       "      <td>0.132962</td>\n",
       "      <td>-0.031939</td>\n",
       "      <td>0.041951</td>\n",
       "      <td>-0.022574</td>\n",
       "      <td>-0.025266</td>\n",
       "      <td>-0.037565</td>\n",
       "      <td>-0.000543</td>\n",
       "      <td>0.078678</td>\n",
       "      <td>0.051030</td>\n",
       "      <td>-0.138265</td>\n",
       "      <td>-0.015298</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>0.040759</td>\n",
       "      <td>0.011018</td>\n",
       "      <td>-0.016050</td>\n",
       "      <td>-0.031403</td>\n",
       "      <td>0.156097</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.128194</td>\n",
       "      <td>0.099771</td>\n",
       "      <td>0.011218</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.011965</td>\n",
       "      <td>-0.052640</td>\n",
       "      <td>0.018473</td>\n",
       "      <td>-0.014286</td>\n",
       "      <td>-0.003375</td>\n",
       "      <td>-0.067664</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.010976</td>\n",
       "      <td>0.053012</td>\n",
       "      <td>0.094811</td>\n",
       "      <td>-0.002640</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.067735</td>\n",
       "      <td>-0.018512</td>\n",
       "      <td>-0.254771</td>\n",
       "      <td>0.071273</td>\n",
       "      <td>-0.051432</td>\n",
       "      <td>-0.071520</td>\n",
       "      <td>-0.152259</td>\n",
       "      <td>-0.101020</td>\n",
       "      <td>0.032957</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>0.086050</td>\n",
       "      <td>0.029118</td>\n",
       "      <td>-0.204728</td>\n",
       "      <td>-0.120686</td>\n",
       "      <td>0.062728</td>\n",
       "      <td>0.023854</td>\n",
       "      <td>0.077081</td>\n",
       "      <td>-0.200534</td>\n",
       "      <td>-0.020813</td>\n",
       "      <td>-0.054003</td>\n",
       "      <td>0.050946</td>\n",
       "      <td>0.078880</td>\n",
       "      <td>-0.043078</td>\n",
       "      <td>-0.010351</td>\n",
       "      <td>0.088473</td>\n",
       "      <td>-0.071948</td>\n",
       "      <td>-0.045024</td>\n",
       "      <td>0.150489</td>\n",
       "      <td>-0.070225</td>\n",
       "      <td>-0.126994</td>\n",
       "      <td>-0.184913</td>\n",
       "      <td>0.016316</td>\n",
       "      <td>-0.047108</td>\n",
       "      <td>-0.012073</td>\n",
       "      <td>-0.000892</td>\n",
       "      <td>0.015582</td>\n",
       "      <td>0.035427</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>-0.022749</td>\n",
       "      <td>-0.136526</td>\n",
       "      <td>0.114649</td>\n",
       "      <td>0.125293</td>\n",
       "      <td>-0.019128</td>\n",
       "      <td>-0.026219</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>-0.057383</td>\n",
       "      <td>-0.009448</td>\n",
       "      <td>0.028628</td>\n",
       "      <td>0.105630</td>\n",
       "      <td>0.029502</td>\n",
       "      <td>-0.009566</td>\n",
       "      <td>-0.108423</td>\n",
       "      <td>0.138693</td>\n",
       "      <td>0.067969</td>\n",
       "      <td>-0.029740</td>\n",
       "      <td>-0.024762</td>\n",
       "      <td>-0.087287</td>\n",
       "      <td>0.049560</td>\n",
       "      <td>-0.043415</td>\n",
       "      <td>-0.071539</td>\n",
       "      <td>0.013015</td>\n",
       "      <td>0.039787</td>\n",
       "      <td>0.053245</td>\n",
       "      <td>0.008588</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>0.027968</td>\n",
       "      <td>0.198176</td>\n",
       "      <td>-0.019399</td>\n",
       "      <td>-0.008457</td>\n",
       "      <td>0.060304</td>\n",
       "      <td>0.010698</td>\n",
       "      <td>-0.098484</td>\n",
       "      <td>0.037808</td>\n",
       "      <td>0.058621</td>\n",
       "      <td>-0.045760</td>\n",
       "      <td>-0.003858</td>\n",
       "      <td>-0.064603</td>\n",
       "      <td>0.073517</td>\n",
       "      <td>-0.007521</td>\n",
       "      <td>0.130630</td>\n",
       "      <td>0.019164</td>\n",
       "      <td>0.007748</td>\n",
       "      <td>0.020354</td>\n",
       "      <td>0.014207</td>\n",
       "      <td>0.063871</td>\n",
       "      <td>0.018775</td>\n",
       "      <td>0.073229</td>\n",
       "      <td>-0.198742</td>\n",
       "      <td>-0.015091</td>\n",
       "      <td>0.018608</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>-0.012129</td>\n",
       "      <td>0.051527</td>\n",
       "      <td>0.011950</td>\n",
       "      <td>0.026577</td>\n",
       "      <td>-0.042300</td>\n",
       "      <td>0.060232</td>\n",
       "      <td>-0.023604</td>\n",
       "      <td>-0.090631</td>\n",
       "      <td>-0.128217</td>\n",
       "      <td>0.041554</td>\n",
       "      <td>-0.016648</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>-0.020934</td>\n",
       "      <td>-0.023262</td>\n",
       "      <td>0.025320</td>\n",
       "      <td>0.144954</td>\n",
       "      <td>-0.140504</td>\n",
       "      <td>-0.130804</td>\n",
       "      <td>0.023644</td>\n",
       "      <td>-0.174123</td>\n",
       "      <td>0.057423</td>\n",
       "      <td>0.068828</td>\n",
       "      <td>0.042943</td>\n",
       "      <td>-0.146163</td>\n",
       "      <td>-0.089786</td>\n",
       "      <td>-0.015138</td>\n",
       "      <td>-0.046200</td>\n",
       "      <td>-0.012913</td>\n",
       "      <td>-0.078264</td>\n",
       "      <td>0.038056</td>\n",
       "      <td>-0.244333</td>\n",
       "      <td>0.027528</td>\n",
       "      <td>0.125404</td>\n",
       "      <td>-0.018875</td>\n",
       "      <td>0.041066</td>\n",
       "      <td>-0.090907</td>\n",
       "      <td>-0.089479</td>\n",
       "      <td>-0.091914</td>\n",
       "      <td>-0.106503</td>\n",
       "      <td>0.044365</td>\n",
       "      <td>-0.087022</td>\n",
       "      <td>-0.035011</td>\n",
       "      <td>0.054868</td>\n",
       "      <td>-0.046315</td>\n",
       "      <td>0.040110</td>\n",
       "      <td>-0.146919</td>\n",
       "      <td>0.044359</td>\n",
       "      <td>-0.066022</td>\n",
       "      <td>0.089831</td>\n",
       "      <td>-0.316845</td>\n",
       "      <td>-0.013252</td>\n",
       "      <td>-0.076032</td>\n",
       "      <td>-0.186906</td>\n",
       "      <td>0.158232</td>\n",
       "      <td>-0.087315</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>-0.004051</td>\n",
       "      <td>-0.027815</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>-0.005640</td>\n",
       "      <td>0.030167</td>\n",
       "      <td>0.028966</td>\n",
       "      <td>0.012788</td>\n",
       "      <td>-0.051551</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.010673</td>\n",
       "      <td>0.087780</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.132022</td>\n",
       "      <td>0.045886</td>\n",
       "      <td>-0.200817</td>\n",
       "      <td>-0.087444</td>\n",
       "      <td>-0.005142</td>\n",
       "      <td>0.163071</td>\n",
       "      <td>-0.105380</td>\n",
       "      <td>-0.031106</td>\n",
       "      <td>0.033246</td>\n",
       "      <td>-0.059405</td>\n",
       "      <td>0.034432</td>\n",
       "      <td>0.043843</td>\n",
       "      <td>0.054052</td>\n",
       "      <td>-0.059933</td>\n",
       "      <td>0.110711</td>\n",
       "      <td>-0.056312</td>\n",
       "      <td>0.059739</td>\n",
       "      <td>-0.255591</td>\n",
       "      <td>-0.069557</td>\n",
       "      <td>0.025855</td>\n",
       "      <td>-0.036724</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>-0.065299</td>\n",
       "      <td>-0.013836</td>\n",
       "      <td>0.136376</td>\n",
       "      <td>-0.091545</td>\n",
       "      <td>0.021060</td>\n",
       "      <td>0.063957</td>\n",
       "      <td>-0.075947</td>\n",
       "      <td>0.059096</td>\n",
       "      <td>-0.066626</td>\n",
       "      <td>-0.019741</td>\n",
       "      <td>-0.009368</td>\n",
       "      <td>-0.022790</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>-0.174362</td>\n",
       "      <td>0.281627</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>-0.068341</td>\n",
       "      <td>0.071571</td>\n",
       "      <td>0.014303</td>\n",
       "      <td>0.021512</td>\n",
       "      <td>-0.036467</td>\n",
       "      <td>0.134576</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.059663</td>\n",
       "      <td>0.126461</td>\n",
       "      <td>-0.073000</td>\n",
       "      <td>0.015973</td>\n",
       "      <td>0.101863</td>\n",
       "      <td>-0.120853</td>\n",
       "      <td>-0.044747</td>\n",
       "      <td>0.045176</td>\n",
       "      <td>-0.025129</td>\n",
       "      <td>-0.018980</td>\n",
       "      <td>0.112251</td>\n",
       "      <td>-0.110540</td>\n",
       "      <td>0.300593</td>\n",
       "      <td>0.030803</td>\n",
       "      <td>0.041492</td>\n",
       "      <td>-0.156844</td>\n",
       "      <td>-0.006527</td>\n",
       "      <td>-0.049807</td>\n",
       "      <td>-0.088625</td>\n",
       "      <td>0.064630</td>\n",
       "      <td>-0.048084</td>\n",
       "      <td>0.015949</td>\n",
       "      <td>-0.061306</td>\n",
       "      <td>-0.044027</td>\n",
       "      <td>0.095928</td>\n",
       "      <td>0.130412</td>\n",
       "      <td>0.100665</td>\n",
       "      <td>-0.034348</td>\n",
       "      <td>0.024057</td>\n",
       "      <td>0.061430</td>\n",
       "      <td>-0.118531</td>\n",
       "      <td>-0.234168</td>\n",
       "      <td>-0.023533</td>\n",
       "      <td>0.019178</td>\n",
       "      <td>-0.050930</td>\n",
       "      <td>0.085276</td>\n",
       "      <td>-0.061181</td>\n",
       "      <td>-0.007003</td>\n",
       "      <td>-0.075845</td>\n",
       "      <td>-0.110490</td>\n",
       "      <td>0.027183</td>\n",
       "      <td>0.011176</td>\n",
       "      <td>0.026414</td>\n",
       "      <td>-0.047913</td>\n",
       "      <td>0.113115</td>\n",
       "      <td>0.041505</td>\n",
       "      <td>0.106121</td>\n",
       "      <td>-0.017673</td>\n",
       "      <td>-0.024482</td>\n",
       "      <td>-0.178906</td>\n",
       "      <td>-0.046138</td>\n",
       "      <td>-0.037265</td>\n",
       "      <td>0.011731</td>\n",
       "      <td>0.162945</td>\n",
       "      <td>0.006201</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.059489</td>\n",
       "      <td>-0.006977</td>\n",
       "      <td>0.005811</td>\n",
       "      <td>-0.165146</td>\n",
       "      <td>0.073464</td>\n",
       "      <td>-0.045554</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.115462</td>\n",
       "      <td>0.106223</td>\n",
       "      <td>-0.099896</td>\n",
       "      <td>-0.165819</td>\n",
       "      <td>-0.042277</td>\n",
       "      <td>-0.039802</td>\n",
       "      <td>0.023112</td>\n",
       "      <td>0.059918</td>\n",
       "      <td>0.114238</td>\n",
       "      <td>0.028082</td>\n",
       "      <td>-0.038542</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.280589</td>\n",
       "      <td>-0.103788</td>\n",
       "      <td>0.081325</td>\n",
       "      <td>0.027540</td>\n",
       "      <td>-0.180665</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.021964</td>\n",
       "      <td>0.058320</td>\n",
       "      <td>0.039684</td>\n",
       "      <td>-0.061685</td>\n",
       "      <td>-0.092563</td>\n",
       "      <td>0.158403</td>\n",
       "      <td>-0.070127</td>\n",
       "      <td>-0.007711</td>\n",
       "      <td>-0.078097</td>\n",
       "      <td>0.038862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>military</td>\n",
       "      <td>#na#</td>\n",
       "      <td>0.071649</td>\n",
       "      <td>0.048773</td>\n",
       "      <td>0.149381</td>\n",
       "      <td>0.072745</td>\n",
       "      <td>-0.153474</td>\n",
       "      <td>0.219653</td>\n",
       "      <td>0.105631</td>\n",
       "      <td>-0.101802</td>\n",
       "      <td>-0.074127</td>\n",
       "      <td>0.070232</td>\n",
       "      <td>0.052787</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.058096</td>\n",
       "      <td>0.018044</td>\n",
       "      <td>-0.015089</td>\n",
       "      <td>0.200928</td>\n",
       "      <td>-0.060976</td>\n",
       "      <td>-0.116336</td>\n",
       "      <td>0.144278</td>\n",
       "      <td>-0.044305</td>\n",
       "      <td>-0.009169</td>\n",
       "      <td>-0.071641</td>\n",
       "      <td>-0.027731</td>\n",
       "      <td>0.150415</td>\n",
       "      <td>0.038074</td>\n",
       "      <td>0.150955</td>\n",
       "      <td>-0.016712</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>0.060747</td>\n",
       "      <td>0.117532</td>\n",
       "      <td>-0.038306</td>\n",
       "      <td>-0.117428</td>\n",
       "      <td>-0.153199</td>\n",
       "      <td>0.127205</td>\n",
       "      <td>-0.033736</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>0.024128</td>\n",
       "      <td>0.122355</td>\n",
       "      <td>0.158344</td>\n",
       "      <td>-0.148204</td>\n",
       "      <td>-0.131825</td>\n",
       "      <td>-0.019603</td>\n",
       "      <td>-0.038929</td>\n",
       "      <td>-0.137530</td>\n",
       "      <td>-0.018131</td>\n",
       "      <td>0.050824</td>\n",
       "      <td>-0.126347</td>\n",
       "      <td>0.113782</td>\n",
       "      <td>-0.046500</td>\n",
       "      <td>-0.078884</td>\n",
       "      <td>-0.169211</td>\n",
       "      <td>0.189170</td>\n",
       "      <td>-0.047784</td>\n",
       "      <td>-0.043524</td>\n",
       "      <td>0.090539</td>\n",
       "      <td>0.067378</td>\n",
       "      <td>0.143502</td>\n",
       "      <td>-0.040625</td>\n",
       "      <td>-0.106228</td>\n",
       "      <td>0.040286</td>\n",
       "      <td>0.009914</td>\n",
       "      <td>-0.080668</td>\n",
       "      <td>-0.101415</td>\n",
       "      <td>-0.123414</td>\n",
       "      <td>-0.129508</td>\n",
       "      <td>0.105385</td>\n",
       "      <td>0.038816</td>\n",
       "      <td>0.013209</td>\n",
       "      <td>0.182379</td>\n",
       "      <td>-0.239337</td>\n",
       "      <td>-0.062278</td>\n",
       "      <td>-0.096283</td>\n",
       "      <td>-0.220886</td>\n",
       "      <td>-0.003395</td>\n",
       "      <td>-0.057327</td>\n",
       "      <td>-0.056770</td>\n",
       "      <td>0.024094</td>\n",
       "      <td>-0.082056</td>\n",
       "      <td>-0.015400</td>\n",
       "      <td>-0.156324</td>\n",
       "      <td>-0.072187</td>\n",
       "      <td>-0.015479</td>\n",
       "      <td>-0.045358</td>\n",
       "      <td>-0.067658</td>\n",
       "      <td>0.082780</td>\n",
       "      <td>0.013263</td>\n",
       "      <td>0.116848</td>\n",
       "      <td>0.106683</td>\n",
       "      <td>-0.125648</td>\n",
       "      <td>0.058811</td>\n",
       "      <td>-0.060642</td>\n",
       "      <td>-0.067587</td>\n",
       "      <td>-0.060255</td>\n",
       "      <td>-0.017406</td>\n",
       "      <td>-0.074917</td>\n",
       "      <td>0.155131</td>\n",
       "      <td>0.037369</td>\n",
       "      <td>0.039981</td>\n",
       "      <td>0.037007</td>\n",
       "      <td>0.060395</td>\n",
       "      <td>0.048224</td>\n",
       "      <td>0.159233</td>\n",
       "      <td>-0.053128</td>\n",
       "      <td>-0.143395</td>\n",
       "      <td>-0.069052</td>\n",
       "      <td>0.032208</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>-0.018971</td>\n",
       "      <td>-0.179244</td>\n",
       "      <td>-0.138840</td>\n",
       "      <td>0.293152</td>\n",
       "      <td>0.015421</td>\n",
       "      <td>-0.049685</td>\n",
       "      <td>-0.009213</td>\n",
       "      <td>-0.024273</td>\n",
       "      <td>-0.016616</td>\n",
       "      <td>-0.128610</td>\n",
       "      <td>0.221469</td>\n",
       "      <td>0.115637</td>\n",
       "      <td>0.033495</td>\n",
       "      <td>-0.149941</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>-0.080720</td>\n",
       "      <td>0.069477</td>\n",
       "      <td>0.126118</td>\n",
       "      <td>-0.005011</td>\n",
       "      <td>-0.015579</td>\n",
       "      <td>0.018563</td>\n",
       "      <td>-0.021850</td>\n",
       "      <td>0.038732</td>\n",
       "      <td>0.263692</td>\n",
       "      <td>-0.098678</td>\n",
       "      <td>-0.096085</td>\n",
       "      <td>-0.098767</td>\n",
       "      <td>-0.090899</td>\n",
       "      <td>0.080797</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>-0.015690</td>\n",
       "      <td>-0.152974</td>\n",
       "      <td>0.187532</td>\n",
       "      <td>0.084264</td>\n",
       "      <td>0.134923</td>\n",
       "      <td>-0.015639</td>\n",
       "      <td>0.116470</td>\n",
       "      <td>0.051222</td>\n",
       "      <td>0.235874</td>\n",
       "      <td>-0.131449</td>\n",
       "      <td>-0.019452</td>\n",
       "      <td>-0.178278</td>\n",
       "      <td>0.043799</td>\n",
       "      <td>-0.039440</td>\n",
       "      <td>0.066644</td>\n",
       "      <td>0.042723</td>\n",
       "      <td>0.208909</td>\n",
       "      <td>0.074776</td>\n",
       "      <td>-0.093770</td>\n",
       "      <td>0.240034</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>-0.147511</td>\n",
       "      <td>0.033850</td>\n",
       "      <td>-0.188406</td>\n",
       "      <td>0.063864</td>\n",
       "      <td>0.201634</td>\n",
       "      <td>-0.133269</td>\n",
       "      <td>-0.058252</td>\n",
       "      <td>-0.008406</td>\n",
       "      <td>0.029349</td>\n",
       "      <td>-0.063271</td>\n",
       "      <td>-0.082523</td>\n",
       "      <td>-0.021542</td>\n",
       "      <td>0.120678</td>\n",
       "      <td>0.313176</td>\n",
       "      <td>-0.090264</td>\n",
       "      <td>0.190193</td>\n",
       "      <td>0.032141</td>\n",
       "      <td>0.062125</td>\n",
       "      <td>0.054894</td>\n",
       "      <td>0.069348</td>\n",
       "      <td>-0.023696</td>\n",
       "      <td>-0.004686</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.081296</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>-0.006416</td>\n",
       "      <td>-0.027549</td>\n",
       "      <td>0.143713</td>\n",
       "      <td>0.016495</td>\n",
       "      <td>0.116842</td>\n",
       "      <td>0.116408</td>\n",
       "      <td>0.243155</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>-0.092486</td>\n",
       "      <td>-0.059149</td>\n",
       "      <td>-0.019876</td>\n",
       "      <td>0.030811</td>\n",
       "      <td>-0.064695</td>\n",
       "      <td>-0.034969</td>\n",
       "      <td>0.083199</td>\n",
       "      <td>0.037930</td>\n",
       "      <td>-0.052801</td>\n",
       "      <td>0.171662</td>\n",
       "      <td>0.141220</td>\n",
       "      <td>-0.070420</td>\n",
       "      <td>-0.125043</td>\n",
       "      <td>-0.152895</td>\n",
       "      <td>0.045767</td>\n",
       "      <td>-0.073035</td>\n",
       "      <td>0.024290</td>\n",
       "      <td>0.023624</td>\n",
       "      <td>0.156522</td>\n",
       "      <td>-0.096698</td>\n",
       "      <td>-0.316811</td>\n",
       "      <td>0.012027</td>\n",
       "      <td>-0.325415</td>\n",
       "      <td>-0.055999</td>\n",
       "      <td>-0.248373</td>\n",
       "      <td>0.264290</td>\n",
       "      <td>0.039967</td>\n",
       "      <td>0.122004</td>\n",
       "      <td>-0.265568</td>\n",
       "      <td>-0.104602</td>\n",
       "      <td>-0.123445</td>\n",
       "      <td>0.141885</td>\n",
       "      <td>-0.021108</td>\n",
       "      <td>-0.194948</td>\n",
       "      <td>0.032504</td>\n",
       "      <td>-0.324625</td>\n",
       "      <td>-0.020252</td>\n",
       "      <td>0.058884</td>\n",
       "      <td>-0.072262</td>\n",
       "      <td>-0.073562</td>\n",
       "      <td>-0.045526</td>\n",
       "      <td>-0.261323</td>\n",
       "      <td>-0.028839</td>\n",
       "      <td>0.024717</td>\n",
       "      <td>0.030412</td>\n",
       "      <td>-0.065840</td>\n",
       "      <td>0.161189</td>\n",
       "      <td>-0.036266</td>\n",
       "      <td>-0.101655</td>\n",
       "      <td>-0.051219</td>\n",
       "      <td>-0.078372</td>\n",
       "      <td>0.072737</td>\n",
       "      <td>-0.126508</td>\n",
       "      <td>0.085295</td>\n",
       "      <td>-0.352792</td>\n",
       "      <td>0.182850</td>\n",
       "      <td>-0.044731</td>\n",
       "      <td>-0.332730</td>\n",
       "      <td>0.210181</td>\n",
       "      <td>0.046391</td>\n",
       "      <td>0.132755</td>\n",
       "      <td>-0.067352</td>\n",
       "      <td>-0.079180</td>\n",
       "      <td>-0.017180</td>\n",
       "      <td>0.018496</td>\n",
       "      <td>0.133457</td>\n",
       "      <td>0.014919</td>\n",
       "      <td>-0.041752</td>\n",
       "      <td>-0.066209</td>\n",
       "      <td>-0.080230</td>\n",
       "      <td>0.097264</td>\n",
       "      <td>0.011689</td>\n",
       "      <td>-0.046302</td>\n",
       "      <td>0.094545</td>\n",
       "      <td>0.059121</td>\n",
       "      <td>-0.100507</td>\n",
       "      <td>-0.045253</td>\n",
       "      <td>-0.021199</td>\n",
       "      <td>0.325278</td>\n",
       "      <td>-0.051644</td>\n",
       "      <td>-0.180743</td>\n",
       "      <td>0.028775</td>\n",
       "      <td>-0.050564</td>\n",
       "      <td>0.102858</td>\n",
       "      <td>0.083130</td>\n",
       "      <td>-0.002266</td>\n",
       "      <td>-0.067914</td>\n",
       "      <td>0.085178</td>\n",
       "      <td>-0.120805</td>\n",
       "      <td>-0.170044</td>\n",
       "      <td>-0.238773</td>\n",
       "      <td>0.133232</td>\n",
       "      <td>0.137942</td>\n",
       "      <td>-0.041660</td>\n",
       "      <td>0.026158</td>\n",
       "      <td>-0.044009</td>\n",
       "      <td>-0.107919</td>\n",
       "      <td>-0.061634</td>\n",
       "      <td>0.020849</td>\n",
       "      <td>0.115876</td>\n",
       "      <td>-0.060235</td>\n",
       "      <td>-0.068679</td>\n",
       "      <td>0.015884</td>\n",
       "      <td>0.232707</td>\n",
       "      <td>-0.042937</td>\n",
       "      <td>-0.076953</td>\n",
       "      <td>0.116262</td>\n",
       "      <td>-0.013012</td>\n",
       "      <td>-0.103001</td>\n",
       "      <td>0.139681</td>\n",
       "      <td>-0.076045</td>\n",
       "      <td>-0.124863</td>\n",
       "      <td>0.066305</td>\n",
       "      <td>-0.023933</td>\n",
       "      <td>-0.072784</td>\n",
       "      <td>-0.220637</td>\n",
       "      <td>0.247534</td>\n",
       "      <td>0.020654</td>\n",
       "      <td>0.137320</td>\n",
       "      <td>0.271929</td>\n",
       "      <td>0.060711</td>\n",
       "      <td>0.014204</td>\n",
       "      <td>-0.053483</td>\n",
       "      <td>-0.122285</td>\n",
       "      <td>0.154373</td>\n",
       "      <td>0.027396</td>\n",
       "      <td>-0.192300</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>-0.129906</td>\n",
       "      <td>-0.004135</td>\n",
       "      <td>0.274247</td>\n",
       "      <td>-0.203898</td>\n",
       "      <td>0.083854</td>\n",
       "      <td>-0.004666</td>\n",
       "      <td>-0.078879</td>\n",
       "      <td>-0.128639</td>\n",
       "      <td>0.051129</td>\n",
       "      <td>0.091764</td>\n",
       "      <td>0.165263</td>\n",
       "      <td>-0.002654</td>\n",
       "      <td>0.113504</td>\n",
       "      <td>-0.001238</td>\n",
       "      <td>0.037717</td>\n",
       "      <td>0.046421</td>\n",
       "      <td>-0.090438</td>\n",
       "      <td>0.167070</td>\n",
       "      <td>-0.134985</td>\n",
       "      <td>0.045768</td>\n",
       "      <td>-0.126235</td>\n",
       "      <td>-0.277085</td>\n",
       "      <td>0.090812</td>\n",
       "      <td>0.089359</td>\n",
       "      <td>-0.228049</td>\n",
       "      <td>-0.021931</td>\n",
       "      <td>-0.188144</td>\n",
       "      <td>-0.041706</td>\n",
       "      <td>-0.116698</td>\n",
       "      <td>-0.142936</td>\n",
       "      <td>-0.159401</td>\n",
       "      <td>-0.258352</td>\n",
       "      <td>0.042741</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>0.252122</td>\n",
       "      <td>0.162179</td>\n",
       "      <td>0.012414</td>\n",
       "      <td>0.100541</td>\n",
       "      <td>-0.006548</td>\n",
       "      <td>0.067839</td>\n",
       "      <td>-0.053630</td>\n",
       "      <td>0.019113</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>-0.026172</td>\n",
       "      <td>-0.021310</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>-0.061059</td>\n",
       "      <td>-0.167010</td>\n",
       "      <td>-0.071662</td>\n",
       "      <td>-0.152882</td>\n",
       "      <td>-0.023657</td>\n",
       "      <td>-0.099236</td>\n",
       "      <td>-0.180189</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>0.234336</td>\n",
       "      <td>-0.098556</td>\n",
       "      <td>-0.102210</td>\n",
       "      <td>-0.136078</td>\n",
       "      <td>-0.009781</td>\n",
       "      <td>-0.027197</td>\n",
       "      <td>0.023554</td>\n",
       "      <td>0.004749</td>\n",
       "      <td>-0.008863</td>\n",
       "      <td>-0.101916</td>\n",
       "      <td>-0.102561</td>\n",
       "      <td>0.092921</td>\n",
       "      <td>-0.103527</td>\n",
       "      <td>-0.037655</td>\n",
       "      <td>0.054191</td>\n",
       "      <td>0.119099</td>\n",
       "      <td>0.116025</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>0.067385</td>\n",
       "      <td>0.187906</td>\n",
       "      <td>0.062123</td>\n",
       "      <td>0.081536</td>\n",
       "      <td>-0.086275</td>\n",
       "      <td>-0.025187</td>\n",
       "      <td>0.068961</td>\n",
       "      <td>0.291553</td>\n",
       "      <td>-0.156042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_390</th>\n",
       "      <th>emb_391</th>\n",
       "      <th>emb_392</th>\n",
       "      <th>emb_393</th>\n",
       "      <th>emb_394</th>\n",
       "      <th>emb_395</th>\n",
       "      <th>emb_396</th>\n",
       "      <th>emb_397</th>\n",
       "      <th>emb_398</th>\n",
       "      <th>emb_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6192</th>\n",
       "      <td>8839</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>sleeping with sirens vai vir pra sp</td>\n",
       "      <td>0</td>\n",
       "      <td>0.490382</td>\n",
       "      <td>0.422509</td>\n",
       "      <td>-0.098451</td>\n",
       "      <td>-0.684747</td>\n",
       "      <td>0.717871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807239</td>\n",
       "      <td>1.018406</td>\n",
       "      <td>-1.775813</td>\n",
       "      <td>-1.177999</td>\n",
       "      <td>-0.019847</td>\n",
       "      <td>-0.090329</td>\n",
       "      <td>0.166692</td>\n",
       "      <td>-0.133344</td>\n",
       "      <td>-0.035306</td>\n",
       "      <td>0.188026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>7768</td>\n",
       "      <td>158</td>\n",
       "      <td>2644</td>\n",
       "      <td>$1 million bail for man accused of #shooting at Fife #police - Aug 5 @ 8:16 PM ET http://t.co/hu5CXqnoBf</td>\n",
       "      <td>1</td>\n",
       "      <td>1.658683</td>\n",
       "      <td>-0.434940</td>\n",
       "      <td>1.315403</td>\n",
       "      <td>-0.384109</td>\n",
       "      <td>0.109086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067902</td>\n",
       "      <td>-0.546903</td>\n",
       "      <td>0.672220</td>\n",
       "      <td>0.413047</td>\n",
       "      <td>0.709461</td>\n",
       "      <td>-0.439854</td>\n",
       "      <td>0.363564</td>\n",
       "      <td>1.255388</td>\n",
       "      <td>0.843311</td>\n",
       "      <td>-1.184751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>6937</td>\n",
       "      <td>141</td>\n",
       "      <td>1133</td>\n",
       "      <td>@BenignoVito @LibertyBell1000 HILLARYMASS MURDERER.</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.391727</td>\n",
       "      <td>-2.019549</td>\n",
       "      <td>2.752736</td>\n",
       "      <td>-0.506640</td>\n",
       "      <td>-1.274974</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.230838</td>\n",
       "      <td>-0.670749</td>\n",
       "      <td>1.182071</td>\n",
       "      <td>2.056793</td>\n",
       "      <td>1.852203</td>\n",
       "      <td>-0.745635</td>\n",
       "      <td>-2.137211</td>\n",
       "      <td>0.233751</td>\n",
       "      <td>2.048053</td>\n",
       "      <td>0.462029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 405 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  keyword  location  \\\n",
       "6192  8839      180         0   \n",
       "5447  7768      158      2644   \n",
       "4873  6937      141      1133   \n",
       "\n",
       "                                                                                                          text  \\\n",
       "6192                                                                       sleeping with sirens vai vir pra sp   \n",
       "5447  $1 million bail for man accused of #shooting at Fife #police - Aug 5 @ 8:16 PM ET http://t.co/hu5CXqnoBf   \n",
       "4873                                                       @BenignoVito @LibertyBell1000 HILLARYMASS MURDERER.   \n",
       "\n",
       "      target     emb_0     emb_1     emb_2     emb_3     emb_4  ...   emb_390  \\\n",
       "6192       0  0.490382  0.422509 -0.098451 -0.684747  0.717871  ...  0.807239   \n",
       "5447       1  1.658683 -0.434940  1.315403 -0.384109  0.109086  ...  0.067902   \n",
       "4873       0 -0.391727 -2.019549  2.752736 -0.506640 -1.274974  ... -1.230838   \n",
       "\n",
       "       emb_391   emb_392   emb_393   emb_394   emb_395   emb_396   emb_397  \\\n",
       "6192  1.018406 -1.775813 -1.177999 -0.019847 -0.090329  0.166692 -0.133344   \n",
       "5447 -0.546903  0.672220  0.413047  0.709461 -0.439854  0.363564  1.255388   \n",
       "4873 -0.670749  1.182071  2.056793  1.852203 -0.745635 -2.137211  0.233751   \n",
       "\n",
       "       emb_398   emb_399  \n",
       "6192 -0.035306  0.188026  \n",
       "5447  0.843311 -1.184751  \n",
       "4873  2.048053  0.462029  \n",
       "\n",
       "[3 rows x 405 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.items.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#na#', 'ablaze', 'accident', 'aftershock', 'airplane%20accident', 'ambulance', 'annihilated', 'annihilation', 'apocalypse', 'armageddon', 'army', 'arson', 'arsonist', 'attack', 'attacked', 'avalanche', 'battle', 'bioterror', 'bioterrorism', 'blaze', 'blazing', 'bleeding', 'blew%20up', 'blight', 'blizzard', 'blood', 'bloody', 'blown%20up', 'body%20bag', 'body%20bagging', 'body%20bags', 'bomb', 'bombed', 'bombing', 'bridge%20collapse', 'buildings%20burning', 'buildings%20on%20fire', 'burned', 'burning', 'burning%20buildings', 'bush%20fires', 'casualties', 'casualty', 'catastrophe', 'catastrophic', 'chemical%20emergency', 'cliff%20fall', 'collapse', 'collapsed', 'collide', 'collided', 'collision', 'crash', 'crashed', 'crush', 'crushed', 'curfew', 'cyclone', 'damage', 'danger', 'dead', 'death', 'deaths', 'debris', 'deluge', 'deluged', 'demolish', 'demolished', 'demolition', 'derail', 'derailed', 'derailment', 'desolate', 'desolation', 'destroy', 'destroyed', 'destruction', 'detonate', 'detonation', 'devastated', 'devastation', 'disaster', 'displaced', 'drought', 'drown', 'drowned', 'drowning', 'dust%20storm', 'earthquake', 'electrocute', 'electrocuted', 'emergency', 'emergency%20plan', 'emergency%20services', 'engulfed', 'epicentre', 'evacuate', 'evacuated', 'evacuation', 'explode', 'exploded', 'explosion', 'eyewitness', 'famine', 'fatal', 'fatalities', 'fatality', 'fear', 'fire', 'fire%20truck', 'first%20responders', 'flames', 'flattened', 'flood', 'flooding', 'floods', 'forest%20fire', 'forest%20fires', 'hail', 'hailstorm', 'harm', 'hazard', 'hazardous', 'heat%20wave', 'hellfire', 'hijack', 'hijacker', 'hijacking', 'hostage', 'hostages', 'hurricane', 'injured', 'injuries', 'injury', 'inundated', 'inundation', 'landslide', 'lava', 'lightning', 'loud%20bang', 'mass%20murder', 'mass%20murderer', 'massacre', 'mayhem', 'meltdown', 'military', 'mudslide', 'natural%20disaster', 'nuclear%20disaster', 'nuclear%20reactor', 'obliterate', 'obliterated', 'obliteration', 'oil%20spill', 'outbreak', 'pandemonium', 'panic', 'panicking', 'police', 'quarantine', 'quarantined', 'radiation%20emergency', 'rainstorm', 'razed', 'refugees', 'rescue', 'rescued', 'rescuers', 'riot', 'rioting', 'rubble', 'ruin', 'sandstorm', 'screamed', 'screaming', 'screams', 'seismic', 'sinkhole', 'sinking', 'siren', 'sirens', 'smoke', 'snowstorm', 'storm', 'stretcher', 'structural%20failure', 'suicide%20bomb', 'suicide%20bomber', 'suicide%20bombing', 'sunk', 'survive', 'survived', 'survivors', 'terrorism', 'terrorist', 'threat', 'thunder', 'thunderstorm', 'tornado', 'tragedy', 'trapped', 'trauma', 'traumatised', 'trouble', 'tsunami', 'twister', 'typhoon', 'upheaval', 'violent%20storm', 'volcano', 'war%20zone', 'weapon', 'weapons', 'whirlwind', 'wild%20fires', 'wildfire', 'windstorm', 'wounded', 'wounds', 'wreck', 'wreckage', 'wrecked']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.classes['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#na#', '  ', '  Glasgow ', '  Melbourne, Australia', '  News', '  å_ ', \" 45å¡ 5'12.53N   14å¡ 7'24.93E\", ' 616 \\x89Û¢ Kentwood , MI ', ' ? ??????? ? ( ?? å¡ ? ? ? å¡)', ' ?currently writing a book?', ' Alberta', ' Alex/Mika/Leo|18|he/she/they', ' BC, US, Asia or Europe.', ' Baku & Erzurum ', ' Blood Indian Reserve', ' Bouvet Island', ' Eugene, Oregon', ' Indiana', ' Jariana Town', ' Little Rock, AR', ' Miami Beach', ' Nevada Carson City,Freeman St', ' Neverland ', ' New Delhi ', ' New England', ' Nxgerxa', ' Quantico Marine Base, VA.', ' Queensland, Australia', ' Road to the Billionaires Club', ' Somewhere.', ' The World', ' Tropical SE FLorida', ' snapchat // fvck_casper ', ' |IG: imaginedragoner', '#1 Vacation Destination,HAWAII', '#937??#734', '#????? Libya#', '#BlackLivesMatter', '#BossNation!', '#Bummerville otw', '#EngleWood CHICAGO ', '#FLIGHTCITY UK  ', '#ForeverWithBAP 8 ', '#GDJB #ASOT', '#Gladiator \\x89Û¢860\\x89Û¢757\\x89Û¢', '#HAMont', '#HarleyChick#PJNT#RunBenRun', '#KaumElite;#F?VOR;#SMOFC', '#LemonGang ', '#MadeInNorthumberland', '#MayGodHelpUS', '#NewcastleuponTyne #UK', '#ODU', '#PhanTrash', '#RedSoxNation', '#SOUTHAMPTON ENGLAND', '#SandraBland', '#UNITE THE BLUE  ', '#WashingtonState #Seattle', \"#WhereverI'mAt\", '#expelcl*y', '#freegucci', '#goingdownthetoilet Illinois', '#iminchina', '#keepthefaith J&J', '#otrakansascity', '#partsunknown', '$ad $hawty', \"'Merica\", \"'SAN ANTONIOOOOO'\", \"'soooota\", '( ?å¡ ?? ?å¡), ', '(RP)', '(Spain)', '(a) property of the universe', '(he/him)', '-6.152261,106.775995', '-?s?s?j??s-', '... -.- -.--', '//??//', '//RP\\\\ ot @Mort3mer\\\\\\\\', '05/04/2014 18:23 ?', '1/10 Taron squad', '1/3 of the blam squad ', '10 Steps Ahead.  Cloud 9', '10-Jul', '107-18 79TH STREET', '11/4/14', '11202', '11th dimension, los angeles', '1313 W.Patrick St, Frederick', '14/cis/istj ', '140920-21 & 150718-19 BEIJING', '1648 Queen St. West, Toronto.', '17-Feb', '17th Dimension', '18 | 509 ', '18 \\x89Û¢ CC', '19.600858, -99.047821', '1D | 5SOS | AG', '2 high 2 come down ', '2,360 miles away', '2005 |-/', '204, 555 11 Ave. S.W.', '21 | PNW', '21, Porto', '21.462446,-158.022017', '23 countries and counting!', '253', '261 5th Avenue New York, NY ', '2B Hindhede Rd, Singapore', '3.28.15|7.20.15|7.25.15', '3000 miles from everyone', '302', '302???? 815', '304', '316', '36 & 38', '3?3?7?SLOPelousas??2?2?5?', '3???2???????', '3rd Eye Chakra', \"401 livin'\", '412 NW 5th Ave. Portland OR', '434', '46.950109,7.439469', '48.870833,2.399227', '5-Feb', '5/5 access / rt link please x', '50% Queanbeyan - 50% Sydney', '505 W. Maple, Suite 100', '518', '518 åá NY', '52.479722, 62.184971', '570 Vanderbilt; Brooklyn, NY', '6', '60th St (SS)', '617-BTOWN-BEATDOWN', '627', '65', '650/559', '709?', '73101', '772 Temperance Permenence', '801 SL,UT', '828/704(Soufside)/while looking goofy in NJ', '828??864??803', '876 Jamrock.', '9/1/13', '92', '94123', '956', '?', '? ', '? Jet Life ?', '? Philly Baby ?', '? icon by @Hashiren_3 ?', '? miranda ? 521 mi', '?205?478?', '??', '?? ?+254? ? \\\\??å¡_??å¡_???å¡_?/??', '?? ??', '?? Cloud Mafia ??', '?? Made in the Philippines ??', '??+ ... ??+', '??9?', '???', '???  Dreamz', '??? ?? ??????? ', '??? ??? ????? ??? ???.', '??? ???? ??????', '??? ???? ?f glory. ?', '??? ?????????????', '????', '???? ???????', '?????', '????? ???? ????', '??????', '?????? ??? ?????? ????????', '?????? ???? ??????', '?????? in Yokohama Japan', '??????? ??????? ????????', \"???????, ??'??????\", '???????, Texas', '???????? ?????????.', '???????????', '??????????? ???????????..? ', '????????????', '?????????????, Thailand ', '??????????????', '??????????????????', '????s ?? ????Ìø????Ì¡a', '??t?a', '?@symbolicjensen?', '?Gangsta OC / MV RP; 18+.?', '?^åá??åá?^?? ??', '?s????ss? a?????', '?semekeepschanging@soyeh?', '@ ForSL/RP', '@UntmdOutdoors #T.O.R.K ', '@cockerelshoes', '@notoriousD12', '@potteratthedisc', \"@protectingtitan's side.\", 'A Hoop Somewhere', 'A little house in the outback.', 'A small federal enclave', 'A sofa', 'A.A.S my Aztec Princess', 'ACCRA GHANA', 'AEP', 'AFRICA', 'AKRON OHIO USA', 'ALWAYS DYING NEVER RESTING', 'ANDY 4 LEADER X', 'ANYWEHERE !!', 'ARBAILO', 'ARGENTINA', 'ARIZONA', 'ATL ? SEA ', 'ATL ??', 'ATL, GA', 'ATL??AL??', 'ATLANTA , GEORGIA ', 'ATX', 'AUS', 'AUSTRALIA-SOUTHAFRICA-CAMBODIA', 'AZ', 'Aarhus, Central Jutland', 'Ab, Canada', 'Aberdeenshire', 'Above the snake line - #YoNews', 'Absecon, NJ', 'Abuja', 'Abuja, Nigeria', 'Abuja,Nigeria', 'Accra,Ghana', 'Across the Atlantic', 'Ad Majorem Dei Glorium', 'Adelaide', 'Adelaide, Australia', 'Adelaide, South Australia', 'Adventist - Lesson Sabbath', 'Adventuring in Narnia', 'Afghanistan', 'Afghanistan, USA', 'Africa', 'Aix-en-Provence, France', 'Aix-en-Provence/Utrecht', 'Akure city in ondo state ', 'Alabama', 'Alabama, USA', 'Alameda and Pleasanton, CA', 'Alameda, CA', 'Alaska', 'Alaska, USA', 'Albany/NY', 'Alberta ', 'Alberta Pack', 'Alberta | Sask. | Montana', 'Alberta, VA', 'Albuquerque', 'Albuquerque New Mexico', 'Alexandria, Egypt.', 'Alexandria, VA', 'Alexandria, VA, USA', 'Alger-New York-San Francisco', 'Alicante, Spain', 'Alicante, Valencia', 'All Around the World', 'All around the world', 'All around the world baby', 'All around the world!', 'Alliston Ontario', 'Alphen aan den Rijn, Holland', 'Also follow ?', 'Alvin, TX', 'Am International', 'Amarillo', 'Amazon Seller , Propagandist', 'America', 'America of Founding Fathers', 'America | New Zealand ', 'Americas Newsroom', 'Ames, IA', 'Ames, Iowa', 'Amman, Jordan', 'Amman,Jordan', 'Amsterdam', 'Amsterdam & Worldwide', 'Amsterdam | San Francisco', 'An eight-sided polygon', 'Anaheim', 'Anchorage, AK', 'Anderson, SC', 'Ankara - Malatya - ad Orontem', 'Anna Maria, FL', 'Annapolis, MD', 'Anonymous', 'Antarctica', 'Antigua ?? NYC ', 'Antioch, CA ', 'Anywhere', 'Anywhere I like', 'Anywhere Safe', 'Aperture Science Test Facility', 'Aracaju - Sergipe', 'Area 8 ', 'Argentina', 'Argus Industries \\\\m/666\\\\m/', 'Arizona', 'Arizona ', 'Arkansas', 'Arkansas, Jonesboro', 'Arlington, TX', 'Arlington, VA', 'Arlington, VA and DC', 'Arnhem, the Netherlands', 'Aro Diaspora', 'Around the world', 'Arthas US', 'Arundel ', 'Arvada, CO', 'Asgard', 'Ashburn, VA', 'Asheboro, NC', 'Asheville, NC', 'Ashford, Kent, United Kingdom', 'Ashland, Oregon', 'Ashxjonespr@gmail.com', 'Asia', 'Asia European Continent Korea ', 'Asia Pacific   ', 'Astley, Manchester', 'AsunciÌ_n-PY / TÌ_bingen-GER', 'At Da Laundry Mat Wit Nivea ', 'At Work', 'At your back', 'Athens - Nicosia', 'Athens, Greece', 'Athens,Greece', 'Atlanta', \"Atlanta - FAU class of '18\", 'Atlanta Georgia', 'Atlanta Georgia ', 'Atlanta g.a.', 'Atlanta(ish), GA', 'Atlanta, GA', 'Atlanta, Ga', 'Atlanta, Georgia', 'Atlanta, Georgia USA', 'Atlanta,Ga', 'Atlantic Highlands, NJ', 'Atlantic, IA', 'Atmosphere', 'Attock', 'Auburn ', 'Auburn, AL', 'Auckland', 'Auckland, New Zealand', 'Augusta, GA', 'Augusta, Maine, 04330', 'Aurora, IL', 'Aurora, Ontario ', 'Austin', 'Austin TX', 'Austin | San Diego', 'Austin, TX', 'Austin, Texas', 'Austin/Los Angeles', 'Australia', 'Australia ', 'Australian Capital Territory', 'Aveiro, Portugal', 'Avon', 'Avon, OH', 'Azeroth', 'Aztec NM', 'B&B near Alton Towers', 'BC', 'BIG D  HOUSTON/BOSTON/DENVER', 'BILASPUR,CHHATTISGARH,495001', 'BKI-KUA', 'BOSTON-LONDON', 'BOT ACCOUNT', 'BROKE NIGGAS DREAM!!', 'BROOKLYN, NYC', 'Back East in PA', 'Bacon', 'Bagalkote Karnataka ', 'Bahrain', 'Baker City Oregon', 'Bakersfield, CA', 'Bakersfield, California', 'Balikesir - Eskisehir', 'Baltimore', 'Baltimore, MD', 'Banbridge', 'Bandar Lampung, Indonesia', 'Bandung', 'Bangalore', 'Bangalore City, India', 'Bangalore, INDIA', 'Bangalore, India', 'Bangalore. India', 'Bangkok', 'Bangkok Thailand', 'Bangor, Co.Down', 'Barbados', 'Barcelona, Spain', 'Bartholomew County, Indiana', 'Based in CA - Serve Nationwide', 'Based out of Portland, Oregon', 'Basketball City, USA ', 'Basking Ridge, NJ', 'Bathtub de Bett ', 'Baton Rouge', 'Baton Rouge, LA', 'Bay Area', 'Bay Area, CA', 'Baydestrian', 'Bayonne, NJ', 'Beacon Hills', 'Beaumont, TX', 'Beautiful British Columbia', 'Bedford IN ', 'Bedford, England', 'Behind The Obama Curtain', 'Beirut, Lebanon', 'Beirut/Toronto', 'Beit El - Israel', 'Belbroughton, England', 'Belfast', 'Belgium', 'Belgrade', 'Belleville, Illinois', 'Bellevue NE', 'Bellville, Ohio', 'Bend, Oregon', 'Benedict College', 'Benicia, CA ', 'Benton City, Washington', 'Berlin - Germany', 'Berlin, Germany', 'Berlin, NY, DC, Malibu', 'Beside Basketball', 'BestCoast', 'Between Dire and Radiant', 'Between the worlds ', 'Bhopal, Madhya Pradesh, India.', 'Bhubneshwar', 'Big NorthEast Litter Box', 'Bikini bottom', 'Biloxi, Mississippi', 'Birdland, New Meridian, FD', 'Birmingham', 'Birmingham & Bristol', 'Birmingham UK', 'Birmingham and the Marches', 'Birmingham, England', 'Birmingham, UK', 'Birmingham, United Kingdom', 'Bishops Lydeard, England', 'Bishops Stortford, England', 'BiÌ±an,Laguna', 'Black Canyon New River, AZ', 'Blackpool', 'Blackpool, England, UK.', 'Bleak House', 'Blogland', 'Bloomington, IN', 'Bloomington, Indiana', 'BodÌü, Norge', 'Boise, Idaho', 'Bokaro Steel City, Jharkhand', 'Boksburg', 'Bolivar, MO', 'Bolton & Tewkesbury, UK', 'Bombardment Bay', 'Bon Temps Louisiana', 'Books Published, USA', 'Born in Baltimore Living in PA', 'Bossland', 'Boston', 'Boston MA', 'Boston \\x89Û¢ Cape Cod ?', 'Boston, MA', 'Boston, Massachusetts', 'Boston/Montreal ', 'Boulder', 'Boulder, CO', 'Bournemouth', 'Bournemouth, Dorset, UK', 'Bow, NH', 'Bozeman, Montana', 'Brackley Beach, PE, Canada', 'Bracknell', 'Brasil', 'Brasil, Fortaleza ce', 'Brasil,SP', 'BrasÌ_lia', 'Brazil', 'Brazil ', 'Brazos Valley, Texas', 'Breaking News', 'Brecksville, OH', 'Bremerton, WA', 'Brentwood, NY', 'Brentwood,TN', 'Bridport, England', 'Brighton and Hove', 'Brisbane', 'Brisbane Australia', 'Brisbane, Queensland', 'Brisbane.', 'Bristol', 'Bristol, England', 'Bristol, UK', 'British Columbia, Canada', 'British girl in Texas', 'Brizzle City !', 'Broadview Heights, Ohio', 'Bronx NY', 'Bronx NYC / M-City NY', 'Bronx, NY', 'Bronx, New York', 'Brooklyn', 'Brooklyn, NY', 'Brooklyn, New York', 'Broomfield, CO', 'BrowardCounty // Florida ', 'Brum/Lestah ', 'Bucharest', 'Bucks County, Pa', 'Budapest, Hungary', 'Buenos Aires', 'Buenos Aires, Argentina', 'Buffalo NY', 'Buffalo, NY', 'Buffalo/DC', 'Bug Forest', 'Bukittinggi  ?? Sumatera Barat', 'Bulgaria', 'Burbank,CA', 'Burlington, VT', 'Buscame EL tu Melte', 'Bushkill pa', 'Buxton, Venice, and Nottingham', 'Buy Give Me My Money ', 'CA', 'CA ??DC', 'CA physically- Boston Strong?', 'CA via Brum', 'CA, AZ & NV', 'CAMARILLO, CA', 'CCH ', 'CHICAGO', 'CHICAGO (312)', 'CLT', 'CLVLND', 'COMING SOON', 'CORNFIELDS', 'CPT & JHB, South Africa', 'CT & NY', 'CT ? NYC', 'CT, USA', 'Cairo, Egypt', 'Cairo, Egypt.', 'Calgary', 'Calgary, AB', 'Calgary, AB, Canada', 'Calgary, Alberta', 'Calgary, Alberta, Canada', 'Calgary, Canada', 'Calgary,AB, Canada', 'Calgary/Airdrie/RedDeer/AB', 'California', 'California ', 'California or Colorado', 'California, USA', 'California, United States', 'CamaquÌ£/Pelotas', 'Camberwell, Melbourne', 'Cambridge, MA', 'Cambridge, Massachusetts', 'Cambridge, Massachusetts, U.S.', 'Cameroon', 'Campinas Sp', 'Campo Grande-MS', 'Canada', 'Canada ', 'Canada BC', 'Canada Eh! ', 'Canadian bread', 'Canberra, Australian Capital Territory', 'Canterbury kent', 'Cape Cod', 'Cape Cod, Massachusetts USA', 'Cape Neddick, ME', 'Cape Town', 'Cape Town, Khayelitsha', 'Caracas, Venezuela.', 'Cardiff, UK', 'Cardiff, Wales', 'Caribbean', 'Carol Stream, Illinois', 'Carregado', 'Carry On Jutta!!!', 'Carterville', 'Cascadia', 'Caserta-Roma, Italy ', 'Cassadaga Florida', 'Castaic, CA', 'Catalonia, Spain', 'Catskills', 'Cavite, Philippines', 'Cedar Island, Clinton CT 06413', 'Center for Domestic Preparedness', 'Central Coast, California', 'Central Florida', 'Central Illinois', 'Centurion ', 'Chamblee, Georgia', 'Chandler, AZ', 'Chapel Hill, NC', 'Chappaqua NY and Redlands CA', 'Charleston S.C.', 'Charleston, IL', 'Charleston, SC', 'Charleston, WV', 'Charlotte', 'Charlotte ', 'Charlotte County Florida', 'Charlotte NC', 'Charlotte, N.C.', 'Charlotte, NC', 'Charlotte, NC | KÌ¦ln, NRW', 'Charlotte, North Carolina', 'Charlottetown', 'Charter Member of the VRWC', 'Chasing My Dreams w/Jass??', 'Chatham, IL', 'Chattanooga TN', 'Chennai', 'Cherry Creek Denver CO', 'Cheshire. London. #allover', 'Chester', 'Chester ', 'Chester Football Club', 'Chester, IL', 'Chevy Chase, MD', 'Chicago', 'Chicago - Lake Buena Vista', 'Chicago Area', 'Chicago Heights, IL', 'Chicago IL', 'Chicago, IL', 'Chicago, IL ', 'Chicago, IL 60607', 'Chicago, Il', 'Chicago, Illinois', 'Chicago, but Philly is home', 'Chicago,Illinois', 'ChicagoRObotz', 'Chicagoland', 'Chicopee MA', 'Chicora ?? Oakland', 'Chile', 'China', 'Chippenham/Bath, UK', 'Chiswick, London', 'Chiyoda Ward, Tokyo', 'Chorley, Lancashire, UK', 'Christchurch New Zealand', 'Christiana,Tennessee', 'Cimahi,West Java,Indonesia', 'Cimerak - Pangandaran', 'Cincinnati, OH', 'City Of Joy', 'City of Angels, CA', 'City of London, London', 'Ciudad AutÌ_noma de Buenos Aires, Argentina', 'Clayton, NC', 'Clean World', 'Clearwater, FL', 'Cleveland, OH', 'Cleveland, OH - San Diego, CA', 'Cleveland, Ohio', 'Cleveland, TN', 'Click the link below, okay ', 'Cloud 9', 'Co. Tyrone Northern Ireland', 'Coasts of Maine & California', 'Cobblestone', 'Cochrane, Alberta, Canada', 'Coconut Creek, Florida', 'Coimbatore', 'Colchester Essex ', 'College Station, TX', 'ColoRADo', 'Colombia', 'Colombo,Sri Lanka.', 'Colonial Heights, VA', 'Colorado', 'Colorado Springs', 'Colorado, USA', 'Colorado/WorldWide', 'Columbia Heights, MN', 'Columbia, SC', 'Columbus', 'Columbus ?? North Carolina', 'Columbus, Georgia', 'Columbus, OH', 'Concord, CA', 'Concord, N.C.', 'Concord, NH ', 'Connecticut', 'Conroe, TX', 'Contoocook Valley Region of Ne', \"Conversing In Janet's CafÌ¬\", 'Coolidge, AZ', 'Copenhagen, Capital Region of Denmark', 'Cornwall', 'Corpus - Las Vegas - Houston', 'Corpus Christi', 'Corpus Christi, Texas', 'Cosmic Oneness', 'Costa Rica', 'Cottonwood Arizona', 'County Durham, United Kingdom', 'Coventry', 'Coventry, Rhode Island', 'Coventry, UK', 'Crato - CE ', 'Crayford, London', 'Crouch End, London', 'Croydon', 'Cuernavaca, Morelos, MÌ©xico.', 'Cumming, GA', 'Cuttack, Orissa', 'Cydia', 'Cymru araul', 'Cypress, CA 90630', 'Cyprus', 'D(M)V  ', 'D.C. - Baltimore - Annapolis', 'DC', 'DC Metro area', 'DC, frequently NYC/San Diego', 'DFW, Texas', 'DMV', \"DRAW A CIRCLE THAT'S THE EARTH\", 'DaKounty, Pa', 'Daddy Kink Central', 'Dagenham, Essex', 'Dakar', 'Dalkeith, Scotland', 'Dallas Fort-Worth', 'Dallas, TX', 'Dallas, TX ', 'Dallas, Tejas', 'Dallas, Texas. ', 'Dalston, Hackney', 'Dammam- KSA', 'Danbury, CT', 'Danville, VA', 'Dappar (Mohali) Punjab', 'Darlington', 'Darnley, Prince Edward Island', 'Daruka (near Tamworth) NSW', 'Davao City', 'Davidson, NC', 'Davis, California', 'Dayton, OH', 'Dayton, Ohio', 'Deadend, UK', 'Decatur, GA', 'Definitely NOT the stables', 'Delhi', 'Delhi ', 'Den Helder, Rijkswerf', 'Denton, Texas', 'Denver Colorado. Fun Times', 'Denver, CO', 'Denver, Colorado', 'Deployed in the Middle East', 'Depok', 'Derby', 'Derbyshire, United Kingdom', 'Derry, 17 ', 'Des Moines, IA', 'Des Moines, Iowa ', 'Desde Republica Argentina', 'Desert Storm?? |BCHS|', 'Detroit', 'Detroit Tigers Dugout', 'Detroit, MI', 'Detroit, MI, United States', 'Detroit, Michigan', 'Detroit/Windsor', 'Devon/London ', 'Dhaka', 'Dhaka, Bangladsh', 'Diamondville', 'Dicky Beach', 'Did anybody see me here ??', \"Dil's Campsite\", 'Dimapur', \"Dime's Palace\", 'Dindigul,TamilNadu.', 'Displaced Son of TEXAS!', 'District 12 - Orange County', 'District of Gentrification/ DC', 'Docker container', 'Does it really matter!', 'Doghouse', 'Dorset, UK', 'Dorset, United Kingdom', 'Dover, DE', 'Downtown Churubusco, Indiana', 'Downtown Oklahoma City', 'Dreieich, Germany', 'Dubai', 'Dubai, UAE', 'Dubai, United Arab Emirates', 'Dublin', 'Dublin City, Ireland', 'Dublin, Ireland', 'Dudetown', 'Duncan', 'Dundas, Ontario', 'Dundee', 'Dundee, UK', 'Dunwoody, GA', 'Durand, MI', 'Durban, South Africa', 'Durham N.C ', 'Durham, NC', 'Dutch/English/German', 'Duval, WV 25573, USA ?', 'DÌ_sseldorf, Germany', 'EARTH', 'EARTH ', 'ECSU16', 'EGYPT', 'EIC', 'EIU  Chucktown/LaSalle IL', 'ELVY', 'EPTX', 'EVERYWHERE', 'Eagle Mountain, Texas ', 'Eagle Pass, Texas', 'Eagle River Alaska', 'Ealing, London', 'Earth', 'Earth ', 'Earth 0', 'Earth, Milky Way, Universe', 'Earth-616', 'Earth: Senseless nonsense', 'Earthling (For now!)', 'East Atlanta, Georgia', 'East Aurora, NY', 'East Coast', 'East Islip, NY', 'East Kilbride', 'East Lansing, MI', 'East London', 'East London. ', 'East TN.', \"EastAtlanta ??#WestGeorgia'18\", 'EastCarolina', 'Eastbourne England', 'Eastern Iowa', 'Eastlake, OH', 'Eau Claire, Wisconsin', 'Eaubonne, 95, France', 'Ebola', 'Ecuador', 'Edappally,Kochi', 'Eddyville, Oregon 97343', 'Edinburgh', 'Edinburgh, Scotland', 'Edmonton, Alberta', 'Edmonton, Alberta - Treaty 6', 'El Dorado, Arkansas', 'El Dorado, KS', 'El Paso, TX', 'El Paso, Texas', 'Elchilicitanierraversal ', 'Eldoret, kenya', 'Elizabeth, NJ', 'Elk Grove, CA, USA', 'Elkhart, IN', 'Ellensburg to Spokane', 'Elmwood Park, NJ', 'Elsewhere, NZ', 'Ely, Cambridgeshire', 'Email: Lovethterry@gmail.com', 'Emirates', 'Enfield, UK', 'England', 'England ', 'England & Wales Border, UK', 'England, Great Britain.', 'England, United Kingdom', 'England,UK,Europe,Sol 3.', 'England.', 'English Midlands', 'Enniscrone & Aughris, Sligo ', 'Enterprise, Alabama', 'Enterprise, NV', 'Epic City, BB.', 'Erbil', 'Erie, PA', 'Escondido, CA', 'EspaÌ±a', 'EspaÌ±a - Spain - Espagne', 'EspaÌ±a, Spain', 'Espoo, Finland', 'EspÌ_rito Santo', 'Essex', 'Essex, England', 'Essex/Brighton', 'Est. September 2012 - Bristol', 'Estados Unidos', 'Eugene, Oregon', 'Eureka, California, USA', 'Europe', 'Evanston, IL', 'Evansville, IN', 'Everett, WA', 'Evergreen Colorado', 'Every Where in the World', 'Every where', 'EveryWhere', 'Everywhere', 'Ewa Beach, HI', \"Eww, I'm not Paul Elam\", 'Extraterrestrial Highway', 'FCT, Abuja ', 'FILM OUT LATE 2015', 'FIMAK A.S Ist Bolge Muduru', 'FLYEST HIPPIE YOU KNOW ', 'FOLLOWS YOU everywhere you go', \"FSC '19\", 'Fairfax, VA', 'Fairfield, California', 'Fairgrounds Resident', 'Fairy Tail! ', 'Fakefams', 'Far Away From Home', 'Fashion Heaven. IG: TMId_', 'Federal Capital Territory', 'Fife, WA', 'Financial News and Views', 'Finland', 'Fiore, Lamia Scale', 'Five down from the Coffeeshop', 'Fleet/Oxford, UK', 'Flipadelphia', 'Florida', 'Florida Forever', 'Florida USA', 'Florida but I wanna be n Texas', 'Florida, USA', 'Flushing, Queens', 'Football Field', 'Forging my Story', 'Former Yugoslav Republic of Macedonia', 'Fort Calhoun, NE', 'Fort Collins, CO', 'Fort Fizz, Ohio', 'Fort Knox, KY 40121', 'Fort Lauderdale, FL', 'Fort Myers, Florida', 'Fort Smith, AR', 'Fort Valley,GA/Fayetteville,AR', 'Fort Walton Beach, FL', 'Fort Walton Beach, Fl', 'Fort Wayne, IN', 'Fort Worth,  Texas ', 'Fort Worth, Texas', 'Fountain City, IN ', 'Fountain Valley, CA', 'France', 'Frankfort, KY', 'Franklin, TN near Nashville', 'Frascati', 'Freddy Fazbears pizzeria', 'Fredericksburg, Virginia', 'Fredonia,NY', 'Free State, South Africa', 'Freeport IL. USA', 'Freeport Ny', 'Freeport il ', 'Fresno', 'Fresno, CA', 'Fresno, California', 'Friendswood, TX', 'Frisco, TX', 'From NY. In Scranton, PA', 'From a torn up town MANCHESTER', 'Frome, Somerset, England', 'Frostburg', 'Fruit Bowl', 'Fukuoka, Japan', 'Fukushima city Fukushima.pref', 'Funtua, Nigeria', 'GCC', 'GLOBAL', 'GLOBAL/WORLDWIDE', 'GO BLUE! HAIL YES!!', 'GOT7SupportPH', 'GREENSBORO,NORTH CAROLINA', 'Gaborone, Botswana', 'Gages Lake, IL', 'Gainesville, FL', 'Gainesville/Tampa, FL', 'Galapa / AtlÌÁntico', 'Galatians 2:20 ', 'Galveston, Texas', 'Gameday', 'Gander NF', 'Garden City, NY', 'Garden Grove', 'Garrett', 'Geneva', 'Geneva, Switzerland', 'Geneva. And beyond. ', 'Georgia', 'Georgia ? Tennessee', 'Georgia, U.S.A.', 'Georgia, USA', 'Germany', 'Get our App', 'Gettysburg, PA', 'Giddy, Greenland', 'Gidi', 'Glasgow', 'Glasgow, Scotland', 'Glendale, CA', 'Glenview to Knoxville ', 'Global', 'Global Edition', 'Global-NoLocation', 'Gloucester', 'Gloucester, MA', 'Gloucestershire , UK', 'Goa, India', 'God is Love. ', 'God.Family.Money', 'Gold Coast', 'Gold Coast, Australia', 'Gold Coast, Qld, Australia', 'Gotham', 'Gotham City', 'Gotham City,USA', 'GrC Founder, 8,000 Subscribers', 'Grand Rapids MI', 'Greater Los Angeles Bearia', 'Greater Manchester, UK', 'Greeley, CO', 'Greenfield, Massachusetts', 'Greenpoint', 'Greenpoint, Brooklyn', 'Greensboro, NC', 'Greensboro, North Carolina', 'Greensburg, PA', 'Greenville', 'Greenville, S.C.', 'Greenville,SC', 'Greenwich Meridian', \"Greg's place\", 'Griffin :3', 'Grimsby, England', 'Groningen, Netherlands, Europe', 'Groton, CT', 'Guatemala', 'Guayaquil', 'Guelph Ontario Canada', 'Guildford, UK', 'Gujranwala, Pakistan', 'Gumptown', 'Gurgaon, Haryana. ', 'Gwersyllt, Wales', 'H / pez & sophia ', 'HI\\x89Û¢UT\\x89Û¢AS', 'HOMRA.', 'HTX', 'Haarlem', 'Hackney, London', 'Haddonfield, NJ', 'Hagerstown, MD', 'Haiku, Maui, Hawaii', 'Hailing from Dayton ', 'Halfrica', 'Halifax', 'Halifax, NS, Canada', 'Halifax, Nouvelle-Ìäcosse', 'Halifax, Nova Scotia', 'Halton Region', 'Halton, Ontario', 'Hamburg, DE', 'Hame', 'Hamilton County, IN', 'Hamilton, ON', 'Hamilton, Ontario CA', 'Hamilton, Ontario Canada', 'Hammersmith, London', 'Hampshire UK', 'Hampshire, UK', 'Hampstead, London.', 'Hampton Roads, VA', 'Hannover, Germany', 'Happily Married with 2 kids ', 'Harbour Heights, FL', 'Harlem, NY or Chocolate City', 'Harlem, New York', 'Harlingen, TX', 'Harper Woods, MI', 'Harpurhey, Manchester, UK', 'Harris County, Texas', 'Hartford  London Hong Kong', 'Hartford,  connecticut', 'Hartford, Connecticut', 'Hatteras, North Carolina', 'Hattiesburg, MS', 'Haveli, Maharashtra', 'Havenford', 'Hawaii', 'Hawaii USA', 'Hawaii, USA', 'Hawthorne, NE', 'Haysville, KS', 'Head Office: United Kingdom', 'Headed To The Top', 'Hearts & Minds', 'Heathrow', 'Heaven', 'Heinz Field ', 'Hell', 'Helsinki', 'Helsinki, Finland', 'Hemel Hempstead', 'Henderson, NV', 'Henderson, Nevada', 'Hendersonville, NC', 'Hensley Street, Portland', 'Here & There', 'Here And There', 'Here there and everywhere', 'Here, there and everywhere', 'Here, unless there.  ', 'Here.', 'Hermitage, PA', 'Hermosa Beach, CA', 'Hertfordshire ', 'Hickville, USA', 'Higher Places', 'Highland Park, CA', 'Hillsville/Lynchburg, VA', 'Hilton Head, SC  ', 'Himalayan Mountains', 'Hinterestland', 'Hinton, W.Va.', 'Hogsmeade', 'Hogwarts', 'Holland MI via Houston, CLE', 'Holly Springs, NC ', 'Holly, MI', 'Hollywood', 'Hollywood, CA', 'Home is where we park it!', 'Home of the Takers.', 'Homs- Syria', 'Honduras', 'Honeymoon Ì£ve.', 'Hong Kong', 'Honolulu, Hawaii', 'Honolulu,Hawaii ', 'Hooters on Peachtree', 'Hope Road, Jamaica ', 'Horsemind, MI', 'Hospital, bc of SKH vid.', 'Host of #MindMoversPodcast', 'Houma La', 'Houston', 'Houston ', 'Houston TX', 'Houston |??| Corsicana', 'Houston,  TX', 'Houston, TX', 'Houston, TX  ', 'Houston, Texas', 'Houston, Texas ! ', 'Hoxton, London', 'Htx', 'Huber Heights, OH', 'Hudson Valley, NY', 'Hueco Mundo', 'Hughes, AR', 'Huntington, WV', 'Huntley, IL', 'Huntsville AL', 'Huntsville, AL', 'Huntsville, Alabama', 'Hustletown', 'Hyderabad Telangana INDIA', 'Hyrule', 'I ACCEPT SONG REQUESTS', 'I Heard #2MBikers', 'I O W A', 'I rap to burn shame.', \"I'm standing behind you\", 'I-75 in Florida', 'IDN', 'IG : Sincerely_TSUNAMI', 'IG/SC:bjfordiani', 'IG: AyshBanaysh', 'IJmuiden, The Netherlands', 'IL', 'IM LOST ', 'IN', 'IN our hearts  Earth Global ', 'INDIA', 'IRAQ', 'ITALY', \"IUPUI '19\", 'Ibadan,Oyo state', 'Idaho', 'Ideally under a big tree', 'Ikeja, Nigeria', 'Ikorodu', 'Ile-Ife,Osun state, Nigeria', 'Iliff,Colorado  ', 'Illinois', 'Illinois, USA', 'Illumination ', 'Im Around ... Jersey', 'Im In Route ', 'In #Fairie, where else? ;-)', \"In @4SkinChan 's arms\", 'In Hell', 'In My Lab Creating ', 'In Space', 'In The Mansion', 'In Your Notifications ', 'In a crazy genius mind', 'In a graveyard ', 'In my own world!!!', 'In my studio', 'In the Shadows', 'In the Shadows...', 'In the clouds...', 'In the middle of no where', 'In the moment', 'In the potters hands', 'In the spirit world', 'In your head', 'In your mind', 'Inang Pamantasan', 'Incognito', 'IndiLand ', 'India', 'Indiana', 'Indiana, USA', 'Indianapolis, IN', 'Indonesia', 'Indonesia\\n', 'Inexpressible Island ', 'Inglewood, CA', 'Innerhalb der LÌ_cke', 'Innsmouth, Mass.', 'Inside the Beltway (DC Area)', 'Inside your mind.', 'Inside your webcam. Stop that.', 'Instagram - @heyimginog ', 'Instagram: trillrebel_', 'Insula Barataria', 'Intermountain West', 'International', 'International ', 'International Action', 'Internet', 'InterplanetaryZone', 'Intramuros, Manila', 'Inverness, Nova Scotia', 'Iowa, USA', 'Iraq|Afghanistan| RSA |Baghdad', 'Ireland', 'Irving , Texas', 'Islamabad', 'Island Lake, IL', 'Isle of Man', 'Isolated City In World Perth', 'Israel', 'Istanbul', 'Italy', 'Itirapina, SÌ£o Paulo', 'Ivano-Frankivsk', 'JDB/LJC/AGB/TW/PLL', 'JKT48-Muse-A7X', 'Jackson', 'Jackson TN', 'Jackson, MS', 'Jacksonville Beach, FL', 'Jacksonville, FL', 'Jaipur, India', 'Jaipur, Rajasthan, India', 'Jakarta', 'Jakarta, Indonesia', \"Jakarta/Kuala Lumpur/S'pore\", 'JamDung', 'Jamaica', 'Jammu and Kashmir', 'Jammu | Kashmir | Delhi', 'Jamshedpur, Jharkhand', 'Japan', 'Jeddah_Saudi Arabia.', 'Jersey', 'Jersey - C.I', 'Jersey City, NJ', 'Jersey City, New Jersey', 'Jersey Shore', 'Jerseyville, IL', 'Jerusalem', 'Jerusalem!', 'Jerusalem, Israel', 'Jogja, Indonesia Slowly Asia', 'Johannesburg ', 'Johannesburg, South Africa', 'Johannesburg, South Africa ', 'Johns Creek, GA', 'Jonesboro, AR MO, IOWA USA', 'Jonesboro, Arkansas USA', 'Joshua Tree, CA', 'Jubail IC, Saudi Arabia', 'Jubail IC, Saudi Arabia.', 'July 11th, 2015. ?', 'Jump City', 'Jupiter', 'Just Happy to Be Anywhere', 'Justin and Ariana follow', 'KADUNA, NIGERIA', 'KLA,Uganda', 'KOLKATA', 'KSU 2017', 'Kabul, Tuebingen, Innsbruck', 'Kajang ? UiTM Puncak Alam', 'Kalamazoo, Michigan', 'Kalimantan Timur, Indonesia', 'Kama | 18 | France ', 'Kamloops, BC', 'Kaneohe', 'Kansas', 'Kansas City', 'Kansas City, MO', 'Kansas City, Mo.', 'Kansas, The Free State! ~ KC', 'Karachi', 'Karachi ', 'Karachi Pakistan', 'Karachi, Pakistan', 'Karolinska vÌ_gen 18, Solna', 'Kashmir!', 'Kauai, Hawaii', 'Kawartha Lakes, Ontario, Canad', 'Keighley, England', 'Kelowna, BC', 'Kenosha, WI 53143', 'Kensington, MD', 'Kent', 'Kenton, Ohio', 'Kentucky, USA', 'Kenya', 'Kernow', 'Kettering, OH', 'Kicking Horse Pass', 'Kilkenny', 'Killa Hill, CO', 'Killafornia made me ', 'Killarney', 'Kingston, Jamaica', 'Kingston, Pennsylvania', 'Kingswinford', 'Kirkwall', 'Kleenex factory', 'Knoxville, TN', 'Kodiak, AK', 'Kokomo, In', 'Kolkata', 'Kolkata, India', 'Konoha', 'Konoha Village', 'Korea', 'Ktx', 'Kuala Lumpur', 'Kuala Lumpur, Malaysia', 'Kualar Lumpur, Malaysia', 'KurveZ@GearHeadCentral.net', 'Kutztown, PA', 'Kuwait', 'Kuwait ', 'Kwajalein/Virginia/Dayton, OH', 'Kwara, Nigeria', 'Kyiv, Ukraine', 'KÌ¦ln, Nordrhein-Westfalen', \"L'Enfant Plaza Metro Station\", 'L. A.', 'L/S/Z/L/T/H/C/H/R/A/S/C', 'LA', 'LA - everywhere', 'LA ??', 'LA/OC/Vegas', 'LAGOS', 'LEALMAN, FLORIDA', 'LFC x GSW', 'LITTLETON, CO, USA, TERRAN', 'LIVERPOOL', 'LOCAL ATLANTA NEWS 4/28/00 - 4/28/15 FREELANCER', 'LONG ISLAND, NY', 'LP, MN USA', 'LYNBROOK', 'La Grange Park, IL', 'La Puente, CA', 'Lagos', 'Lagos Nigeria', 'Lagos, Nigeria', 'Laguna Beach, Calif. ', 'Lahar & Gwalior', 'Lahore', 'Lahti, Finland', 'Lake Highlands', 'Lake Monticello, VA', 'Lakewood, Tennessee', 'Lancashire, United Kingdom', 'Lancaster California', 'Lancaster, CA', 'Lancaster, Pennsylvania, USA', 'Land Of The Kings', 'Land of Lincoln', 'Lansdale,Pennsylvania', 'Lansing, Michigan', 'Laredo, TX', 'Largo, MD', 'Las Cruces, NM', 'Las Vegas', 'Las Vegas aka Hell', 'Las Vegas, NV', 'Las Vegas, NV ', 'Las Vegas, NV USA', 'Las Vegas, Nevada', 'Laventillemoorings ', 'Lawrence, KS via Emporia, KS', 'Layang-Layang, Perak', 'Le Memenet', \"Le Moyne '16\", 'Leaving Bikini Bottom', 'Lebanon, Tennessee', 'Leduc, Alberta, Canada', 'Leeds', 'Leeds, England', 'Leeds, U.K.', 'Leeds, UK', 'Leeds, United Kingdom', 'Leesburg, FL', 'Lehigh Valley, PA', 'Leicester', 'Leicester, England', 'Leitchfield Kentucky', 'Lethbridge, AB, Canada', 'Lethbridge, Alberta, Canada', 'Level 3 Garrison, Sector G', 'Lexington', 'LiVE MÌ\\x81S', 'Liberty Lake, WA', 'Liberty Township, Ohio', 'Light and dark, form and void', 'Like us on Face ', 'Lima, OH', 'Lima, Ohio', 'Lima, Peru', 'Lima, PerÌ¼', 'Lima-Peru', 'Lincoln', 'Lincoln City Oregon', 'Lincoln, IL', 'Lincoln, NE', 'Lindenhurst', 'Linton Hall, VA', 'Lisbon, Portugal', 'Littleton, CO', 'Littleton, CO, USA', 'Live Oak, TX', 'Live On Webcam', 'Live mÌÁs', 'Live4Heed??', 'Liverpool', 'Lives in London', 'Livingston, IL  U.S.A.', 'Livingston, MT', 'Livonia, MI', \"Lizzy's Knee\", 'LiÌ¬ge', 'Ljubljana, Slovenia', 'Loading...', 'Location', 'London', 'London ', 'London / Berlin / Online', 'London / Birmingham', 'London UK', 'London, England', 'London, Greater London, UK', 'London, Kent & SE England.', 'London, Riyadh', 'London, Sydney', 'London, UK', 'London, United Kingdom', 'London.', 'London/Bristol/Guildford', 'London/Lagos/FL ÌÏT: 6.6200132,', 'London/New York', 'London/Outlaw Country ', 'London/Surrey ', 'Londonstan', 'Long Beach, CA', 'Long Eaton åá Derbyshire åá UK', 'Long Island', 'Long Island NY & San Francisco', 'Long Island, NY', 'Los Angeles', 'Los Angeles ', 'Los Angeles New York', 'Los Angeles for now', 'Los Angeles, CA', 'Los Angeles, Calif.', 'Los Angeles, California', 'Los Angeles, London, Kent', 'Los Angeles,CA, USA', 'Los Angeles... CA... USA', 'Los Angles, CA', 'Louavul, KY', 'Loughborough, England', 'Loughborough.', 'Loughton, Essex, UK', 'Louisiana', 'Louisiana, USA', 'Louisville, KY', 'Louisville, KY ', 'Loveland Colorado', 'Lowell, MA', 'Lubbock, TX', 'Lubbock, Texas', 'Lucknow, India', 'Lurking', 'Lyallpur, Pakistan', 'Lynchburg, VA', 'Lynnfield, MA', 'Lynwood, CA', \"Lytham St Anne's \", 'M!$$!$$!PP!', 'M!A: None', 'MA', 'MA via PA', 'MAD as Hell', 'MAURITIUS', 'MD', 'MI', 'MI - CA', 'MI,USA', 'MIchigan', 'MNL', 'MUM-DEL', 'MY RTs ARE NOT ENDORSEMENTS', 'Macclesfield', 'Mackay, QLD, Australia', 'Mackem in Bolton', 'Macon, GA', 'Macon, Georgia', 'Made Here In Detroit ', 'Made in America', 'Madison, GA', 'Madison, WI', 'Madison, WI & St. Louis MO', 'Madison, Wisconsin, USA', 'Madisonville TN', 'Madrid', 'Madrid, Comunidad de Madrid', 'Magnolia', 'Magnolia, Fiore ', 'Maharashtra', 'Makai', 'Making Worldwide Change Near U', 'Malang', 'Malaysia', 'Malaysia/Jordan', 'Maldives', 'Malibu/SantaFe/Winning!', 'Manavadar, Gujarat', 'Manchester', 'Manchester UK', 'Manchester, England', 'Manchester, NH', 'Manchester, The World, England', 'Manchester, UK', 'Manhattan', 'Manhattan, NY', 'Manila', 'Manila City', 'Manila, Philippines', 'Mankato, MN', 'Maracay y Nirgua, Venezuela', 'Marbella. Spain', 'Maricopa, AZ', 'Marietta, GA', 'Mariveles, Bataan', 'Marrakech MÌ©dina, Marrakech - Tensift - Al Haouz', 'Mars', 'Maryland', 'Maryland ', 'Maryland, USA', 'Maryland,Baltimore', 'Marysville, MI', 'Mass', 'Massachusetts', 'Massachusetts ', 'Massachusetts, USA', 'McLean, VA', \"Me mammy's belly\", 'Medan,Indonesia', 'MedellÌ_n, Antioquia', 'Medford, NJ', 'Medford, Oregon', 'Meereen ', 'Melbourne', 'Melbourne Australia', 'Melbourne, Australia', 'Melbourne, Australia.', 'Melbourne, FL', 'Melbourne, Florida', 'Melbourne, Victoria', 'Melbourne-ish', 'Melrose', 'Melton, GA', 'Memphis', 'Memphis, TN', 'Memphis, in the Tennessees', 'Memphis,TN/ World Wide', 'Menasha, WI', 'Menlo Park. SFO. The World.', 'Mentor OH', 'Merica!', 'Mesa, AZ', 'Methville, CA', 'Metro Manila', 'Mexico City', 'Mexico! ^_^', 'Miami', 'Miami ??', 'Miami Beach, Fl', 'Miami via Lima', 'Miami, FL', 'Miami, Florida', 'Miami,FL', 'Miami,Fla', 'Miami?Gainesville', 'Michel Delving.', 'Michigan', 'Michigan ', 'Michigan, USA', 'Mid West', 'Mid north coast of NSW', 'Middle Earth / Asgard / Berk', 'Midwest', 'Midwest City, OK', 'Midwestern USA', 'Milky Way galaxy ', 'Milton Keynes ', 'Milton Keynes, England', 'Milton keynes', 'Milton/Tallahassee', 'Milwaukee County', 'Milwaukee WI', 'Milwaukee, WI', 'Minna, Nigeria', 'Minneapolis - St. Paul', 'Minneapolis, MN', 'Minneapolis,MN,US', 'Minneapolis/St. Paul', 'Minority Privilege, USA', 'Mississauga, Ontario', 'Missouri, USA', 'Mo.City', 'Mogadishu, New Jersey', 'Mogadishu, Somalia', 'Moncton, New Brunswick', 'Mongolia', 'Montana ', 'Montana, USA', 'Monterrey, MÌ©xico', 'Montgomery County, MD', 'Montgomery, AL', 'Montreal', 'MontrÌ©al, QuÌ©bec', 'Moore, OK', 'Mooresville, NC', 'Mooseknuckle, Maine', 'Morganville, Texas.', 'Morioh, Japan', 'Morocco', 'Morris, IL', 'Moscow', 'Moscow, Russia', 'Mostly Wellington, NZ ', 'Mostly Yuin.', 'Mount Vernon, NY', 'Mountains', \"Mpela'zwe \", 'Multinational *****', 'Mumbai', 'Mumbai (India)', 'Mumbai , India', 'Mumbai india', 'Mumbai, India', 'Mumbai, Maharashtra', 'Muntinlupa City, Philippines', 'Murray Hill, New Jersey', 'My contac 27B80F7E 08170156520', 'My heart is a ghost town!', 'My mind is my world', 'My old New England home', 'My subconscious', 'Mysore, Karnataka', 'MÌ©rida, YucatÌÁn', 'MÌ©xico', 'MÌ©xico D.F.', 'N. California USA', 'N?? Y???.', 'NAIROBI  KENYA ', 'NAWF SIDE POKING OUT ', 'NBO', 'NC', 'NC || OR', 'NEPA/570', 'NEW YORK', 'NEWCASTLE', 'NH via Boston, MA', 'NIFC', 'NIGERIA', 'NJ', 'NJ/NY/NM/NE/ND', 'NJ/NYC', 'NOLA ?? TX', 'NV', 'NY', 'NY Capital District', 'NY || live easy? ', 'NY, CT & Greece', 'NY, NY', 'NYC', 'NYC / International', 'NYC :) Ex- #Islamophobe', 'NYC area', 'NYC metro', 'NYC&NJ', 'NYC, New York', 'NYC,US - Cali, Colombia', 'NYC-LA-MIAMI', 'NYHC', 'Na waffi', 'Na:tinixw / Hoopa, Berkeley', 'Nadiad ,Gujarat , India!!', 'Nagpur', 'Nairobi', 'Nairobi , Kenya', 'Nairobi, Kenya', 'Nairobi, Kenya ', 'Nairobi-KENYA', 'Nakhon Si Thammarat', \"Namjoon's pants\", 'Nanaimo, BC, Canada', 'Nantes, France', 'Napa, CA', 'Naperville', 'Narnia', 'Narnia, Maryland', 'Nashua NH', 'Nashville', 'Nashville, TN', 'Nashville, Tennessee', 'Nashville, Tn', 'Near Richmond, VA', 'Near Warrington', 'Near Yosemite', 'Neath, South Wales', 'Nebraska, Colorado & The GLOBE', 'Nelspruit, South Africa', 'Netherlands', 'Netherlands,Amsterdam-Virtual ', 'Nevada (wishing for Colorado)', 'Nevada, USA', 'Neverland', 'New Britain, CT', 'New Brunswick, NJ', 'New Chicago', 'New Delhi', 'New Delhi, Delhi', 'New Delhi, India', 'New Delhi,India', 'New England', 'New Hampshire', 'New Hampshire, USA', 'New Hanover County, NC', 'New Haven, Connecticut', 'New Jersey', 'New Jersey ', 'New Jersey, USA', 'New Jersey, usually', 'New Jersey/ D.R.', 'New Jersey/New York', 'New Mexico, USA', 'New Orleans ,Louisiana', 'New Orleans, LA', 'New Orleans, Louisiana', 'New South Wales, Australia', 'New Sweden', 'New York', 'New York ', 'New York - Connecticut', 'New York / Worldwide', 'New York 2099', 'New York ? ATL', 'New York Brooklyn', 'New York City', 'New York City ,NY', 'New York City, NY', 'New York NYC', 'New York, NY', 'New York, NY ', 'New York, New York', 'New York, USA', 'New York, United States', 'New York. NY', 'New Your', 'New Zealand', 'Newark, NJ', 'Newcastle', 'Newcastle Upon Tyne, England', 'Newcastle upon Tyne', 'Newcastle, England', 'Newcastle, England ', 'Newcastle, OK', 'Newport, Wales, UK', 'Newton Centre, Massachusetts', 'Newton, NJ 07860', 'Niagara Falls, Ontario', \"Niall's arms\", 'Nice places ', 'Nicola Valley', 'Nicoma Park, OK', 'Nigeria', 'Nigeria ', 'Nigeria, Global', 'Nigeria, WORLDWIDE', 'Nirvana', 'Njoro, Kenya', 'No ID, No VOTE!!!', 'Noida, NCR, India', 'Nomad, USA', 'Norf Carolina', 'Norman, Oklahoma', 'North', 'North America', 'North Cack/919', 'North Carolina', 'North Carolina ', 'North Carolina, USA', 'North Dartmouth, Massachusetts', 'North East / Middlesbrough ', 'North East USA', 'North East Unsigned Radio', 'North East, England', 'North Ferriby, East Yorkshire', 'North Hastings Ontario', 'North Highlands, CA', 'North Jersey', 'North London', 'North Memphis/Global Citizen', 'North Port, FL', 'North Vancouver, BC', 'North West England UK', 'North West London', 'North-East Region, Singapore', 'Northampton, MA', 'Northern California U.S.A.', 'Northern Colorado', 'Northern Ireland', 'Northern Kentucky, USA', 'Norwalk, CT', 'Norway', 'Norwich', 'Not Los Angeles, Not New York.', 'Not Of This World', 'Not Steven Yeun / AMC.', 'Not a U.S resident', 'Not where I want to be, yet', 'Nottingham', 'Nottingham, England', 'Nottingham, United Kingdom', 'Notts', 'Nova Scotia, Canada', 'Novi, MI', 'Nowhere Islands/Smash Manor', 'Nowhere. Everywhere.', 'Numa casa de old yellow bricks', 'Numenor', 'Nunya', 'OES 4th Point. sisSTAR & TI', 'OK', 'ON', 'Oakland', 'Oakland, CA', 'Oakland, Ca', 'Oblivion?', 'Ocean City, NJ', 'Odawara, Japan', 'Official Website', 'Ogba, Lagos, Nigeria', 'Ohio', 'Ohio, USA', 'Ojodu,Lagos', 'Okanagan Valley, BC', 'Oklahoma', 'Oklahoma City', 'Oklahoma City, OK', 'Oklahoma, USA', 'Okuma Town, Fukushima', 'Olathe, KS', 'Old Blighty', 'Oldenburg // London', 'Olympia, WA', 'On a beach ', 'On the court ', 'On the toilet having a dump ', 'Ondo', 'One World', 'Oneonta, NY/ Staten Island, NY', 'Online 24/7. Not even kidding.', 'Ontario', 'Ontario Canada', 'Ontario, Canada', 'Ontario, Canada. ', 'Orange County, CA', 'Orange County, Calif.', 'Orange County, California', 'Orange County, NY', 'Orbost, Victoria, Australia', 'Oregon', 'Oregon and Washington', 'Oregon, USA', 'Orlando', 'Orlando ', 'Orlando, FL', 'Orlando, Fl', 'Orlando,FL  USA', 'Orlando/Cocoa Beach, FL', 'Orm', 'Ormond By The Sea, FL', 'Oshawa, Canada', 'Oshawa/Toronto', 'Oslo, Norway', 'Otsego, MI', 'Ottawa, Canada', 'Ottawa, Ontario', 'Ottawa,Ontario Canada', 'Our Empire State', 'Outside The Matrix, I Think.', 'Over the Moon...', 'Overland Park, KS', 'Overton NV', 'Own planet!!', 'Oxford / bristol', 'Oxford, MS', 'Oxford, OH', 'PA', 'PA, USA', 'PA.USA', 'PARACHUTE', 'PDX', 'PG Chillin!', 'PG County, MD', 'PH', 'PLFD cuh..', 'POFFIN', 'PROUD INDIANS', 'PROV', 'PS4, now stop asking', 'PSA Nursing ', 'PSN: Pipbois ', 'PURPLE BOOTH STUDIO\\x89ã¢', 'Pacific Northwest', 'Paducah, KY', 'Paignton', 'Pakistan', 'Pakistan, Islamabad', 'Palermo, Sicily', 'Palestine ', 'Palestine Texas', 'Palm Bay, FL (Kissimmee)', 'Palm Beach County, FL', 'Palm Desert, CA', 'Palma, Islas Baleares', 'Palmyra, NJ', 'Palo Alto, CA', 'Palo Alto, California', 'Paname City', 'PanamÌÁ ', 'Paonia, Colorado ', 'Paradise City', 'Paradise, NV', 'Paranaque City', 'Paris', 'Paris ', 'Paris (France)', 'Paris, France', 'Paris.', 'Park Ridge, Illinois', 'Passamaquoddy', 'Paterson, New Jersey ', 'Patra-Greece.', 'Paulton, England', 'Pawnee', 'Pedophile hunting ground', 'Pekanbaruå¡Batam Islandå¡Medan', 'Pelham, AL', 'Pembroke NH', 'Penn Hills, PA', 'Pennsylvania', 'Pennsylvania, PA', 'Pennsylvania, USA', 'Pensacola, FL', 'Peoria', 'Perenjori, WA', 'Perth, Australia', 'Perth, Western Australia', 'Perthshire ', 'Peru', 'Peshawar', 'Petaluma, CA', 'Peterborough, On', 'Peterborough, Ont.', 'Peterborough, Ontario, Canada', 'Phila.', 'Philadelphia', 'Philadelphia, PA', 'Philadelphia, PA ', 'Philadelphia, PA USA', 'Philadelphia, Pennsylvania', 'Philadelphia, Pennsylvania USA', 'Philippines', 'Philippines ', 'Phoenix', 'Phoenix Az', 'Phoenix, AZ', 'Phoenix, Arizona, USA', 'Photo : Blue Mountains ', 'Piedmont Area, North Carolina', 'Piedmont Triad, NC', 'Pig Symbol, Alabama', 'Pioneer Village, KY', 'Pittsburgh', 'Pittsburgh ', 'Pittsburgh PA', \"Plain O' Texas\", 'Planet Earth', 'Planet Eyal, Shandral System', 'Planet of da Bathing Apes', 'Plano, IL', 'Plano, Texas', 'Plano,TX', 'Play For Ryan ??', 'Playa', 'Playa del Carmen, Mexico', 'Pleasanton, CA', 'Pluto', 'Pocatello, ID', 'Pocatello, Idaho', 'Poconos', 'Polmont ', 'Pomfret/Providence', 'Pompano Beach, FL', 'Pon Di Gully', 'Pontefract UK', 'Pontevedra, Galicia', 'Poplar, London', 'Port Charlotte, FL', 'Port Harcourt, Nigeria', 'Port Jervis, NY', 'Port Orange, FL', 'Port Williams NS', 'Portage, IN / Worldwide', 'Porthcawl', 'Portland, OR', 'Portland, Ore. ', 'Portland, Oregon', 'Porto Alegre, Rio Grande do Sul', 'Portoviejo-Manabi-Ecuador', 'Portsmouth, UK', 'Portsmouth, VA', 'Portugal', 'Positive 852', 'Pratt-on-Wye', 'Predjama, Eslovenia.', 'Prehistoric Earth', 'Pretoria', 'Primum non nocere', 'Principality of Zeron', 'Pro-American and Anti-#Occupy', 'Proud @BuckMasonUSA supporter!', 'Proudly Canadian!', 'Proudly frozen Canuck eh !!', 'Providence RI / Lisnaskea ', 'Pueblo, CO', 'Pueblo, Colorado', 'Puerto Rico', 'PunPunlÌ¢ndia', 'Pune, Maharashtra', 'Pune, mostly ', 'Punjab', 'Purfleet', 'Purgatory, USA', 'QLD Australia', 'QUEENS.', 'Quantico, VA', 'Queen Creek AZ', 'Queens New York', 'Queens, NY', 'Queensland', 'Queensland, Australia', 'Quezon City, Philippines', 'Quilmes , Arg', 'Quincy', 'Quincy MA', 'Quito, Ecuador.', \"R'lyeh, South Pacific\", 'REPUBLICA DOMINICANA', 'RSN: Tru', 'Rafael castillo', 'Raleigh (Garner/Cleveland) NC', 'Raleigh Durham, NC', 'Raleigh, NC', 'Rapid City, Black Hills, SD', 'Rapid City, South Dakota', 'Reading MA', 'Reading UK', 'Reading a romance novel', 'Reality', 'Realville', 'Redding, California, USA', 'Reddit', 'Reddit ', 'Redondo Beach, CA', 'Regalo Island', 'Renfrew, Scotland', 'Republic of Texas', 'Republic of the Philippines', 'Republica Dominicana', 'Reston, VA, USA', 'Rheinbach / Germany', 'Rhode Island', 'RhodeIsland', 'Rhyme Or Reason?', 'Richardson TX', 'Richmond Heights, OH', 'Richmond, VA', 'Right here', 'Right next to Compton', 'Rio', 'Rio de Janeiro', 'Riverdale, GA ', 'Riverside, CA', 'Riverside, California.', 'Riverview, FL ', 'Riyadh', \"Riyadh ')\", 'Roads/Trails Everywhere', 'Roadside', 'Roaming around the world', 'Roanoke VA', 'Roanoke, VA', \"Robin Hood's County \", 'Rochelle, GA', 'Rochester', 'Rochester Hills, MI', 'Rochester, NY', 'Rock Hill, SC', 'Rock Springs, WY', 'Rocketing through the galaxy', 'Rockford, IL', 'Rockland County, NY', 'Rockville, Maryland', 'Rocky Mountains', 'Rogersville, MO', 'Romania', 'Rome, Italy', 'Room 234', 'Roppongi, Minato, Tokyo ', 'Rotterdam, The Netherlands', 'Rotterdam, Zuid-Holland', 'Royton', 'Rural Northern Nevada', 'Russia', 'Rutherfordton, NC', 'S.F. Bay area', 'SD |Norway| KSA', 'SE London(heart is by the sea)', 'SEA Server', 'SEATTLE, WA USA', 'SEC Country', 'SF Bay Area', 'SF Bay Area, California / Greater Phoenix, AZ', 'SOUTHERN CALIFORNIA DESERT', 'STL ?NOLA', 'SURROUNDED BY WEEABOOS', 'SWMO', 'SWinfo@dot.state.al.us', 'Sacae Plains', 'Sacramento', 'Sacramento, CA', 'Sacramento, California', 'Saint Louis, Missouri', 'Saint Lucia', 'Saint Marys, GA', 'Saint Paul', 'Saipan, CNMI', 'Sale, England', 'Saline, MI', 'Salt Lake City, UT', 'Salt Lake City, Utah', 'Saltillo, Coahuila de Zaragoza', 'San Antonio, TX', 'San Antonio-ish, TX', 'San Diego', 'San Diego CA', 'San Diego California 92101', 'San Diego, CA', 'San Diego, Calif.', 'San Diego, California', 'San Diego, Texas.', 'San Francisco', 'San Francisco , CA', 'San Francisco Bay Area', 'San Francisco, CA', 'San Fransokyo', 'San Jose', 'San Jose, CA', 'San Jose, CA, USA', 'San Jose, California', 'San Juan, Puerto Rico', 'San Luis Obispo, CA', 'San Mateo County, CA', 'Sand springs oklahoma', 'Sandton, South Africa', 'Sanganer, Rajasthan', 'Santa Clara, CA', 'Santa Cruz, CA', 'Santa Maria, CA', 'Santa Monica, CA', 'Santiago Bernabeau', 'Santiago de Chile', 'Santiago de Cmpostela Galicia', 'Santiago,RepÌ¼blica Dominicana', 'Santo Domingo Alma Rosa ', 'Sao Paulo', 'Sao Paulo, Brazil', 'Sarasota, FL', 'Saskatchewan, Canada', 'SaudI arabia - riyadh ', 'Saudi Arabia', 'Savage States of America', 'Savannah, GA', 'Scituate, MA', 'Score More Goals Buying @', 'Score Team Goals Buying @', 'Scotland', 'Scotland ', 'Scotland, United Kingdom', 'Scotts Valley, CA', 'Scottsdale, AZ', 'Scottsdale. AZ', 'Screwston, TX', 'Searching for Bae ', 'Seattle', 'Seattle WA', 'Seattle native in Prescott, AZ', 'Seattle, WA', 'Seattle, Washington', 'See the barn of bleakness', 'Selangor', 'Selena | Britney | Hilary', 'Selma2Oakland', 'Semarang, Indonesia', 'Serva Fidem', 'Sevier County.', 'Shady Pines ', 'Shah Alam,Malaysia', 'Shanghai', \"Sharkatraz/Bindle's Cleft, PA\", 'Sheff/Bangor/Salamanca/Madrid', 'Sheffield Township, Ohio', 'Sheffield/Leeds', 'Shelby County', 'Sherwood, Brisbane, Australia', 'Shipwreck Cove', 'Shirley, NY', 'Shity land of Northern Ireland', 'Shrewsbury', 'Sicamous, British Columbia', 'Silang, Cavite / ParaÌ±aque', 'Silesia, Poland', 'Silicon Valley', 'Silver Spring, MD', 'Silvermoon or Ironforge', 'Singapore', 'Sioux Falls, S.D. ', 'Sioux Falls, SD', 'Skyhold', 'Skyport de la Rosa', 'Slappin and Smackin ', 'Slateport City, Hoenn', 'Slatina,Romania', 'Sligo and Galway, Ireland', 'Smash Manor/Kanto', 'SoCal', 'SoDak', 'Sochi, KDA, RU', 'Somalia', 'Some Where in this World', 'Some other mansion', 'Some pum pum', 'Some where', 'Somecity, Somerset, MD', \"Someday I'll live in England. \", 'Somerset, UK', 'Somewhere', 'Somewhere ', 'Somewhere Around You', 'Somewhere Only We Know ?', 'Somewhere Out There', 'Somewhere Powerbraking A Chevy', 'Somewhere between Chicago & Milwaukee', 'Somewhere between here & there', 'Somewhere else...', 'Somewhere in China.', 'Somewhere in Jersey', 'Somewhere in Spain', 'Somewhere in the Canada', 'Somewhere out there', 'Somewhere with Clyde', 'Soufside', 'Soul Somalia/Body Montreal', 'South 37', 'South Africa', 'South Asia', 'South Bloomfield, OH', 'South Carolina', 'South Carolina, USA', 'South Central Wales', 'South Florida', 'South Korea GMT+9', 'South Pasadena, CA', 'South Stand', 'South West, England', 'South africa', 'South east of U.K', 'South of D.C.', 'South, England', 'South, USA', 'SouthEast Asia', 'Southern Califorina', 'Southern California', 'Southern Maine', 'Spain', 'Spain - China - Latin America.', 'Spain but Opa-Locka, FL', \"Spare 'Oom\", 'Speaking the Truth in Love', 'Spinning through time.', 'Spokane, WA', 'Spokane, Washington', 'Spokane, Washington 99206', 'Spring Grove, IL', 'Spring Tx', 'Spying on your thoughts', 'Srinagar,Kashmir', 'St Austell, Cornwall', 'St Charles, MD', 'St Joseph de Beauce', 'St Louis, MO', 'St Paul, MN', 'St PetersburgFL', 'St. Catharines, Ontario', \"St. John's, NL, Canada\", 'St. Joseph, Minnesota', 'St. Louis', 'St. Louis Mo.', 'St. Louis, MO', 'St. Louis, Missouri', 'St. Louis, Mo', \"St. Patrick's Purgatory\", 'St.Cloud, MN', 'Stage with Trey Songz', 'Stalybridge, Tameside', 'Stamford & Cork (& Shropshire)', 'Stanford University', 'Starling City', 'State College, PA', 'State College, Pa', 'State of Dreaming', 'State of Georgia', 'Stateless Global Citizen', 'Statute Of Limitations_', 'Stay Fly?', 'Stay Tuned ;) ', 'Still. ??S.A.N.D.O.S??', 'Stockholm, Sweden', 'Stockton on tees Teesside UK', 'Storybrooke ', 'Storybrooke / The Moors', 'Stowmarket', 'Stratford, CT', 'Street of Dallas', 'Studio', 'Subconscious LA', 'Suburban Detroit, Michigan', 'Sugar Land, TX', 'Sugarhouse, UT', 'Suginami-ku, Tokyo, Japan', 'Suitland', 'Sumter, SC', 'Sunbury, Ohio', 'Sunny South florida ', 'Sunnyvale, CA', 'Sunrise Manor, NV', 'Sunshine Coast, Queensland', 'Suplex City', 'Surabaya', 'Surrey & Manchester', 'Surry Hills, Sydney', 'Surulere Lagos,Home Of Swagg', 'Sutton, London UK', 'Suva, Fiji Islands.', 'Swag Francisco', 'Swan River', 'Swaning Around', 'Sweden', 'Swindon,England ', 'Switzerland', 'Sydney', 'Sydney & Worldwide', 'Sydney Australia', 'Sydney, Australia', 'Sydney, NSW', 'Sydney, New South Wales', 'Sylacauga, Alabama', 'SÌ£o Paulo', 'SÌ£o Paulo SP,  Brasil', 'SÌ£o Paulo, Brasil', 'T E X A S | wwat 8.24.14', 'T-Ville', 'THANJAVUR', 'THE 6IX', 'THE WORLD T.G.G / M.M.M ', 'TN', 'TV5, Philippines', 'TX', 'Tacoma,Washington', 'Tafekop Ga-Matsepe', 'Tallahassee Florida', 'Tallahassee, FL', 'Tama, Iowa', 'Tampa', 'Tampa, FL', 'Tampa, Fl', 'Tampa-St. Petersburg, FL', 'Tamworth', 'Tarragona', 'Taylor Swift', 'Team Slytherin', 'TechFish ', 'Ted&Qz Inc, Ireland, Europe', 'Telangana', 'Tema,Accra', 'Temecula, CA', 'Temporary Towers', 'Tennessee', 'Tennessee, USA', 'Tennessee/Gallifrey', 'Terlingua, Texas', 'Terre Haute, IN', 'Texas', 'Texas ', 'Texas af', 'Texas, USA', 'Texas-USA\\x89ã¢ ?', 'Thailand', 'Thailand Malaysia Indonesia ', 'Thane', 'The 5th Dimension. ', 'The ?? below ???', 'The American Wasteland (MV)', 'The Canopy Kingdom', 'The Circle of Life', 'The Citadel, Oldtown, Westeros', 'The D', 'The Desert', 'The Desert of the Real', 'The Empire/First Order', 'The Epicenter, and Beyond', 'The Forever Girl', 'The Globe', 'The Great State of Maine ', 'The Great State of Texas', 'The Grey Area', 'The Hammock, FL, USA', 'The Harbinger.', 'The Howling', 'The Internet', 'The Internet & NYC', 'The Internetz', 'The Jewfnited State', 'The Kingdom of Fife, Scotland', 'The Land of MAss Stupidity', 'The Low-Cal Calzone Zone', 'The Main ', 'The Meadow', 'The Memesphere', 'The Multiverse', 'The Netherlands', 'The North', 'The Orwellion police-state', 'The P (South Philly)', 'The Peach State', 'The Pig Sty', 'The Pumpkin Carriage of Dreams', 'The Sanctuary Network, Rome', 'The Shady Hyenatown of Finland', 'The Shire', 'The South & WestCoast ', \"The Sun's Corona\", 'The TARDIS', 'The Triskelion', 'The UK', 'The Universe', 'The Waystone Inn', 'The Web', 'The Weird Part of Wonderland', 'The Windy City', 'The Wood', 'The World', 'The barn', 'The dark', 'The green and pleasant land.', 'The land of New Jersey. ', 'The shores of Lake Kilby', 'The windy plains of Denver', 'Thibodaux, LA', 'Third rock from the Sun', 'This Is Paradise. Relax. ', 'Thornton  Colorado', 'Thrissur', 'Timaru District, New Zealand', 'Tipperary (Long Way) ', 'Tips on my blog at', 'Tn', 'To The Right of You!', 'Tokyo', 'Tokyo & Osaka', 'Toledo, OH', 'TonyJ@Centralizedhockey.com', 'Top Secret', 'Top secret bunker ', 'Topeka, KS', 'Tornado Alley, USA ', 'Toronto', 'Toronto, Bob-Lo, Miami Beach', 'Toronto, Canada', 'Toronto, ON', 'Toronto, ON, Canada', 'Toronto, Ontario', 'Toronto, Worldwide ', 'Toronto-Citizen of Canada & US', 'Torrance, CA', 'Torry Alvarez love forever ? ?', 'Trackside California', 'Tractor land aka Bristol', 'Trancy Manor', 'Trapped in my Conscience ', 'Travelling around the world ', 'Traverse City, MI', 'Tri-Cities, Wash.', 'Tring ', 'Tring, UK', 'Trinidad & Tobago', 'Trinidad and Tobago', 'Trinity, Bailiwick of Jersey', 'Tripsburg, ms.', 'Trost District', 'Trumann, Arkansas', 'Tucson, AZ', 'Tucson, Arizona ', 'Tucson, Az', 'Tulalip, Washington', 'Tulsa, OK', 'Tulsa, Oklahoma', 'Tunbridge Wells', 'Turkmenistan', 'Twitter Lockout in progress', 'Twitterville', 'Two Up Two Down', 'Tyler, TX', 'TÌÁchira - Venezuela', 'U.K.', 'U.S', 'U.S.', 'U.S. Northern Virginia', 'U.S.A', 'U.S.A and Canada', 'U.S.A.   FEMA Region 5', 'U.S.A. - Global Members Site', 'UAE,Sharjah/ AbuDhabi', \"UGA '15 Alumnus - Economics \", 'UK', 'UK  & Germany', 'UK & Ibiza', 'UK Great Britain ', 'UK, Republic of Ireland and Australia', 'UK,singer,songwriter,?2 act', 'UPTOWN ', 'US', 'US, PA', 'US-PR', 'US: 44.414510,8.942499', 'USA', 'USA ', 'USA (Formerly @usNOAAgov)', 'USA , AZ', 'USA - Canada - Europe - Asia', 'USA, Alabama', 'USA, Haiti, Nepal', 'USA, North Dakota', 'USA, WA', 'USA/SO FLORIDA via BROOKLYN NY', 'USAoV', 'Uganda', 'Ukraine', 'Ukraine and Ireland', 'Under Santa Barbara Skies', 'Unite. Bless. Wallahi ', 'United Hoods of the Globe', 'United Kingdom', 'United Kingdom,Fraserburgh', 'United States', 'United States of America', \"United States where it's warm\", 'Unites States', 'University Heights, Ohio', 'University of Chicago', 'University of Limerick', 'University of South Florida', 'University of Toronto', 'Unknown', 'Unknown ', 'Unnamed City', 'Up a hill', 'Upper St Clair, PA', 'Upper manhattan, New York', 'Uppsala, Sweden', 'Upstairs.', 'Upstate New York', 'Uruguay / Westeros / Gallifrey', 'Use #TMW in tweets get #RT', 'Utah', 'Utah, USA', 'Utica NY', 'Uyo, Akwa Ibom State, Nigeria', 'V-RP @OZRP_ ?MV, AU, R18+?', 'VCU', 'VISIT MY YOUTUBE CHANNEL.', 'VONT ISLAND, LAGOS', 'Va Beach, Virginia', 'Vail Valley', 'Valle Del Sol', 'Valparaiso ', 'Van Buren, MO', 'Vancouver', 'Vancouver (HQ) and worldwide', 'Vancouver BC', 'Vancouver Canada', 'Vancouver, BC', 'Vancouver, BC, Canada', 'Vancouver, BC.', 'Vancouver, British Columbia', 'Vancouver, Canada', 'Vancouver, Colombie-Britannique', 'Varanasi', 'Varies ', 'Venezuela', 'Ventura', 'Ventura, Ca', 'Vermont, USA', 'Vero Beach , FL', 'Very SW CA, USA....Draenor', 'Victoria, Australia, Earth', 'Victoria, BC', 'Victoria, BC  Canada', 'Victoria, British Columbia', 'Victoria, Canada', 'Victoria, Tx.', 'Victorville, CA', 'Vidalia GA', 'Viejo', 'Vietnam', 'Vilnius', 'Vineyard', 'Virginia', 'Virginia, USA', 'Virginia, United States', 'Virgo Supercluster', 'Visit our  dedicated website @', 'Vista, CA', \"Viterbo BFA Acting '18\", 'VitÌ_ria (ES)', 'Voorhees, NJ', 'VÌ_a LÌÁctea', 'VÌ_sterÌ´s, Sweden', 'W.I.T.S Academy', 'WA State', 'WAISTDEEP, TX', 'WASHINGTON,DC', 'WESTSIDE OF PHILLY 7? BLOCK??', 'WORDLDWIDE', 'WORLD', 'WORLD WIDE', 'WORLDWI$E ', 'WORLDWIDE!', 'WORLDWIDE-BOSTON', 'WV, love the blue and gold', 'Waco TX', 'Waco, Texas', 'Waddesdon', 'Wahpeton, ND', 'Waialua, Hawaii', 'Wailuku, Maui', 'Wakanda', 'Wakefield MA', 'Wakefield, West Yorkshire', 'Wales', 'Wales, United Kingdom', 'Walker County, Alabama', 'Walthamstow, London', 'Wanderlust', 'Wandsworth, London', 'Warm Heart Of Africa', 'Warrandyte, Australia', 'Warri', 'Warsaw', 'Warszawa', 'Warwick, RI @Dollarocracy also', 'Washington', 'Washington D.C.', 'Washington DC', 'Washington DC / Nantes, France', 'Washington State', 'Washington state', 'Washington, D.C.', 'Washington, D.C. ', 'Washington, D.C., area', 'Washington, DC', 'Washington, DC & Charlotte, NC', 'Washington, DC 20009', 'Washington, DC NATIVE', 'Washington, Krasnodar (Russia)', 'Washington, USA', 'Wasington, DC', 'Watch Those Videos -', 'Waterford MI', 'Waterfront', 'Waterloo, ON', 'Waterloo, Ont', 'Watertown, Mass.', 'Waukesha, WI', 'Wausau, Wisconsin', 'Waverly, IA', 'We are global!', \"We're All Mad Here\", 'Webster, TX', 'Wellington', 'Wellington, New Zealand', 'Welt', 'Wema building', 'West', 'West Africa', 'West Bank, Gaza Strip', 'West Chester, PA', 'West Coast, Cali USA', 'West Coast, USA', 'West Hollywood', 'West Hollywood, CA', 'West Lancashire, UK.', 'West Midlands', 'West Palm Beach, Florida', 'West Richland, WA', 'West Vancouver, B.C.', 'West Virginia, USA', 'West Wales', 'WestEnd, Puritan Ave ', 'Westchester', 'Westerland', 'Western New York', 'Western Washington', 'Weston super mare', 'Weyburn', 'Where I Need To Be', 'Where ever i please', 'Where the money at', \"Wherever I'm needed\", \"Wherever I'm sent\", 'Whippany, NJ', 'Whitby, ON', 'White Plains, NY', 'Whiterun, Skyrim', 'Whole World ', 'Why should you know?', 'Wilbraham, MA', 'Wild Wild Web', 'Wildomar, CA', 'Williamsbridge, Bronx, New Yor', 'Williamsburg, VA', 'Williamstown, VT', 'Wilmington, DE', 'Wilmington, Delaware', 'Wilmington, NC', 'Wiltshire', 'Windsor,Ontario', 'Winnipeg', 'Winnipeg, MB, Canada', 'Winnipeg, Manitoba', 'Winston Salem, North Carolina', 'Winston-Salem, NC', 'Winter Park, Colorado', 'Wisconsin', 'Wisconsin, USA', 'Wolmers Trust School for Boys ', 'Wolverhampton', 'Wolverhampton/Brum/Jersey', 'Wonderland\\x89ÛÓ ?????? ???? ??????', 'Wood Buffalo, Alberta', 'Woodcreek HS, Roseville, CA', 'Woosley', 'Worcester, MA', 'World', 'World Wide', 'World Wide Web', 'World Wide!!', 'World news', 'WorldWide', 'WorldWideWeb', 'Worldwide', 'Worldwide - Global', 'Worldwide.', 'Wrex', 'Wrigley Field', 'Wynne, AR', 'Wyoming, MI (Grand Rapids)', \"Xi'an, China\", 'YA MOTHA BED', 'Yadkinville, NC', 'Yamaku Academy, Class 3-4', 'Yeezy Taught Me , NV', 'Yellowknife', 'Yellowknife, NT', 'Yewa zone', 'Ylisse', 'Yobe State', 'Yogya Berhati Nyaman', 'Yooooooo', \"You're not 19 forever   \", 'Youngstown, OH', 'Your Conversation', \"Your Sister's Bedroom\", 'Your Six', 'Your notifications', 'Your screen', 'Yuba City, CA', 'Yulee, FL', \"Yuuko-san's shop\", 'Zac Newsome loves me', 'Zeerust, South Africa', 'Zero Branco', 'Ziam af ', 'Zimbabwe', '[ Blonde Bi Fry. ]', '[ kate + they/them + infp-t ]', '[@blackparavde is my frankie]', '[Gia.] | #KardashianEmpire', '[marvel\\x89Û¢dragon age\\x89Û¢wicdiv]', 'a botanical garden probably', 'a box', 'a feminist, modernist hag.', 'a van down by the river', 'access to njh/5 and cth/4', 'africa', 'aggressive cannoli eater ', 'alberta, canada', 'all over the world', 'am everywhere', 'amsterdayum 120615 062415', 'antioch, california', 'antoine fisher ', 'anzio,italy', 'at my home', 'atlanta', 'atx', 'austin tx', 'austin, texas', 'ava', 'ayr', 'az', 'back in japan ??????????', 'bahstun/porta reeko', 'bajaur', 'balvanera', 'bangalore', 'beacon hills ', 'beijing .China', 'belleville', 'between ideas & 3-5pm AEST', 'big boy \\x89Û¢ 0802', 'bk. ', 'blackfalds.', 'boston', 'brisbane, australia', 'brooklyn, NYC', 'btwn a rock and a hard place', 'buenos aires argentina', 'buffalo / madrid / granada', 'buhh', 'but i love kaylen ??', 'by a piano probably. ', 'ca(NADA) ', 'calgary,ab', 'california', 'california mermaid ? ', 'california | oregon | peru |', 'call me peach or sam lo', 'canada', 'canberra', 'cedar rapids ia', 'central chazifornia', 'cereal aisle #17:i4', 'chicago', 'chillin at ceder rapids', 'china', 'cigarknub@gmail.com', 'cleveland, oh', 'cody, austin follows ?*?', 'cognitive dissonance town', 'columbus ohio', 'come here in 20 minutes for an ass kicking', 'contactSimpleNews@gmail.com', 'cork', 'cuba', 'cyprus', 'dallas', 'death star', 'denmark', 'denver colorado', 'di langit 7 bidadari (^,^ )', 'dmv ?? fashion school @ KSU. ', \"don't buy the s*n\", \"don't run\", 'dope show', 'dorito land', 'dreamy lake', 'dubai ', 'dublin ', 'ducked off . . . ', 'dundalk ireland', 'eARth 3', 'eBooks, North America', 'eating strawberry shitsickles', \"elena's bed // info on link\", 'elizabeth king', 'emily | helen | shelley ', 'en el pais de los arrechos', 'eritrean', 'everydaynigerian@gmail.com', 'everywhere', 'everywhere ', 'facebook.com/tradcatknights', 'far away', 'fl', 'fluffy cloud', 'followurDREAMS(& my instagram)', 'front row at a show', 'fujo garbage heaven ', 'gaffney, sc ', 'gamertag: bexrayandvav ', 'garowe puntland somalia', 'germany', 'glasgow', 'global', 'golborne, north west england.', 'guaravitas', 'h+l', 'hatena bookmark', 'have car; will travel', 'he/him or she/her (ask)', 'heart of darkness, unholy ?', 'heccfidmss@gmail.com', 'hell', 'hertfordshire.', 'hey Georgia', 'high way 99', 'highlands&slands scotland', 'hkXfYMhEx', 'hollywoodland ', 'home ', 'honeymoon avenue', 'houstn', 'houston', 'http://twitch.tv/jcmonkey', 'http://www.amazon.com/dp/B00HR', 'hyderabad', 'i beg vines sorry ', 'i got 1/13 menpa replies, omg', 'i love the smurfs 2', 'i love you zayn', 'i luv raquel', \"i'm a Citizen of the World\", 'iPhone: -27.499212,153.011072', 'iPhone: 33.104393,-96.628624', 'iTunes', 'iTunes - RSS', 'iamdigitalent.com', 'icon: cheese3d', 'ilford', 'ill yorker', 'illinois. united state ', 'im definitely taller than you.', \"in Dimitri's arms\", 'in my head', 'in my own personal hell (:', 'in the Word of God', 'india', 'indiana', 'infj ', 'instagram- Chloe_Bellx', 'instagram: bribriony', 'israel', \"it's a journey \", 'italy', 'japon', 'jayankondacholapuram.tamilnadu', 'jeddah | Khartoum', 'jersey ', 'justin & ari follow || tvd', 'kano', 'kansas', 'kediri,,jawa timur', 'keli x', 'kenya', 'khanna', 'khartoum sudan', 'kissimmee,fl.', 'kisumu', 'labuan, malaysia', 'labyrinthia', 'lagos nigeria', 'lagos. Unilag', 'lakewood colorado', 'laying on the bass', 'lee london', 'lesa * she/her', 'leyland', 'lia\\x89Û¢dani\\x89Û¢laura', 'liverpool ', 'livin in a plastic world', 'livin life in the 610', 'ljp/4', 'london', 'london / st catharines ?', 'london essex england uk', 'london town..', 'los angeles', 'los angeles, ca', 'lost in history', 'lost in my thoughts', 'louisville, kentucky', 'lowestoft', 'lrhcthband;four - bournemouth', 'lugo', 'm3, k, a, d', 'mainly California', 'manaus', 'manchester, uk.', 'marvel | books | hp | tmr', 'maryland', 'marysville ca ', 'melbourne', 'mexico', 'mi', 'miami', 'miami x dallas ', 'middle eastern palace', 'midwest', 'milky way', 'mind ya business', 'missouri USA', 'mnl', 'moss chamber b', 'mpls. ', 'mumbai', 'music.', 'my deli', 'my house', 'nap central', 'nap queen', 'nashville, tn ', 'nbc washington', 'nc', 'nearest trash can ', \"neil's kitchen  |  32215\", 'new york', 'new york, ny', 'ngapain?', 'nigeria', 'nj', 'nj/ny', 'nor*cal', 'norway', 'not so cool KY', 'nowhere', 'ny', 'nyc', 'ohio', 'oklahoma', 'oman muscat al seeb ', 'on a catwalk somewhere', 'on the go', 'on the web', 'on to the next adventure', 'on twitter', 'on twitter ', 'ona block w/ my BOY ??', 'online ', 'oxford', 'paradise', 'peekskill. new york, 10566 ', 'perth, australia ', 'peshawar pakistan ', 'pettyville, usa', 'ph', 'philly', 'philly ', 'phuket thailand', 'pissing off antis', 'pittsboro', 'planet earth', 'planeta H2o', 'playing soccer & eating pizza', 'please H? ?:??', 'plymouth', 'port matilda pa', 'portland, oregon', 'potters bar', 'prob turning up with sheen', 'probably not home', 'probably petting an animal', 'probably the strip club', 'probably watching survivor', 'proudly South African', 'qosqo', 'right here', 'right next to you', 'rio de janeiro | brazil', 'rome', 'rowyso dallas ', 'rural ohio (fuck)', 'russia', 'rzl ?', 'san gabriel la union', 'santo domingo', \"satan's colon\", 'scandinavia', 'scumbernauld', 'seattle grace mercy death', 'seattle wa', 'seoul', 'she/her/your majesty/empress', 'sheffield // rotherham', 'shoujo hell ', 'sindria', 'sisterhood', 'sitting on Eddie Vedders lap,', 'sitting on the fence, New York', 'snapchat~ maddzz_babby ', 'sneaking glances at Thancred', 'someplace living my life', 'somewhere USA ', 'somewhere in Indiana ', 'somewhere in Portugal', 'somewhere in cali ', 'somewhere outside', 'somewhere over a rainbow', 'somewhere too cold for me', 'south africa eastern cape', 'south of heaven ', 'southern california', 'southwest, Tx', 'sri lanka', 'ss', 'st.louis county missouri ', 'staggering on tenement roofs', 'statesboro/vidalia', 'sweden', 'swindon', 'sydney, australia', 'sÌ£o luis', 'taco bell', 'taken by piper curda', 'taking bath do not disturb', 'taking pain like pleasure', 'teh internets', 'texas', 'texas a&m university', 'texasss', 'the Dirty D', 'the Refrigerator ', 'the azure cloud', 'the burrow', 'the insane asylum. ', 'the local dump', 'the moon', 'the own zone layer ', 'the road to success', 'the void, U.S.A', 'they/her', 'they/them', 'they/them ', 'timeline kamu', 'todaysbigstock.com', 'tokyo', 'toledo', 'too far', 'toronto', 'toronto \\x89Û¢ dallas', 'toronto, ontario', 'trapped in America', \"travelling to tae's pants\", 'tri state', 'tripoli international airport', 'turner fenton', 'twitch.tv/naturalemblem26', 'tx', 'uk', 'under the blanket', 'urÌ£nus', 'us', 'us-east-1a', 'va', 'vancouver usa', 'victoria mozÌ£o ', 'w. Nykae ', 'walking the tightrope', 'washington, d.c.', 'watford', 'we?it \\x89Û¢ ixwin', 'westwestwestwestwestwestwest', \"where I'm supposed to be\", 'where the wild things are', 'wherever the $$$ at', \"wherever there's netflix\", 'wherever-the-fuck washington', \"whs '17\", 'winston-salem north carolina', 'wisco', 'with Doflamingo', 'wny', 'world', 'worldwide', \"wrapped arnd hyuk's finger\", 'www.aprylpooley.com', 'www.facebook.com/stuntfm', 'www.tmgcgart.com', 'www.twitch.tv/PKSparkxx', 'www.youtube.com?Malkavius2', \"xiumin's nonexistent solos\", 'y(our) boyfriends legs ', 'y/e/l', 'yorkshire\\n', 'youtube.com/channel/UCHWTLC9B4ZjUGh7yDlb55Iw', 'zboyer@washingtontimes.com', '{Detailed}', '{GoT | Modern AU | Lizz}', '| CA \\x89Û¢ GA  |', '|-/', '|| c h i c a g o ||', '\\x81Êwagger!Ì\\x90ominicanÌ÷', '\\x89Û¢ Views From The Six \\x89Û¢', '\\x89Û¢5\\x89Û¢12\\x89Û¢14\\x89Û¢ | åÈ#SaviourSquadåÇ', '\\x89Û¢901\\x89Û¢', '\\x89Û¢FLG\\x89Û¢', '\\x89Û¢III.XII.MMXI\\x89Û¢', '\\x89Û¢OlderCandyBloom\\x89Û¢', '\\x89ÛÊ\\x89ÛÊ\\x89ÛÊ', 'ÌÏT: -26.695807,27.837865', 'ÌÏT: 0.0,0.0', 'ÌÏT: 1.50225,103.742992', 'ÌÏT: 10.614817868480726,12.195582811791382', 'ÌÏT: 19.123127,72.825133', 'ÌÏT: 27.9136024,-81.6078532', 'ÌÏT: 30.307558,-81.403118', 'ÌÏT: 33.209923,-87.545328', 'ÌÏT: 35.223347,-80.827834', 'ÌÏT: 36.142163,-95.979189', 'ÌÏT: 39.982988,-75.261624', 'ÌÏT: 40.562796,-75.488849', 'ÌÏT: 40.707762,-74.014213', 'ÌÏT: 41.252426,-96.072013', 'ÌÏT: 42.910975,-78.865828', 'ÌÏT: 43.631838,-79.55807', 'ÌÏT: 6.4682,3.18287', 'ÌÏT: 6.488400524109015,3.352798039832285', 'ÌøåÀå_T: 40.736324,-73.990062', 'å_: ?? ÌÑ ? : ?', 'å_å_Los Mina City\\x89ã¢', 'å¡å¡Midwest \\x89Û¢\\x89Û¢', 'åÊ(?\\x89Û¢`?\\x89Û¢å«)??', 'åø\\\\_(?)_/åø']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.classes['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_390</th>\n",
       "      <th>emb_391</th>\n",
       "      <th>emb_392</th>\n",
       "      <th>emb_393</th>\n",
       "      <th>emb_394</th>\n",
       "      <th>emb_395</th>\n",
       "      <th>emb_396</th>\n",
       "      <th>emb_397</th>\n",
       "      <th>emb_398</th>\n",
       "      <th>emb_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6192</th>\n",
       "      <td>8839</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>sleeping with sirens vai vir pra sp</td>\n",
       "      <td>0</td>\n",
       "      <td>0.490382</td>\n",
       "      <td>0.422509</td>\n",
       "      <td>-0.098451</td>\n",
       "      <td>-0.684747</td>\n",
       "      <td>0.717871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807239</td>\n",
       "      <td>1.018406</td>\n",
       "      <td>-1.775813</td>\n",
       "      <td>-1.177999</td>\n",
       "      <td>-0.019847</td>\n",
       "      <td>-0.090329</td>\n",
       "      <td>0.166692</td>\n",
       "      <td>-0.133344</td>\n",
       "      <td>-0.035306</td>\n",
       "      <td>0.188026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>7768</td>\n",
       "      <td>158</td>\n",
       "      <td>2644</td>\n",
       "      <td>$1 million bail for man accused of #shooting at Fife #police - Aug 5 @ 8:16 PM ET http://t.co/hu5CXqnoBf</td>\n",
       "      <td>1</td>\n",
       "      <td>1.658683</td>\n",
       "      <td>-0.434940</td>\n",
       "      <td>1.315403</td>\n",
       "      <td>-0.384109</td>\n",
       "      <td>0.109086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067902</td>\n",
       "      <td>-0.546903</td>\n",
       "      <td>0.672220</td>\n",
       "      <td>0.413047</td>\n",
       "      <td>0.709461</td>\n",
       "      <td>-0.439854</td>\n",
       "      <td>0.363564</td>\n",
       "      <td>1.255388</td>\n",
       "      <td>0.843311</td>\n",
       "      <td>-1.184751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>6937</td>\n",
       "      <td>141</td>\n",
       "      <td>1133</td>\n",
       "      <td>@BenignoVito @LibertyBell1000 HILLARYMASS MURDERER.</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.391727</td>\n",
       "      <td>-2.019549</td>\n",
       "      <td>2.752736</td>\n",
       "      <td>-0.506640</td>\n",
       "      <td>-1.274974</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.230838</td>\n",
       "      <td>-0.670749</td>\n",
       "      <td>1.182071</td>\n",
       "      <td>2.056793</td>\n",
       "      <td>1.852203</td>\n",
       "      <td>-0.745635</td>\n",
       "      <td>-2.137211</td>\n",
       "      <td>0.233751</td>\n",
       "      <td>2.048053</td>\n",
       "      <td>0.462029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 405 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  keyword  location  \\\n",
       "6192  8839      180         0   \n",
       "5447  7768      158      2644   \n",
       "4873  6937      141      1133   \n",
       "\n",
       "                                                                                                          text  \\\n",
       "6192                                                                       sleeping with sirens vai vir pra sp   \n",
       "5447  $1 million bail for man accused of #shooting at Fife #police - Aug 5 @ 8:16 PM ET http://t.co/hu5CXqnoBf   \n",
       "4873                                                       @BenignoVito @LibertyBell1000 HILLARYMASS MURDERER.   \n",
       "\n",
       "      target     emb_0     emb_1     emb_2     emb_3     emb_4  ...   emb_390  \\\n",
       "6192       0  0.490382  0.422509 -0.098451 -0.684747  0.717871  ...  0.807239   \n",
       "5447       1  1.658683 -0.434940  1.315403 -0.384109  0.109086  ...  0.067902   \n",
       "4873       0 -0.391727 -2.019549  2.752736 -0.506640 -1.274974  ... -1.230838   \n",
       "\n",
       "       emb_391   emb_392   emb_393   emb_394   emb_395   emb_396   emb_397  \\\n",
       "6192  1.018406 -1.775813 -1.177999 -0.019847 -0.090329  0.166692 -0.133344   \n",
       "5447 -0.546903  0.672220  0.413047  0.709461 -0.439854  0.363564  1.255388   \n",
       "4873 -0.670749  1.182071  2.056793  1.852203 -0.745635 -2.137211  0.233751   \n",
       "\n",
       "       emb_398   emb_399  \n",
       "6192 -0.035306  0.188026  \n",
       "5447  0.843311 -1.184751  \n",
       "4873  2.048053  0.462029  \n",
       "\n",
       "[3 rows x 405 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.items.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pickle(path/'to.pkl',to)\n",
    "# to = load_pickle(path/'to.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,y = to.train.xs,to.train.y\n",
    "valid_xs,valid_y = to.valid.xs,to.valid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4375821287779238"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def random_forest(xs, y, n_estimators=100, min_samples_leaf=5):\n",
    "    rf = RandomForestClassifier(n_estimators, min_samples_leaf=min_samples_leaf)\n",
    "    rf.fit(xs, y);\n",
    "    return rf\n",
    "     \n",
    "\n",
    "m = random_forest(xs, y)\n",
    "mean_absolute_error(valid_y, m.predict(valid_xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# should we add more trees?\n",
    "plt.plot([mean_absolute_error(preds[:i+1].mean(0), valid_y) for i in range(100)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/cactus/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.483646517739816"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n",
    "mean_absolute_error(preds.mean(0), valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had 78% with LSTM and text column only, need to review what's going on in this solution, are the embedding what we expect?\n",
    "\n",
    "Maybe there's an index mismatch with the original df when loading in to the random forest? TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
